{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Transformer"
      ],
      "metadata": {
        "id": "GiWulKAdmAHX"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9KsBGZpKkWki"
      },
      "source": [
        "Transformer의 핵심 구조인 Multi-head Attention을 구현하는 실습입니다.\n",
        "1. Multi-head attention 및 self-attention 구현.\n",
        "2. 각 과정에서 일어나는 연산과 input/output 형태 이해."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8qRU5DFY2OM8"
      },
      "source": [
        "### 필요 패키지 import"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lDtMioSQQ1bB"
      },
      "source": [
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "from tqdm import tqdm\n",
        "\n",
        "import torch\n",
        "import math"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Req. 2-1 Multi-head self-attention 구조 익히기"
      ],
      "metadata": {
        "id": "HH0VdC4uJJVG"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QBiZObgRep_Q"
      },
      "source": [
        "### **데이터 전처리**\n",
        "vocab_size 100인 가상의 시퀀스 데이터를 생성합니다. \n",
        "\n",
        "각 데이터에 할당된 숫자는 tokenizing과 정수화가 이뤄진 형태입니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e9ULZIqTenSc"
      },
      "source": [
        "pad_id = 0\n",
        "vocab_size = 100\n",
        "\n",
        "data = [\n",
        "  [62, 13, 47, 39, 78, 33, 56, 13, 39, 29, 44, 86, 71, 36, 18, 75],\n",
        "  [60, 96, 51, 32, 90],\n",
        "  [35, 45, 48, 65, 91, 99, 92, 10, 3, 21, 54],\n",
        "  [75, 51],\n",
        "  [66, 88, 98, 47],\n",
        "  [21, 39, 10, 64, 21],\n",
        "  [98],\n",
        "  [77, 65, 51, 77, 19, 15, 35, 19, 23, 97, 50, 46, 53, 42, 45, 91, 66, 3, 43, 10],\n",
        "  [70, 64, 98, 25, 99, 53, 4, 13, 69, 62, 66, 76, 15, 75, 45, 34],\n",
        "  [20, 64, 81, 35, 76, 85, 1, 62, 8, 45, 99, 77, 19, 43]\n",
        "]"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Hx3mcivgMyH"
      },
      "source": [
        "# 길이 맞춰주기 위해 패딩합니다.\n",
        "def padding(data):\n",
        "  max_len = len(max(data, key=len))\n",
        "  print(f\"Maximum sequence length: {max_len}\")\n",
        "\n",
        "  for i, seq in enumerate(tqdm(data)):\n",
        "    if len(seq) < max_len:\n",
        "      data[i] = seq + [pad_id] * (max_len - len(seq))\n",
        "\n",
        "  return data, max_len"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s3e8FiNvgX60",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8a1f6879-451e-4dcd-a444-91ea5600a0a0"
      },
      "source": [
        "data, max_len = padding(data)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Maximum sequence length: 20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10/10 [00:00<00:00, 61862.89it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hwPSIWYugaN0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7f749fbf-f755-4a28-9d13-b1a2cfef76f8"
      },
      "source": [
        "data"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[62, 13, 47, 39, 78, 33, 56, 13, 39, 29, 44, 86, 71, 36, 18, 75, 0, 0, 0, 0],\n",
              " [60, 96, 51, 32, 90, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              " [35, 45, 48, 65, 91, 99, 92, 10, 3, 21, 54, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              " [75, 51, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              " [66, 88, 98, 47, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              " [21, 39, 10, 64, 21, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              " [98, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              " [77,\n",
              "  65,\n",
              "  51,\n",
              "  77,\n",
              "  19,\n",
              "  15,\n",
              "  35,\n",
              "  19,\n",
              "  23,\n",
              "  97,\n",
              "  50,\n",
              "  46,\n",
              "  53,\n",
              "  42,\n",
              "  45,\n",
              "  91,\n",
              "  66,\n",
              "  3,\n",
              "  43,\n",
              "  10],\n",
              " [70, 64, 98, 25, 99, 53, 4, 13, 69, 62, 66, 76, 15, 75, 45, 34, 0, 0, 0, 0],\n",
              " [20, 64, 81, 35, 76, 85, 1, 62, 8, 45, 99, 77, 19, 43, 0, 0, 0, 0, 0, 0]]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rwqjACx8iidc"
      },
      "source": [
        "### Hyperparameter 세팅 및 embedding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p-Ngp2nWimS8"
      },
      "source": [
        "d_model = 512  # model의 hidden size\n",
        "num_heads = 8  # head의 개수\n",
        "\n",
        "# d_model이 입력을 projection 시킬 임베딩 space의 차원이므로, num_heads로 나누어 떨어져야 한다."
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GJMi2Xsni5uq"
      },
      "source": [
        "embedding = nn.Embedding(vocab_size, d_model)\n",
        "\n",
        "# B: batch size, L: maximum sequence length\n",
        "batch = torch.LongTensor(data)  # (B, L)\n",
        "batch_emb = embedding(batch)  # (B, L, d_model)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3tLCUQwojcUb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "148bcf5f-0cc7-4cad-92e5-5e50a5cbc5ba"
      },
      "source": [
        "print(batch_emb)\n",
        "print(batch_emb.shape)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[-7.4030e-01, -2.2425e-01, -1.3538e+00,  ...,  1.1576e-01,\n",
            "           9.3618e-01,  8.3588e-01],\n",
            "         [-8.7616e-01, -1.6564e-01,  7.6842e-01,  ..., -3.9596e-01,\n",
            "           1.3460e+00, -1.1531e+00],\n",
            "         [ 6.8036e-01, -5.8503e-01, -1.1536e+00,  ..., -1.6724e+00,\n",
            "          -7.4470e-01,  9.5158e-01],\n",
            "         ...,\n",
            "         [ 8.7331e-01, -2.2447e+00,  1.5299e+00,  ...,  1.0030e-03,\n",
            "          -3.3851e-01,  4.8882e-01],\n",
            "         [ 8.7331e-01, -2.2447e+00,  1.5299e+00,  ...,  1.0030e-03,\n",
            "          -3.3851e-01,  4.8882e-01],\n",
            "         [ 8.7331e-01, -2.2447e+00,  1.5299e+00,  ...,  1.0030e-03,\n",
            "          -3.3851e-01,  4.8882e-01]],\n",
            "\n",
            "        [[-8.3087e-01, -1.0621e+00,  1.8359e+00,  ..., -4.9555e-01,\n",
            "          -1.0726e+00, -3.4788e-01],\n",
            "         [ 9.3795e-01, -1.3214e+00, -1.6135e-01,  ..., -1.5055e+00,\n",
            "           8.4032e-03, -1.5041e+00],\n",
            "         [-2.9202e-01,  3.2602e-01, -4.7028e-01,  ..., -4.6722e-01,\n",
            "          -2.0455e+00,  1.2131e+00],\n",
            "         ...,\n",
            "         [ 8.7331e-01, -2.2447e+00,  1.5299e+00,  ...,  1.0030e-03,\n",
            "          -3.3851e-01,  4.8882e-01],\n",
            "         [ 8.7331e-01, -2.2447e+00,  1.5299e+00,  ...,  1.0030e-03,\n",
            "          -3.3851e-01,  4.8882e-01],\n",
            "         [ 8.7331e-01, -2.2447e+00,  1.5299e+00,  ...,  1.0030e-03,\n",
            "          -3.3851e-01,  4.8882e-01]],\n",
            "\n",
            "        [[-2.5388e-01, -1.0041e+00,  1.3966e+00,  ..., -4.0406e-01,\n",
            "           5.3822e-01, -2.2107e-01],\n",
            "         [ 3.8278e-01, -1.8204e-01,  3.2623e+00,  ...,  1.1613e-01,\n",
            "           2.5527e-01,  4.0312e-02],\n",
            "         [-2.0027e-01,  5.8650e-01, -1.6862e+00,  ..., -9.1964e-01,\n",
            "          -1.2019e+00, -1.1970e+00],\n",
            "         ...,\n",
            "         [ 8.7331e-01, -2.2447e+00,  1.5299e+00,  ...,  1.0030e-03,\n",
            "          -3.3851e-01,  4.8882e-01],\n",
            "         [ 8.7331e-01, -2.2447e+00,  1.5299e+00,  ...,  1.0030e-03,\n",
            "          -3.3851e-01,  4.8882e-01],\n",
            "         [ 8.7331e-01, -2.2447e+00,  1.5299e+00,  ...,  1.0030e-03,\n",
            "          -3.3851e-01,  4.8882e-01]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ 1.7967e+00,  6.3176e-01, -1.7073e+00,  ...,  5.7552e-01,\n",
            "           1.3983e+00, -2.6340e-01],\n",
            "         [ 1.1401e+00,  2.8413e-01, -2.8336e-01,  ...,  1.0175e+00,\n",
            "          -1.2776e-01,  1.9321e+00],\n",
            "         [-2.9202e-01,  3.2602e-01, -4.7028e-01,  ..., -4.6722e-01,\n",
            "          -2.0455e+00,  1.2131e+00],\n",
            "         ...,\n",
            "         [-1.3845e+00, -4.1314e-02, -7.0112e-01,  ...,  1.3063e+00,\n",
            "           9.2486e-02, -1.8645e+00],\n",
            "         [-8.5362e-01, -8.1717e-01, -6.9296e-01,  ...,  7.9604e-01,\n",
            "          -1.5971e+00,  1.0795e+00],\n",
            "         [-7.2682e-01,  4.2012e-01,  2.8447e-02,  ..., -1.6337e-01,\n",
            "           5.0918e-01,  7.3834e-01]],\n",
            "\n",
            "        [[-4.0502e-01, -4.9593e-01,  2.7237e-01,  ..., -4.2316e-01,\n",
            "           5.5301e-01,  3.3017e-01],\n",
            "         [ 2.1491e+00, -4.2717e-02,  7.0024e-01,  ..., -5.1877e-01,\n",
            "          -1.8177e+00,  1.9197e+00],\n",
            "         [-1.8890e-01, -4.3429e-01, -1.1447e+00,  ..., -2.0205e+00,\n",
            "          -3.4897e-01,  1.7148e+00],\n",
            "         ...,\n",
            "         [ 8.7331e-01, -2.2447e+00,  1.5299e+00,  ...,  1.0030e-03,\n",
            "          -3.3851e-01,  4.8882e-01],\n",
            "         [ 8.7331e-01, -2.2447e+00,  1.5299e+00,  ...,  1.0030e-03,\n",
            "          -3.3851e-01,  4.8882e-01],\n",
            "         [ 8.7331e-01, -2.2447e+00,  1.5299e+00,  ...,  1.0030e-03,\n",
            "          -3.3851e-01,  4.8882e-01]],\n",
            "\n",
            "        [[-4.8453e-01,  1.4388e-01, -6.4543e-01,  ..., -2.1303e-01,\n",
            "           2.6432e-01, -3.6102e-01],\n",
            "         [ 2.1491e+00, -4.2717e-02,  7.0024e-01,  ..., -5.1877e-01,\n",
            "          -1.8177e+00,  1.9197e+00],\n",
            "         [-9.2275e-01,  2.6345e+00, -8.7301e-01,  ..., -9.9279e-01,\n",
            "           6.7811e-01, -2.2477e+00],\n",
            "         ...,\n",
            "         [ 8.7331e-01, -2.2447e+00,  1.5299e+00,  ...,  1.0030e-03,\n",
            "          -3.3851e-01,  4.8882e-01],\n",
            "         [ 8.7331e-01, -2.2447e+00,  1.5299e+00,  ...,  1.0030e-03,\n",
            "          -3.3851e-01,  4.8882e-01],\n",
            "         [ 8.7331e-01, -2.2447e+00,  1.5299e+00,  ...,  1.0030e-03,\n",
            "          -3.3851e-01,  4.8882e-01]]], grad_fn=<EmbeddingBackward0>)\n",
            "torch.Size([10, 20, 512])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s0Lhx892gmi3"
      },
      "source": [
        "### Linear projection & 여러 head로 나누기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "urXMBRnRgqvw"
      },
      "source": [
        "Multi-head attention 내에서 쓰이는 linear projection matrix들을 정의합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9DWKDqgCgfMk"
      },
      "source": [
        "w_q = nn.Linear(d_model, d_model)\n",
        "w_k = nn.Linear(d_model, d_model)\n",
        "w_v = nn.Linear(d_model, d_model)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tcLuhda7m-Lm"
      },
      "source": [
        "w_0 = nn.Linear(d_model, d_model)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H-vSL7PwnV6k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f4f4c777-98cb-4b21-f401-f8a73c9b4b07"
      },
      "source": [
        "q = w_q(batch_emb)  # (B, L, d_model)\n",
        "k = w_k(batch_emb)  # (B, L, d_model)\n",
        "v = w_v(batch_emb)  # (B, L, d_model)\n",
        "\n",
        "print(q.shape)\n",
        "print(k.shape)\n",
        "print(v.shape)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([10, 20, 512])\n",
            "torch.Size([10, 20, 512])\n",
            "torch.Size([10, 20, 512])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wnvlum-LnF1T"
      },
      "source": [
        "Q, k, v를 `num_head`개의 차원 분할된 여러 vector로 만듭니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sXcYLZYvJT_1"
      },
      "source": [
        "- 이론적으로는 multi-head attention을 수행하면 input을 각각 다른 head 개수만큼의 Wq, Wk, Wv로 linear transformation 해서 각각 여러번의 attention 수행한 후 concat 한 후 linear transformation 수행해준다\n",
        "- 구현에서는 Wq, Wk, Wv 한 개씩\n",
        "- 실제 `attention is all you need` 논문의 구현 예시는 Query vector 한개를 dim으로 쪼개서 진행한다"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_tiOKAv9nEli",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "025a8837-43f0-430e-847a-a623e2e924f3"
      },
      "source": [
        "batch_size = q.shape[0]\n",
        "d_k = d_model // num_heads\n",
        "\n",
        "# num_heads * d_k로 쪼갠다\n",
        "q = q.view(batch_size, -1, num_heads, d_k)  # (B, L, num_heads, d_k)\n",
        "k = k.view(batch_size, -1, num_heads, d_k)  # (B, L, num_heads, d_k)\n",
        "v = v.view(batch_size, -1, num_heads, d_k)  # (B, L, num_heads, d_k)\n",
        "\n",
        "print(q.shape)\n",
        "print(k.shape)\n",
        "print(v.shape)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([10, 20, 8, 64])\n",
            "torch.Size([10, 20, 8, 64])\n",
            "torch.Size([10, 20, 8, 64])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5tNb2isfn5Cx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "23b79c7b-cf2c-4b0c-baf9-f4ba3581a6eb"
      },
      "source": [
        "# num_heads를 밖으로 뺌으로써\n",
        "# 각 head가 (L, d_k) 만큼의 matrix를 가지고 self-attention 수행\n",
        "\n",
        "q = q.transpose(1, 2)  # (B, num_heads, L, d_k)\n",
        "k = k.transpose(1, 2)  # (B, num_heads, L, d_k)\n",
        "v = v.transpose(1, 2)  # (B, num_heads, L, d_k)\n",
        "\n",
        "print(q.shape)\n",
        "print(k.shape)\n",
        "print(v.shape)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([10, 8, 20, 64])\n",
            "torch.Size([10, 8, 20, 64])\n",
            "torch.Size([10, 8, 20, 64])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NWrDA5_Sofad"
      },
      "source": [
        "### Scaled dot-product self-attention 구현"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w52C4k3Wfl8m"
      },
      "source": [
        "각 head에서 실행되는 self-attetion 과정입니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A5waKr0Hfi2K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "08bbf754-16a2-4478-9179-624b859718f9"
      },
      "source": [
        "# shape - (L, L)\n",
        "# 같은 sequence 내에 서로 다른 token들에게 얼마나 가중치를 두고 attention을 해야하는가\n",
        "attn_scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(d_k)  # (B, num_heads, L, L)\n",
        "# softmax - row-wise이기 때문에 dim은 -1\n",
        "attn_dists = F.softmax(attn_scores, dim=-1)  # (B, num_heads, L, L)\n",
        "\n",
        "print(attn_dists)\n",
        "print(attn_dists.shape)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[[0.0464, 0.0435, 0.0620,  ..., 0.0449, 0.0449, 0.0449],\n",
            "          [0.0606, 0.0403, 0.0247,  ..., 0.0402, 0.0402, 0.0402],\n",
            "          [0.0308, 0.0306, 0.0444,  ..., 0.0335, 0.0335, 0.0335],\n",
            "          ...,\n",
            "          [0.0371, 0.0447, 0.0505,  ..., 0.0461, 0.0461, 0.0461],\n",
            "          [0.0371, 0.0447, 0.0505,  ..., 0.0461, 0.0461, 0.0461],\n",
            "          [0.0371, 0.0447, 0.0505,  ..., 0.0461, 0.0461, 0.0461]],\n",
            "\n",
            "         [[0.0438, 0.0585, 0.0423,  ..., 0.0263, 0.0263, 0.0263],\n",
            "          [0.0650, 0.0533, 0.0920,  ..., 0.0339, 0.0339, 0.0339],\n",
            "          [0.0522, 0.0552, 0.0349,  ..., 0.0364, 0.0364, 0.0364],\n",
            "          ...,\n",
            "          [0.0699, 0.0366, 0.0527,  ..., 0.0413, 0.0413, 0.0413],\n",
            "          [0.0699, 0.0366, 0.0527,  ..., 0.0413, 0.0413, 0.0413],\n",
            "          [0.0699, 0.0366, 0.0527,  ..., 0.0413, 0.0413, 0.0413]],\n",
            "\n",
            "         [[0.0435, 0.0422, 0.0433,  ..., 0.0691, 0.0691, 0.0691],\n",
            "          [0.0370, 0.0463, 0.0338,  ..., 0.0470, 0.0470, 0.0470],\n",
            "          [0.0432, 0.0368, 0.0524,  ..., 0.0309, 0.0309, 0.0309],\n",
            "          ...,\n",
            "          [0.0414, 0.0582, 0.0585,  ..., 0.0357, 0.0357, 0.0357],\n",
            "          [0.0414, 0.0582, 0.0585,  ..., 0.0357, 0.0357, 0.0357],\n",
            "          [0.0414, 0.0582, 0.0585,  ..., 0.0357, 0.0357, 0.0357]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[0.0774, 0.0229, 0.0453,  ..., 0.0410, 0.0410, 0.0410],\n",
            "          [0.0460, 0.0513, 0.0581,  ..., 0.0469, 0.0469, 0.0469],\n",
            "          [0.0421, 0.0417, 0.0516,  ..., 0.0331, 0.0331, 0.0331],\n",
            "          ...,\n",
            "          [0.0521, 0.0473, 0.0268,  ..., 0.0627, 0.0627, 0.0627],\n",
            "          [0.0521, 0.0473, 0.0268,  ..., 0.0627, 0.0627, 0.0627],\n",
            "          [0.0521, 0.0473, 0.0268,  ..., 0.0627, 0.0627, 0.0627]],\n",
            "\n",
            "         [[0.0425, 0.0595, 0.0568,  ..., 0.0444, 0.0444, 0.0444],\n",
            "          [0.0615, 0.0557, 0.0556,  ..., 0.0562, 0.0562, 0.0562],\n",
            "          [0.0368, 0.0556, 0.0607,  ..., 0.0550, 0.0550, 0.0550],\n",
            "          ...,\n",
            "          [0.0642, 0.0406, 0.0473,  ..., 0.0488, 0.0488, 0.0488],\n",
            "          [0.0642, 0.0406, 0.0473,  ..., 0.0488, 0.0488, 0.0488],\n",
            "          [0.0642, 0.0406, 0.0473,  ..., 0.0488, 0.0488, 0.0488]],\n",
            "\n",
            "         [[0.0468, 0.0313, 0.0254,  ..., 0.0759, 0.0759, 0.0759],\n",
            "          [0.0771, 0.0499, 0.0344,  ..., 0.0772, 0.0772, 0.0772],\n",
            "          [0.0435, 0.0623, 0.0391,  ..., 0.0591, 0.0591, 0.0591],\n",
            "          ...,\n",
            "          [0.0634, 0.0313, 0.0406,  ..., 0.0579, 0.0579, 0.0579],\n",
            "          [0.0634, 0.0313, 0.0406,  ..., 0.0579, 0.0579, 0.0579],\n",
            "          [0.0634, 0.0313, 0.0406,  ..., 0.0579, 0.0579, 0.0579]]],\n",
            "\n",
            "\n",
            "        [[[0.0500, 0.0337, 0.0297,  ..., 0.0519, 0.0519, 0.0519],\n",
            "          [0.0712, 0.0546, 0.0587,  ..., 0.0469, 0.0469, 0.0469],\n",
            "          [0.0331, 0.0620, 0.0718,  ..., 0.0471, 0.0471, 0.0471],\n",
            "          ...,\n",
            "          [0.0374, 0.0579, 0.0247,  ..., 0.0520, 0.0520, 0.0520],\n",
            "          [0.0374, 0.0579, 0.0247,  ..., 0.0520, 0.0520, 0.0520],\n",
            "          [0.0374, 0.0579, 0.0247,  ..., 0.0520, 0.0520, 0.0520]],\n",
            "\n",
            "         [[0.0282, 0.0509, 0.0548,  ..., 0.0524, 0.0524, 0.0524],\n",
            "          [0.0584, 0.0975, 0.0471,  ..., 0.0427, 0.0427, 0.0427],\n",
            "          [0.0417, 0.0255, 0.0464,  ..., 0.0529, 0.0529, 0.0529],\n",
            "          ...,\n",
            "          [0.0381, 0.0571, 0.1035,  ..., 0.0458, 0.0458, 0.0458],\n",
            "          [0.0381, 0.0571, 0.1035,  ..., 0.0458, 0.0458, 0.0458],\n",
            "          [0.0381, 0.0571, 0.1035,  ..., 0.0458, 0.0458, 0.0458]],\n",
            "\n",
            "         [[0.0322, 0.0659, 0.0404,  ..., 0.0517, 0.0517, 0.0517],\n",
            "          [0.0218, 0.0289, 0.0424,  ..., 0.0554, 0.0554, 0.0554],\n",
            "          [0.0528, 0.0423, 0.0431,  ..., 0.0522, 0.0522, 0.0522],\n",
            "          ...,\n",
            "          [0.0455, 0.0655, 0.0631,  ..., 0.0497, 0.0497, 0.0497],\n",
            "          [0.0455, 0.0655, 0.0631,  ..., 0.0497, 0.0497, 0.0497],\n",
            "          [0.0455, 0.0655, 0.0631,  ..., 0.0497, 0.0497, 0.0497]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[0.0300, 0.0285, 0.0210,  ..., 0.0562, 0.0562, 0.0562],\n",
            "          [0.0335, 0.0819, 0.0528,  ..., 0.0511, 0.0511, 0.0511],\n",
            "          [0.0348, 0.0373, 0.0458,  ..., 0.0535, 0.0535, 0.0535],\n",
            "          ...,\n",
            "          [0.0256, 0.0309, 0.0363,  ..., 0.0565, 0.0565, 0.0565],\n",
            "          [0.0256, 0.0309, 0.0363,  ..., 0.0565, 0.0565, 0.0565],\n",
            "          [0.0256, 0.0309, 0.0363,  ..., 0.0565, 0.0565, 0.0565]],\n",
            "\n",
            "         [[0.0753, 0.0898, 0.0456,  ..., 0.0462, 0.0462, 0.0462],\n",
            "          [0.0393, 0.0676, 0.0900,  ..., 0.0455, 0.0455, 0.0455],\n",
            "          [0.0576, 0.0841, 0.0826,  ..., 0.0444, 0.0444, 0.0444],\n",
            "          ...,\n",
            "          [0.0560, 0.0601, 0.0649,  ..., 0.0487, 0.0487, 0.0487],\n",
            "          [0.0560, 0.0601, 0.0649,  ..., 0.0487, 0.0487, 0.0487],\n",
            "          [0.0560, 0.0601, 0.0649,  ..., 0.0487, 0.0487, 0.0487]],\n",
            "\n",
            "         [[0.0369, 0.0284, 0.0431,  ..., 0.0510, 0.0510, 0.0510],\n",
            "          [0.0512, 0.0386, 0.0287,  ..., 0.0509, 0.0509, 0.0509],\n",
            "          [0.0758, 0.0316, 0.0540,  ..., 0.0479, 0.0479, 0.0479],\n",
            "          ...,\n",
            "          [0.0531, 0.0507, 0.0596,  ..., 0.0504, 0.0504, 0.0504],\n",
            "          [0.0531, 0.0507, 0.0596,  ..., 0.0504, 0.0504, 0.0504],\n",
            "          [0.0531, 0.0507, 0.0596,  ..., 0.0504, 0.0504, 0.0504]]],\n",
            "\n",
            "\n",
            "        [[[0.0334, 0.0326, 0.0585,  ..., 0.0462, 0.0462, 0.0462],\n",
            "          [0.0465, 0.0650, 0.0305,  ..., 0.0339, 0.0339, 0.0339],\n",
            "          [0.0374, 0.0310, 0.0167,  ..., 0.0569, 0.0569, 0.0569],\n",
            "          ...,\n",
            "          [0.0639, 0.0223, 0.0737,  ..., 0.0489, 0.0489, 0.0489],\n",
            "          [0.0639, 0.0223, 0.0737,  ..., 0.0489, 0.0489, 0.0489],\n",
            "          [0.0639, 0.0223, 0.0737,  ..., 0.0489, 0.0489, 0.0489]],\n",
            "\n",
            "         [[0.0478, 0.0450, 0.0432,  ..., 0.0596, 0.0596, 0.0596],\n",
            "          [0.0373, 0.0616, 0.0337,  ..., 0.0633, 0.0633, 0.0633],\n",
            "          [0.0342, 0.0357, 0.0488,  ..., 0.0515, 0.0515, 0.0515],\n",
            "          ...,\n",
            "          [0.0658, 0.0492, 0.0820,  ..., 0.0446, 0.0446, 0.0446],\n",
            "          [0.0658, 0.0492, 0.0820,  ..., 0.0446, 0.0446, 0.0446],\n",
            "          [0.0658, 0.0492, 0.0820,  ..., 0.0446, 0.0446, 0.0446]],\n",
            "\n",
            "         [[0.0380, 0.0268, 0.0598,  ..., 0.0487, 0.0487, 0.0487],\n",
            "          [0.0201, 0.0291, 0.0267,  ..., 0.0662, 0.0662, 0.0662],\n",
            "          [0.0366, 0.0671, 0.0289,  ..., 0.0550, 0.0550, 0.0550],\n",
            "          ...,\n",
            "          [0.0378, 0.0785, 0.0294,  ..., 0.0467, 0.0467, 0.0467],\n",
            "          [0.0378, 0.0785, 0.0294,  ..., 0.0467, 0.0467, 0.0467],\n",
            "          [0.0378, 0.0785, 0.0294,  ..., 0.0467, 0.0467, 0.0467]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[0.0631, 0.0671, 0.0665,  ..., 0.0546, 0.0546, 0.0546],\n",
            "          [0.0393, 0.0699, 0.0537,  ..., 0.0400, 0.0400, 0.0400],\n",
            "          [0.0965, 0.0368, 0.0769,  ..., 0.0428, 0.0428, 0.0428],\n",
            "          ...,\n",
            "          [0.0380, 0.0293, 0.0612,  ..., 0.0640, 0.0640, 0.0640],\n",
            "          [0.0380, 0.0293, 0.0612,  ..., 0.0640, 0.0640, 0.0640],\n",
            "          [0.0380, 0.0293, 0.0612,  ..., 0.0640, 0.0640, 0.0640]],\n",
            "\n",
            "         [[0.0493, 0.0746, 0.0397,  ..., 0.0574, 0.0574, 0.0574],\n",
            "          [0.0387, 0.0936, 0.0834,  ..., 0.0392, 0.0392, 0.0392],\n",
            "          [0.0495, 0.0460, 0.0913,  ..., 0.0370, 0.0370, 0.0370],\n",
            "          ...,\n",
            "          [0.0566, 0.0527, 0.0944,  ..., 0.0465, 0.0465, 0.0465],\n",
            "          [0.0566, 0.0527, 0.0944,  ..., 0.0465, 0.0465, 0.0465],\n",
            "          [0.0566, 0.0527, 0.0944,  ..., 0.0465, 0.0465, 0.0465]],\n",
            "\n",
            "         [[0.0452, 0.0472, 0.0734,  ..., 0.0430, 0.0430, 0.0430],\n",
            "          [0.0480, 0.0667, 0.0706,  ..., 0.0513, 0.0513, 0.0513],\n",
            "          [0.0523, 0.0738, 0.0549,  ..., 0.0462, 0.0462, 0.0462],\n",
            "          ...,\n",
            "          [0.0430, 0.0332, 0.0424,  ..., 0.0501, 0.0501, 0.0501],\n",
            "          [0.0430, 0.0332, 0.0424,  ..., 0.0501, 0.0501, 0.0501],\n",
            "          [0.0430, 0.0332, 0.0424,  ..., 0.0501, 0.0501, 0.0501]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[0.0273, 0.0543, 0.0499,  ..., 0.0512, 0.0512, 0.0509],\n",
            "          [0.0367, 0.0427, 0.0760,  ..., 0.0337, 0.0363, 0.0538],\n",
            "          [0.0508, 0.0284, 0.0567,  ..., 0.0942, 0.0589, 0.0437],\n",
            "          ...,\n",
            "          [0.0389, 0.0646, 0.1030,  ..., 0.0420, 0.0650, 0.0429],\n",
            "          [0.0423, 0.0482, 0.0379,  ..., 0.0490, 0.0928, 0.0430],\n",
            "          [0.0754, 0.0690, 0.0506,  ..., 0.0248, 0.0173, 0.0647]],\n",
            "\n",
            "         [[0.0870, 0.0508, 0.0358,  ..., 0.0334, 0.0744, 0.0300],\n",
            "          [0.0510, 0.0530, 0.0676,  ..., 0.0273, 0.0419, 0.0486],\n",
            "          [0.0580, 0.0410, 0.0501,  ..., 0.0393, 0.0483, 0.0427],\n",
            "          ...,\n",
            "          [0.0496, 0.0410, 0.0428,  ..., 0.0302, 0.0451, 0.0410],\n",
            "          [0.0469, 0.0254, 0.0412,  ..., 0.0320, 0.0483, 0.0600],\n",
            "          [0.0460, 0.0415, 0.0514,  ..., 0.0858, 0.0556, 0.0684]],\n",
            "\n",
            "         [[0.0950, 0.0309, 0.0832,  ..., 0.0547, 0.0408, 0.0574],\n",
            "          [0.0530, 0.0593, 0.0821,  ..., 0.0431, 0.0608, 0.0391],\n",
            "          [0.0477, 0.0630, 0.0506,  ..., 0.0504, 0.0435, 0.0567],\n",
            "          ...,\n",
            "          [0.0511, 0.0466, 0.0636,  ..., 0.0567, 0.0453, 0.0491],\n",
            "          [0.0646, 0.0354, 0.0625,  ..., 0.0423, 0.0261, 0.0903],\n",
            "          [0.0613, 0.0445, 0.0577,  ..., 0.0720, 0.0426, 0.0622]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[0.0299, 0.0533, 0.0445,  ..., 0.0547, 0.0500, 0.0513],\n",
            "          [0.0561, 0.0512, 0.0505,  ..., 0.0505, 0.0863, 0.0458],\n",
            "          [0.0392, 0.0486, 0.0667,  ..., 0.0283, 0.0460, 0.1028],\n",
            "          ...,\n",
            "          [0.0439, 0.0462, 0.0641,  ..., 0.0533, 0.0495, 0.0536],\n",
            "          [0.0361, 0.0348, 0.0579,  ..., 0.0253, 0.0299, 0.0955],\n",
            "          [0.0423, 0.0443, 0.0570,  ..., 0.0352, 0.0416, 0.0278]],\n",
            "\n",
            "         [[0.0424, 0.0577, 0.0633,  ..., 0.0431, 0.0532, 0.0563],\n",
            "          [0.0434, 0.0396, 0.0648,  ..., 0.0817, 0.0352, 0.0380],\n",
            "          [0.0319, 0.0344, 0.0784,  ..., 0.0332, 0.0308, 0.0960],\n",
            "          ...,\n",
            "          [0.0366, 0.0411, 0.0520,  ..., 0.0537, 0.0568, 0.0731],\n",
            "          [0.0400, 0.0589, 0.0703,  ..., 0.0611, 0.0374, 0.0581],\n",
            "          [0.0491, 0.0189, 0.0301,  ..., 0.0394, 0.0487, 0.0454]],\n",
            "\n",
            "         [[0.0514, 0.0467, 0.0620,  ..., 0.0464, 0.0475, 0.0612],\n",
            "          [0.0548, 0.0462, 0.0420,  ..., 0.0464, 0.0406, 0.0356],\n",
            "          [0.0642, 0.0404, 0.0474,  ..., 0.0438, 0.0519, 0.0403],\n",
            "          ...,\n",
            "          [0.0385, 0.0276, 0.0501,  ..., 0.0388, 0.0407, 0.1152],\n",
            "          [0.0528, 0.0431, 0.0462,  ..., 0.0879, 0.0400, 0.0649],\n",
            "          [0.0480, 0.0955, 0.0569,  ..., 0.0669, 0.0599, 0.0237]]],\n",
            "\n",
            "\n",
            "        [[[0.0223, 0.0444, 0.0741,  ..., 0.0573, 0.0573, 0.0573],\n",
            "          [0.0402, 0.0598, 0.0449,  ..., 0.0676, 0.0676, 0.0676],\n",
            "          [0.0418, 0.0444, 0.0416,  ..., 0.0521, 0.0521, 0.0521],\n",
            "          ...,\n",
            "          [0.0580, 0.0619, 0.0387,  ..., 0.0453, 0.0453, 0.0453],\n",
            "          [0.0580, 0.0619, 0.0387,  ..., 0.0453, 0.0453, 0.0453],\n",
            "          [0.0580, 0.0619, 0.0387,  ..., 0.0453, 0.0453, 0.0453]],\n",
            "\n",
            "         [[0.0344, 0.0441, 0.0894,  ..., 0.0682, 0.0682, 0.0682],\n",
            "          [0.0411, 0.0352, 0.0588,  ..., 0.0624, 0.0624, 0.0624],\n",
            "          [0.0501, 0.0626, 0.0280,  ..., 0.0447, 0.0447, 0.0447],\n",
            "          ...,\n",
            "          [0.0520, 0.0567, 0.0650,  ..., 0.0406, 0.0406, 0.0406],\n",
            "          [0.0520, 0.0567, 0.0650,  ..., 0.0406, 0.0406, 0.0406],\n",
            "          [0.0520, 0.0567, 0.0650,  ..., 0.0406, 0.0406, 0.0406]],\n",
            "\n",
            "         [[0.0441, 0.0423, 0.0401,  ..., 0.0404, 0.0404, 0.0404],\n",
            "          [0.0331, 0.0546, 0.0364,  ..., 0.0487, 0.0487, 0.0487],\n",
            "          [0.0579, 0.0600, 0.0235,  ..., 0.0414, 0.0414, 0.0414],\n",
            "          ...,\n",
            "          [0.0547, 0.0448, 0.0588,  ..., 0.0404, 0.0404, 0.0404],\n",
            "          [0.0547, 0.0448, 0.0588,  ..., 0.0404, 0.0404, 0.0404],\n",
            "          [0.0547, 0.0448, 0.0588,  ..., 0.0404, 0.0404, 0.0404]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[0.0717, 0.0539, 0.0319,  ..., 0.0419, 0.0419, 0.0419],\n",
            "          [0.0547, 0.0498, 0.0478,  ..., 0.0460, 0.0460, 0.0460],\n",
            "          [0.0526, 0.0615, 0.0416,  ..., 0.0359, 0.0359, 0.0359],\n",
            "          ...,\n",
            "          [0.0234, 0.0675, 0.0346,  ..., 0.0656, 0.0656, 0.0656],\n",
            "          [0.0234, 0.0675, 0.0346,  ..., 0.0656, 0.0656, 0.0656],\n",
            "          [0.0234, 0.0675, 0.0346,  ..., 0.0656, 0.0656, 0.0656]],\n",
            "\n",
            "         [[0.0578, 0.0669, 0.0560,  ..., 0.0523, 0.0523, 0.0523],\n",
            "          [0.0326, 0.0426, 0.0584,  ..., 0.0424, 0.0424, 0.0424],\n",
            "          [0.0547, 0.0492, 0.0540,  ..., 0.0414, 0.0414, 0.0414],\n",
            "          ...,\n",
            "          [0.0583, 0.0305, 0.0471,  ..., 0.0444, 0.0444, 0.0444],\n",
            "          [0.0583, 0.0305, 0.0471,  ..., 0.0444, 0.0444, 0.0444],\n",
            "          [0.0583, 0.0305, 0.0471,  ..., 0.0444, 0.0444, 0.0444]],\n",
            "\n",
            "         [[0.0779, 0.0799, 0.0240,  ..., 0.0365, 0.0365, 0.0365],\n",
            "          [0.0577, 0.0551, 0.0342,  ..., 0.0559, 0.0559, 0.0559],\n",
            "          [0.0400, 0.0533, 0.0397,  ..., 0.0616, 0.0616, 0.0616],\n",
            "          ...,\n",
            "          [0.0494, 0.0351, 0.0380,  ..., 0.0515, 0.0515, 0.0515],\n",
            "          [0.0494, 0.0351, 0.0380,  ..., 0.0515, 0.0515, 0.0515],\n",
            "          [0.0494, 0.0351, 0.0380,  ..., 0.0515, 0.0515, 0.0515]]],\n",
            "\n",
            "\n",
            "        [[[0.0647, 0.0511, 0.0330,  ..., 0.0488, 0.0488, 0.0488],\n",
            "          [0.0402, 0.0574, 0.0402,  ..., 0.0650, 0.0650, 0.0650],\n",
            "          [0.1100, 0.0408, 0.0660,  ..., 0.0322, 0.0322, 0.0322],\n",
            "          ...,\n",
            "          [0.0329, 0.0611, 0.0613,  ..., 0.0447, 0.0447, 0.0447],\n",
            "          [0.0329, 0.0611, 0.0613,  ..., 0.0447, 0.0447, 0.0447],\n",
            "          [0.0329, 0.0611, 0.0613,  ..., 0.0447, 0.0447, 0.0447]],\n",
            "\n",
            "         [[0.0504, 0.0380, 0.0441,  ..., 0.0514, 0.0514, 0.0514],\n",
            "          [0.0451, 0.0356, 0.0311,  ..., 0.0630, 0.0630, 0.0630],\n",
            "          [0.0568, 0.0350, 0.0416,  ..., 0.0659, 0.0659, 0.0659],\n",
            "          ...,\n",
            "          [0.0502, 0.0582, 0.0413,  ..., 0.0417, 0.0417, 0.0417],\n",
            "          [0.0502, 0.0582, 0.0413,  ..., 0.0417, 0.0417, 0.0417],\n",
            "          [0.0502, 0.0582, 0.0413,  ..., 0.0417, 0.0417, 0.0417]],\n",
            "\n",
            "         [[0.0572, 0.0563, 0.0349,  ..., 0.0560, 0.0560, 0.0560],\n",
            "          [0.0459, 0.0552, 0.0474,  ..., 0.0493, 0.0493, 0.0493],\n",
            "          [0.0491, 0.0447, 0.1018,  ..., 0.0408, 0.0408, 0.0408],\n",
            "          ...,\n",
            "          [0.0433, 0.0466, 0.0519,  ..., 0.0420, 0.0420, 0.0420],\n",
            "          [0.0433, 0.0466, 0.0519,  ..., 0.0420, 0.0420, 0.0420],\n",
            "          [0.0433, 0.0466, 0.0519,  ..., 0.0420, 0.0420, 0.0420]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[0.0325, 0.0450, 0.0453,  ..., 0.0613, 0.0613, 0.0613],\n",
            "          [0.0502, 0.0457, 0.0508,  ..., 0.0422, 0.0422, 0.0422],\n",
            "          [0.0567, 0.0479, 0.0640,  ..., 0.0485, 0.0485, 0.0485],\n",
            "          ...,\n",
            "          [0.0678, 0.0655, 0.0326,  ..., 0.0637, 0.0637, 0.0637],\n",
            "          [0.0678, 0.0655, 0.0326,  ..., 0.0637, 0.0637, 0.0637],\n",
            "          [0.0678, 0.0655, 0.0326,  ..., 0.0637, 0.0637, 0.0637]],\n",
            "\n",
            "         [[0.0555, 0.0718, 0.0316,  ..., 0.0717, 0.0717, 0.0717],\n",
            "          [0.0448, 0.0428, 0.0386,  ..., 0.0426, 0.0426, 0.0426],\n",
            "          [0.0613, 0.0413, 0.0324,  ..., 0.0441, 0.0441, 0.0441],\n",
            "          ...,\n",
            "          [0.0447, 0.0337, 0.0660,  ..., 0.0490, 0.0490, 0.0490],\n",
            "          [0.0447, 0.0337, 0.0660,  ..., 0.0490, 0.0490, 0.0490],\n",
            "          [0.0447, 0.0337, 0.0660,  ..., 0.0490, 0.0490, 0.0490]],\n",
            "\n",
            "         [[0.0507, 0.0420, 0.0407,  ..., 0.0590, 0.0590, 0.0590],\n",
            "          [0.0694, 0.0515, 0.0590,  ..., 0.0523, 0.0523, 0.0523],\n",
            "          [0.0990, 0.0608, 0.0288,  ..., 0.0387, 0.0387, 0.0387],\n",
            "          ...,\n",
            "          [0.0502, 0.0368, 0.0528,  ..., 0.0540, 0.0540, 0.0540],\n",
            "          [0.0502, 0.0368, 0.0528,  ..., 0.0540, 0.0540, 0.0540],\n",
            "          [0.0502, 0.0368, 0.0528,  ..., 0.0540, 0.0540, 0.0540]]]],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "torch.Size([10, 8, 20, 20])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7megouWpgCck",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "33263c65-5b4b-41c1-ebcc-c448f999b847"
      },
      "source": [
        "attn_values = torch.matmul(attn_dists, v)  # (B, num_heads, L, d_k)\n",
        "\n",
        "print(attn_values.shape)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([10, 8, 20, 64])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LmSTaymdg-P_"
      },
      "source": [
        "### 각 head의 결과물 병합"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YSdQZCk0hCNd"
      },
      "source": [
        "각 head의 결과물을 concat하고 동일 차원으로 linear projection합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eaK0bpMGhQZ2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cb8292d4-2e81-437b-85b1-fd8f610384e2"
      },
      "source": [
        "attn_values = attn_values.transpose(1, 2)  # (B, L, num_heads, d_k)\n",
        "attn_values = attn_values.contiguous().view(batch_size, -1, d_model)  # (B, L, d_model)\n",
        "\n",
        "print(attn_values.shape)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([10, 20, 512])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LTng_2SXhdH1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bc411f64-bd84-409d-8bf8-0a7461de929b"
      },
      "source": [
        "# w_0 : (d_model, d_model)\n",
        "# 서로 다른 의미로 foucsing 된 각 head의 self-attention 정보들을 합쳐주는 역할 수행\n",
        "outputs = w_0(attn_values)\n",
        "\n",
        "print(outputs)\n",
        "print(outputs.shape)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[-0.0425, -0.1513, -0.0322,  ...,  0.0210, -0.0773, -0.0627],\n",
            "         [-0.0367, -0.2024,  0.0317,  ...,  0.0479, -0.0735, -0.0836],\n",
            "         [-0.0208, -0.1433,  0.0025,  ...,  0.0107, -0.0604, -0.1004],\n",
            "         ...,\n",
            "         [-0.0639, -0.1969, -0.0218,  ...,  0.0422, -0.0249, -0.0367],\n",
            "         [-0.0639, -0.1969, -0.0218,  ...,  0.0422, -0.0249, -0.0367],\n",
            "         [-0.0639, -0.1969, -0.0218,  ...,  0.0422, -0.0249, -0.0367]],\n",
            "\n",
            "        [[ 0.1218, -0.2129, -0.0539,  ...,  0.0314, -0.0605, -0.0611],\n",
            "         [ 0.0503, -0.2200, -0.0483,  ...,  0.0445, -0.0394, -0.0813],\n",
            "         [ 0.1081, -0.2482, -0.0596,  ...,  0.0088, -0.0886, -0.0648],\n",
            "         ...,\n",
            "         [ 0.1175, -0.2221, -0.0478,  ...,  0.0287, -0.0427, -0.0798],\n",
            "         [ 0.1175, -0.2221, -0.0478,  ...,  0.0287, -0.0427, -0.0798],\n",
            "         [ 0.1175, -0.2221, -0.0478,  ...,  0.0287, -0.0427, -0.0798]],\n",
            "\n",
            "        [[ 0.0757, -0.1550,  0.0830,  ...,  0.0266, -0.0216,  0.0205],\n",
            "         [ 0.0947, -0.2277,  0.0217,  ...,  0.0327, -0.1092, -0.0378],\n",
            "         [ 0.0871, -0.2261,  0.0511,  ..., -0.0044, -0.0601, -0.0322],\n",
            "         ...,\n",
            "         [ 0.0267, -0.1475,  0.0553,  ..., -0.0237, -0.0831, -0.0114],\n",
            "         [ 0.0267, -0.1475,  0.0553,  ..., -0.0237, -0.0831, -0.0114],\n",
            "         [ 0.0267, -0.1475,  0.0553,  ..., -0.0237, -0.0831, -0.0114]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[-0.1126, -0.0446, -0.0037,  ..., -0.1351,  0.0961,  0.1229],\n",
            "         [-0.1051,  0.0199,  0.0087,  ..., -0.1450,  0.0216,  0.0449],\n",
            "         [-0.1212,  0.0048, -0.0264,  ..., -0.1795,  0.0265,  0.0655],\n",
            "         ...,\n",
            "         [-0.1080, -0.0135,  0.0161,  ..., -0.1549,  0.0120,  0.0546],\n",
            "         [-0.1237, -0.0189, -0.0785,  ..., -0.1679,  0.0980,  0.0326],\n",
            "         [-0.1126, -0.0346, -0.0219,  ..., -0.1451,  0.0735,  0.0503]],\n",
            "\n",
            "        [[ 0.1522, -0.1263,  0.0028,  ...,  0.0199,  0.0110, -0.0750],\n",
            "         [ 0.1212, -0.1510,  0.0171,  ...,  0.0038,  0.0620, -0.0898],\n",
            "         [ 0.1511, -0.1211,  0.0125,  ...,  0.0302,  0.0211, -0.0566],\n",
            "         ...,\n",
            "         [ 0.0765, -0.1493,  0.0231,  ..., -0.0200,  0.0365, -0.0692],\n",
            "         [ 0.0765, -0.1493,  0.0231,  ..., -0.0200,  0.0365, -0.0692],\n",
            "         [ 0.0765, -0.1493,  0.0231,  ..., -0.0200,  0.0365, -0.0692]],\n",
            "\n",
            "        [[ 0.0670, -0.1501, -0.0014,  ...,  0.0346, -0.0256,  0.0379],\n",
            "         [ 0.0706, -0.1436, -0.0114,  ...,  0.0164,  0.0661,  0.0455],\n",
            "         [ 0.0522, -0.1238, -0.0418,  ..., -0.0043,  0.0520,  0.0474],\n",
            "         ...,\n",
            "         [ 0.0473, -0.1792,  0.0095,  ..., -0.0096,  0.0015,  0.0316],\n",
            "         [ 0.0473, -0.1792,  0.0095,  ..., -0.0096,  0.0015,  0.0316],\n",
            "         [ 0.0473, -0.1792,  0.0095,  ..., -0.0096,  0.0015,  0.0316]]],\n",
            "       grad_fn=<ViewBackward0>)\n",
            "torch.Size([10, 20, 512])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "goX70VKqhxQH"
      },
      "source": [
        "## Req. 2-2 Multi-head self-attention 모듈 클래스 구현"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WtNyV7mMj7V_"
      },
      "source": [
        "위의 과정을 모두 합쳐 하나의 Multi-head attention 모듈 class를 구현하겠습니다.\n",
        "\n",
        "아래 코드의 TODO 부분을 채워주세요."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U_kNhOTrkBHm"
      },
      "source": [
        "class MultiheadAttention(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(MultiheadAttention, self).__init__()\n",
        "\n",
        "    # Q, K, V learnable matrices\n",
        "    self.w_q = nn.Linear(d_model, d_model)\n",
        "    self.w_k = nn.Linear(d_model, d_model)\n",
        "    self.w_v = nn.Linear(d_model, d_model)\n",
        "\n",
        "    # Linear projection for concatenated outputs\n",
        "    self.w_0 = nn.Linear(d_model, d_model)\n",
        "\n",
        "  # scaled-dot product attention\n",
        "  def self_attention(self, q, k, v):\n",
        "    attn_scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(d_k)  # (B, num_heads, L, L)\n",
        "    attn_dists = F.softmax(attn_scores, dim=-1)  # (B, num_heads, L, L)\n",
        "\n",
        "    attn_values = torch.matmul(attn_dists, v)  # (B, num_heads, L, d_k)\n",
        "\n",
        "    return attn_values\n",
        "\n",
        "  def forward(self, q, k, v):\n",
        "    batch_size = q.shape[0]\n",
        "\n",
        "    # linear projection\n",
        "    ################################################################################\n",
        "    # TODO 1: Implement the forward pass for linear projection.                #\n",
        "    ################################################################################\n",
        "    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "    q = self.w_q(q)  # (B, L, d_model)\n",
        "    k = self.w_k(k)  # (B, L, d_model)\n",
        "    v = self.w_v(v)  # (B, L, d_model)\n",
        " \n",
        "\n",
        "    # head만큼 쪼개준다\n",
        "    ################################################################################\n",
        "    # TODO 2: Implement the forward pass for \bsplit head.                #\n",
        "    ################################################################################\n",
        "    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "    q = q.view(batch_size, -1, num_heads, d_k)  # (B, L, num_heads, d_k)\n",
        "    k = k.view(batch_size, -1, num_heads, d_k)  # (B, L, num_heads, d_k)\n",
        "    v = v.view(batch_size, -1, num_heads, d_k)  # (B, L, num_heads, d_k)\n",
        "    \n",
        "    # 각 head가 (L, d_k)의 matrix를 담당하도록 만든다\n",
        "    q = q.transpose(1, 2)  # (B, num_heads, L, d_k)\n",
        "    k = k.transpose(1, 2)  # (B, num_heads, L, d_k)\n",
        "    v = v.transpose(1, 2)  # (B, num_heads, L, d_k)\n",
        "\n",
        "    attn_values = self.self_attention(q, k, v)  # (B, num_heads, L, d_k)\n",
        "    attn_values = attn_values.transpose(1, 2).contiguous().view(batch_size, -1, d_model)  # (B, L, num_heads, d_k) => (B, L, d_model)\n",
        "\n",
        "    return self.w_0(attn_values)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jYLuu_9alQxT"
      },
      "source": [
        "multihead_attn = MultiheadAttention()\n",
        "\n",
        "outputs = multihead_attn(batch_emb, batch_emb, batch_emb)  # (B, L, d_model)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KMiXlYjSlTfB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e534ded6-f798-44ee-f601-ec2da575e2a4"
      },
      "source": [
        "print(outputs)\n",
        "print(outputs.shape)  # (batch_size, length, d_model)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[ 1.9788e-02,  3.6366e-02,  1.0439e-02,  ..., -1.9316e-02,\n",
            "           5.2268e-02,  9.0272e-03],\n",
            "         [ 4.1677e-02,  5.1601e-02, -1.5555e-02,  ...,  2.2124e-02,\n",
            "           5.1019e-02,  4.3039e-02],\n",
            "         [ 2.6515e-02,  1.3386e-01, -2.7943e-03,  ..., -3.6344e-02,\n",
            "           4.1472e-02,  4.1080e-02],\n",
            "         ...,\n",
            "         [-2.0826e-02,  7.9785e-02,  3.5761e-02,  ..., -3.4929e-03,\n",
            "           2.9025e-02, -1.4477e-02],\n",
            "         [-2.0826e-02,  7.9785e-02,  3.5761e-02,  ..., -3.4929e-03,\n",
            "           2.9025e-02, -1.4477e-02],\n",
            "         [-2.0826e-02,  7.9785e-02,  3.5761e-02,  ..., -3.4929e-03,\n",
            "           2.9025e-02, -1.4477e-02]],\n",
            "\n",
            "        [[ 4.2514e-01, -1.1362e-02, -2.8477e-01,  ..., -2.4952e-01,\n",
            "           1.2188e-01,  2.0767e-01],\n",
            "         [ 4.5847e-01, -2.5576e-02, -2.7866e-01,  ..., -2.6058e-01,\n",
            "           1.1262e-01,  2.4419e-01],\n",
            "         [ 4.4624e-01, -8.9506e-02, -3.4682e-01,  ..., -3.2138e-01,\n",
            "           1.0517e-01,  1.6000e-01],\n",
            "         ...,\n",
            "         [ 3.7834e-01, -2.3408e-02, -2.7782e-01,  ..., -2.5256e-01,\n",
            "           1.0086e-01,  2.2041e-01],\n",
            "         [ 3.7834e-01, -2.3408e-02, -2.7782e-01,  ..., -2.5256e-01,\n",
            "           1.0086e-01,  2.2041e-01],\n",
            "         [ 3.7834e-01, -2.3408e-02, -2.7782e-01,  ..., -2.5256e-01,\n",
            "           1.0086e-01,  2.2041e-01]],\n",
            "\n",
            "        [[ 3.4136e-01, -9.0607e-03, -3.5781e-01,  ..., -2.4195e-01,\n",
            "           7.9031e-02,  8.1104e-02],\n",
            "         [ 3.4408e-01, -3.9756e-02, -2.9617e-01,  ..., -2.0606e-01,\n",
            "           9.0644e-02,  1.3669e-01],\n",
            "         [ 2.6682e-01,  5.6886e-02, -3.3143e-01,  ..., -1.8174e-01,\n",
            "           6.7582e-02,  1.6257e-01],\n",
            "         ...,\n",
            "         [ 2.8784e-01, -2.7396e-02, -2.7103e-01,  ..., -1.2481e-01,\n",
            "           1.0936e-01,  1.4918e-01],\n",
            "         [ 2.8784e-01, -2.7396e-02, -2.7103e-01,  ..., -1.2481e-01,\n",
            "           1.0936e-01,  1.4918e-01],\n",
            "         [ 2.8784e-01, -2.7396e-02, -2.7103e-01,  ..., -1.2481e-01,\n",
            "           1.0936e-01,  1.4918e-01]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[-2.8921e-03,  3.3003e-02, -2.4741e-02,  ..., -3.7215e-02,\n",
            "           9.7430e-02, -1.6103e-02],\n",
            "         [-5.1022e-02, -4.2417e-04, -4.2939e-02,  ..., -7.4092e-02,\n",
            "           8.9961e-02, -4.8352e-02],\n",
            "         [-3.0666e-02,  5.7031e-02, -4.2082e-02,  ..., -9.3611e-02,\n",
            "           1.0497e-01, -6.9072e-02],\n",
            "         ...,\n",
            "         [-3.1267e-02,  5.2877e-02, -3.3836e-02,  ..., -5.4221e-02,\n",
            "           8.9413e-02, -6.9123e-02],\n",
            "         [-6.8365e-03,  1.2282e-02,  2.1473e-02,  ..., -4.7086e-02,\n",
            "           9.0038e-02, -3.3545e-02],\n",
            "         [ 3.1247e-03,  8.4303e-02, -5.7851e-03,  ..., -5.9107e-02,\n",
            "           1.1039e-01, -3.7844e-02]],\n",
            "\n",
            "        [[ 2.6717e-02, -4.4873e-02, -1.8547e-02,  ...,  1.8557e-02,\n",
            "          -7.7852e-02, -7.5013e-02],\n",
            "         [ 9.5763e-02, -3.4318e-02, -2.6180e-02,  ...,  1.5958e-02,\n",
            "          -7.2621e-02, -1.6060e-02],\n",
            "         [ 7.2686e-02, -2.8726e-02,  2.7591e-03,  ...,  6.6735e-02,\n",
            "          -7.8408e-02, -9.3366e-03],\n",
            "         ...,\n",
            "         [ 7.3161e-02, -5.8689e-02, -3.9361e-02,  ...,  1.1880e-01,\n",
            "          -1.2016e-01, -3.6590e-02],\n",
            "         [ 7.3161e-02, -5.8689e-02, -3.9361e-02,  ...,  1.1880e-01,\n",
            "          -1.2016e-01, -3.6590e-02],\n",
            "         [ 7.3161e-02, -5.8689e-02, -3.9361e-02,  ...,  1.1880e-01,\n",
            "          -1.2016e-01, -3.6590e-02]],\n",
            "\n",
            "        [[ 4.8655e-02,  3.8460e-02, -1.8155e-01,  ..., -1.7752e-01,\n",
            "          -6.3934e-02, -2.7056e-02],\n",
            "         [ 6.1638e-02,  3.6657e-02, -1.5740e-01,  ..., -1.4103e-01,\n",
            "          -5.0004e-02,  1.3214e-02],\n",
            "         [ 7.0552e-02,  7.7873e-02, -1.3536e-01,  ..., -1.7007e-01,\n",
            "          -6.3760e-02,  3.3082e-02],\n",
            "         ...,\n",
            "         [ 5.3341e-02,  2.6794e-02, -1.2268e-01,  ..., -1.3039e-01,\n",
            "          -5.7728e-02, -1.5343e-02],\n",
            "         [ 5.3342e-02,  2.6794e-02, -1.2268e-01,  ..., -1.3039e-01,\n",
            "          -5.7728e-02, -1.5343e-02],\n",
            "         [ 5.3342e-02,  2.6794e-02, -1.2268e-01,  ..., -1.3039e-01,\n",
            "          -5.7728e-02, -1.5343e-02]]], grad_fn=<ViewBackward0>)\n",
            "torch.Size([10, 20, 512])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OTku1fySVR3L"
      },
      "source": [],
      "execution_count": 20,
      "outputs": []
    }
  ]
}