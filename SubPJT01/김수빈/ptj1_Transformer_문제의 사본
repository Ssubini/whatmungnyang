{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1phZ7bP-RrVIHU9AG5fTPTo2TMbZ-EMac","timestamp":1661993716957},{"file_id":"1XNOcAyN3Q9KN6-9oA1t6Ueh37tJYQFGa","timestamp":1658069359050}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","source":["# Transformer"],"metadata":{"id":"GiWulKAdmAHX"}},{"cell_type":"markdown","metadata":{"id":"9KsBGZpKkWki"},"source":["Transformer의 핵심 구조인 Multi-head Attention을 구현하는 실습입니다.\n","1. Multi-head attention 및 self-attention 구현.\n","2. 각 과정에서 일어나는 연산과 input/output 형태 이해."]},{"cell_type":"markdown","metadata":{"id":"8qRU5DFY2OM8"},"source":["### 필요 패키지 import"]},{"cell_type":"code","metadata":{"id":"lDtMioSQQ1bB","executionInfo":{"status":"ok","timestamp":1661993752481,"user_tz":-540,"elapsed":2314,"user":{"displayName":"김수빈","userId":"13390164251685497519"}}},"source":["from torch import nn\n","from torch.nn import functional as F\n","from tqdm import tqdm\n","\n","import torch\n","import math"],"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":["## Req. 2-1 Multi-head self-attention 구조 익히기"],"metadata":{"id":"HH0VdC4uJJVG"}},{"cell_type":"markdown","metadata":{"id":"QBiZObgRep_Q"},"source":["### **데이터 전처리**\n","vocab_size 100인 가상의 시퀀스 데이터를 생성합니다. \n","\n","각 데이터에 할당된 숫자는 tokenizing과 정수화가 이뤄진 형태입니다."]},{"cell_type":"code","metadata":{"id":"e9ULZIqTenSc","executionInfo":{"status":"ok","timestamp":1661993950566,"user_tz":-540,"elapsed":367,"user":{"displayName":"김수빈","userId":"13390164251685497519"}}},"source":["pad_id = 0\n","vocab_size = 100\n","\n","data = [\n","  [62, 13, 47, 39, 78, 33, 56, 13, 39, 29, 44, 86, 71, 36, 18, 75],\n","  [60, 96, 51, 32, 90],\n","  [35, 45, 48, 65, 91, 99, 92, 10, 3, 21, 54],\n","  [75, 51],\n","  [66, 88, 98, 47],\n","  [21, 39, 10, 64, 21],\n","  [98],\n","  [77, 65, 51, 77, 19, 15, 35, 19, 23, 97, 50, 46, 53, 42, 45, 91, 66, 3, 43, 10],\n","  [70, 64, 98, 25, 99, 53, 4, 13, 69, 62, 66, 76, 15, 75, 45, 34],\n","  [20, 64, 81, 35, 76, 85, 1, 62, 8, 45, 99, 77, 19, 43]\n","]"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"6Hx3mcivgMyH","executionInfo":{"status":"ok","timestamp":1661993953594,"user_tz":-540,"elapsed":568,"user":{"displayName":"김수빈","userId":"13390164251685497519"}}},"source":["# 길이 맞춰주기 위해 패딩합니다.\n","def padding(data):\n","  max_len = len(max(data, key=len))\n","  print(f\"Maximum sequence length: {max_len}\")\n","\n","  for i, seq in enumerate(tqdm(data)):\n","    if len(seq) < max_len:\n","      data[i] = seq + [pad_id] * (max_len - len(seq))\n","\n","  return data, max_len"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"s3e8FiNvgX60","colab":{"base_uri":"https://localhost:8080/"},"outputId":"07af1a11-2421-4dad-bde7-a0c0d161ddde","executionInfo":{"status":"ok","timestamp":1661993974275,"user_tz":-540,"elapsed":379,"user":{"displayName":"김수빈","userId":"13390164251685497519"}}},"source":["data, max_len = padding(data)"],"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Maximum sequence length: 20\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 10/10 [00:00<00:00, 80815.11it/s]\n"]}]},{"cell_type":"code","metadata":{"id":"hwPSIWYugaN0","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1661994015924,"user_tz":-540,"elapsed":365,"user":{"displayName":"김수빈","userId":"13390164251685497519"}},"outputId":"9f6208be-09ee-4f0f-ceb3-b2183d703c3f"},"source":["data"],"execution_count":5,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[[62, 13, 47, 39, 78, 33, 56, 13, 39, 29, 44, 86, 71, 36, 18, 75, 0, 0, 0, 0],\n"," [60, 96, 51, 32, 90, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n"," [35, 45, 48, 65, 91, 99, 92, 10, 3, 21, 54, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n"," [75, 51, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n"," [66, 88, 98, 47, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n"," [21, 39, 10, 64, 21, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n"," [98, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n"," [77,\n","  65,\n","  51,\n","  77,\n","  19,\n","  15,\n","  35,\n","  19,\n","  23,\n","  97,\n","  50,\n","  46,\n","  53,\n","  42,\n","  45,\n","  91,\n","  66,\n","  3,\n","  43,\n","  10],\n"," [70, 64, 98, 25, 99, 53, 4, 13, 69, 62, 66, 76, 15, 75, 45, 34, 0, 0, 0, 0],\n"," [20, 64, 81, 35, 76, 85, 1, 62, 8, 45, 99, 77, 19, 43, 0, 0, 0, 0, 0, 0]]"]},"metadata":{},"execution_count":5}]},{"cell_type":"markdown","metadata":{"id":"rwqjACx8iidc"},"source":["### Hyperparameter 세팅 및 embedding"]},{"cell_type":"code","metadata":{"id":"p-Ngp2nWimS8","executionInfo":{"status":"ok","timestamp":1661994039040,"user_tz":-540,"elapsed":363,"user":{"displayName":"김수빈","userId":"13390164251685497519"}}},"source":["d_model = 512  # model의 hidden size\n","num_heads = 8  # head의 개수\n","\n","# d_model이 입력을 projection 시킬 임베딩 space의 차원이므로, num_heads로 나누어 떨어져야 한다."],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"GJMi2Xsni5uq","executionInfo":{"status":"ok","timestamp":1661994049733,"user_tz":-540,"elapsed":360,"user":{"displayName":"김수빈","userId":"13390164251685497519"}}},"source":["embedding = nn.Embedding(vocab_size, d_model)\n","\n","# B: batch size, L: maximum sequence length\n","batch = torch.LongTensor(data)  # (B, L)\n","batch_emb = embedding(batch)  # (B, L, d_model)"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"3tLCUQwojcUb","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1661994052780,"user_tz":-540,"elapsed":818,"user":{"displayName":"김수빈","userId":"13390164251685497519"}},"outputId":"c14778b5-af80-4881-b23c-3774146201f1"},"source":["print(batch_emb)\n","print(batch_emb.shape)"],"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[[-0.7112,  0.5041, -1.9169,  ...,  0.0260,  0.0648,  1.2913],\n","         [-1.5235, -0.5574, -0.0207,  ...,  1.3811, -0.3858,  0.2073],\n","         [-0.6426,  1.0851, -0.5916,  ..., -0.1659, -0.6493, -2.2361],\n","         ...,\n","         [-1.5994,  0.5202, -0.2694,  ...,  1.2810, -0.3965, -1.5195],\n","         [-1.5994,  0.5202, -0.2694,  ...,  1.2810, -0.3965, -1.5195],\n","         [-1.5994,  0.5202, -0.2694,  ...,  1.2810, -0.3965, -1.5195]],\n","\n","        [[ 0.3532,  0.1166,  1.9870,  ..., -0.8638,  0.3632, -0.6996],\n","         [ 1.5848, -0.2167,  1.1560,  ...,  0.6158,  1.2497, -1.0074],\n","         [ 1.0990, -0.7479, -1.1021,  ..., -0.6862,  0.3942,  1.0683],\n","         ...,\n","         [-1.5994,  0.5202, -0.2694,  ...,  1.2810, -0.3965, -1.5195],\n","         [-1.5994,  0.5202, -0.2694,  ...,  1.2810, -0.3965, -1.5195],\n","         [-1.5994,  0.5202, -0.2694,  ...,  1.2810, -0.3965, -1.5195]],\n","\n","        [[-1.4621,  0.6372,  1.3109,  ...,  0.1708, -2.0637,  0.1434],\n","         [-1.6542, -1.9025, -0.6079,  ...,  0.9609,  0.2154,  0.2940],\n","         [ 1.0144,  0.9360, -1.5094,  ...,  0.0872, -0.8235, -1.0351],\n","         ...,\n","         [-1.5994,  0.5202, -0.2694,  ...,  1.2810, -0.3965, -1.5195],\n","         [-1.5994,  0.5202, -0.2694,  ...,  1.2810, -0.3965, -1.5195],\n","         [-1.5994,  0.5202, -0.2694,  ...,  1.2810, -0.3965, -1.5195]],\n","\n","        ...,\n","\n","        [[-1.0051, -0.6381,  0.5381,  ...,  0.4877,  0.2388,  1.6795],\n","         [-0.8626,  0.9560, -0.9844,  ..., -1.2850, -1.4368,  1.3196],\n","         [ 1.0990, -0.7479, -1.1021,  ..., -0.6862,  0.3942,  1.0683],\n","         ...,\n","         [ 0.4138, -1.7403,  0.8979,  ...,  0.2454, -0.5311,  0.8896],\n","         [ 2.4805,  0.3392, -0.3227,  ...,  1.1526, -0.0395, -0.0292],\n","         [-1.3957,  0.7182,  0.3122,  ..., -0.5036,  1.3959,  0.0654]],\n","\n","        [[ 1.7727, -1.4602, -0.6349,  ...,  1.2482, -1.2791, -0.7706],\n","         [-0.9894,  0.1953,  1.6850,  ...,  0.0107, -0.2286, -0.9275],\n","         [-0.8390, -1.0703,  0.4341,  ..., -0.9540, -2.2486,  1.3451],\n","         ...,\n","         [-1.5994,  0.5202, -0.2694,  ...,  1.2810, -0.3965, -1.5195],\n","         [-1.5994,  0.5202, -0.2694,  ...,  1.2810, -0.3965, -1.5195],\n","         [-1.5994,  0.5202, -0.2694,  ...,  1.2810, -0.3965, -1.5195]],\n","\n","        [[-0.1772,  0.1178,  0.4804,  ...,  1.2639,  1.2609, -0.8184],\n","         [-0.9894,  0.1953,  1.6850,  ...,  0.0107, -0.2286, -0.9275],\n","         [ 0.3714,  1.1855,  0.4966,  ...,  0.2445, -1.7805, -0.1546],\n","         ...,\n","         [-1.5994,  0.5202, -0.2694,  ...,  1.2810, -0.3965, -1.5195],\n","         [-1.5994,  0.5202, -0.2694,  ...,  1.2810, -0.3965, -1.5195],\n","         [-1.5994,  0.5202, -0.2694,  ...,  1.2810, -0.3965, -1.5195]]],\n","       grad_fn=<EmbeddingBackward0>)\n","torch.Size([10, 20, 512])\n"]}]},{"cell_type":"markdown","metadata":{"id":"s0Lhx892gmi3"},"source":["### Linear projection & 여러 head로 나누기"]},{"cell_type":"markdown","metadata":{"id":"urXMBRnRgqvw"},"source":["Multi-head attention 내에서 쓰이는 linear projection matrix들을 정의합니다."]},{"cell_type":"code","metadata":{"id":"9DWKDqgCgfMk","executionInfo":{"status":"ok","timestamp":1661994312436,"user_tz":-540,"elapsed":542,"user":{"displayName":"김수빈","userId":"13390164251685497519"}}},"source":["w_q = nn.Linear(d_model, d_model)\n","w_k = nn.Linear(d_model, d_model)\n","w_v = nn.Linear(d_model, d_model)"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"id":"tcLuhda7m-Lm","executionInfo":{"status":"ok","timestamp":1661994315820,"user_tz":-540,"elapsed":469,"user":{"displayName":"김수빈","userId":"13390164251685497519"}}},"source":["w_0 = nn.Linear(d_model, d_model)"],"execution_count":10,"outputs":[]},{"cell_type":"code","metadata":{"id":"H-vSL7PwnV6k","colab":{"base_uri":"https://localhost:8080/"},"outputId":"2116865f-4be8-42a3-dc09-ad4667098994","executionInfo":{"status":"ok","timestamp":1661994319691,"user_tz":-540,"elapsed":3,"user":{"displayName":"김수빈","userId":"13390164251685497519"}}},"source":["q = w_q(batch_emb)  # (B, L, d_model)\n","k = w_k(batch_emb)  # (B, L, d_model)\n","v = w_v(batch_emb)  # (B, L, d_model)\n","\n","print(q.shape)\n","print(k.shape)\n","print(v.shape)"],"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([10, 20, 512])\n","torch.Size([10, 20, 512])\n","torch.Size([10, 20, 512])\n"]}]},{"cell_type":"markdown","metadata":{"id":"Wnvlum-LnF1T"},"source":["Q, k, v를 `num_head`개의 차원 분할된 여러 vector로 만듭니다."]},{"cell_type":"markdown","metadata":{"id":"sXcYLZYvJT_1"},"source":["- 이론적으로는 multi-head attention을 수행하면 input을 각각 다른 head 개수만큼의 Wq, Wk, Wv로 linear transformation 해서 각각 여러번의 attention 수행한 후 concat 한 후 linear transformation 수행해준다\n","- 구현에서는 Wq, Wk, Wv 한 개씩\n","- 실제 `attention is all you need` 논문의 구현 예시는 Query vector 한개를 dim으로 쪼개서 진행한다"]},{"cell_type":"code","metadata":{"id":"_tiOKAv9nEli","colab":{"base_uri":"https://localhost:8080/"},"outputId":"f2cf848e-9b75-47a9-e36a-8b38250536d3","executionInfo":{"status":"ok","timestamp":1661994557254,"user_tz":-540,"elapsed":348,"user":{"displayName":"김수빈","userId":"13390164251685497519"}}},"source":["batch_size = q.shape[0]\n","d_k = d_model // num_heads\n","\n","# num_heads * d_k로 쪼갠다\n","q = q.view(batch_size, -1, num_heads, d_k)  # (B, L, num_heads, d_k)\n","k = k.view(batch_size, -1, num_heads, d_k)  # (B, L, num_heads, d_k)\n","v = v.view(batch_size, -1, num_heads, d_k)  # (B, L, num_heads, d_k)\n","\n","print(q.shape)\n","print(k.shape)\n","print(v.shape)"],"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([10, 20, 8, 64])\n","torch.Size([10, 20, 8, 64])\n","torch.Size([10, 20, 8, 64])\n"]}]},{"cell_type":"code","metadata":{"id":"5tNb2isfn5Cx","colab":{"base_uri":"https://localhost:8080/"},"outputId":"29abdd30-ba29-4aa7-f044-7fe6b2ab6a7d","executionInfo":{"status":"ok","timestamp":1661994562081,"user_tz":-540,"elapsed":369,"user":{"displayName":"김수빈","userId":"13390164251685497519"}}},"source":["# num_heads를 밖으로 뺌으로써\n","# 각 head가 (L, d_k) 만큼의 matrix를 가지고 self-attention 수행\n","\n","q = q.transpose(1, 2)  # (B, num_heads, L, d_k)\n","k = k.transpose(1, 2)  # (B, num_heads, L, d_k)\n","v = v.transpose(1, 2)  # (B, num_heads, L, d_k)\n","\n","print(q.shape)\n","print(k.shape)\n","print(v.shape)"],"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([10, 8, 20, 64])\n","torch.Size([10, 8, 20, 64])\n","torch.Size([10, 8, 20, 64])\n"]}]},{"cell_type":"markdown","metadata":{"id":"NWrDA5_Sofad"},"source":["### Scaled dot-product self-attention 구현"]},{"cell_type":"markdown","metadata":{"id":"w52C4k3Wfl8m"},"source":["각 head에서 실행되는 self-attetion 과정입니다."]},{"cell_type":"code","metadata":{"id":"A5waKr0Hfi2K","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1661994741903,"user_tz":-540,"elapsed":363,"user":{"displayName":"김수빈","userId":"13390164251685497519"}},"outputId":"40a399a2-8b82-4ec5-a1fa-acee12e70551"},"source":["# shape - (L, L)\n","# 같은 sequence 내에 서로 다른 token들에게 얼마나 가중치를 두고 attention을 해야하는가\n","attn_scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(d_k)  # (B, num_heads, L, L)\n","# softmax - row-wise이기 때문에 dim은 -1\n","attn_dists = F.softmax(attn_scores, dim=-1)  # (B, num_heads, L, L)\n","\n","print(attn_dists)\n","print(attn_dists.shape)"],"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[[[0.0698, 0.0551, 0.0572,  ..., 0.0418, 0.0418, 0.0418],\n","          [0.0373, 0.0280, 0.0818,  ..., 0.0624, 0.0624, 0.0624],\n","          [0.0596, 0.0595, 0.0379,  ..., 0.0550, 0.0550, 0.0550],\n","          ...,\n","          [0.0279, 0.0186, 0.0929,  ..., 0.0590, 0.0590, 0.0590],\n","          [0.0279, 0.0186, 0.0929,  ..., 0.0590, 0.0590, 0.0590],\n","          [0.0279, 0.0186, 0.0929,  ..., 0.0590, 0.0590, 0.0590]],\n","\n","         [[0.0378, 0.0712, 0.0488,  ..., 0.0412, 0.0412, 0.0412],\n","          [0.0419, 0.0408, 0.0513,  ..., 0.0378, 0.0378, 0.0378],\n","          [0.0732, 0.0961, 0.0320,  ..., 0.0340, 0.0340, 0.0340],\n","          ...,\n","          [0.0484, 0.0610, 0.0406,  ..., 0.0455, 0.0455, 0.0455],\n","          [0.0484, 0.0610, 0.0406,  ..., 0.0455, 0.0455, 0.0455],\n","          [0.0484, 0.0610, 0.0406,  ..., 0.0455, 0.0455, 0.0455]],\n","\n","         [[0.0684, 0.0430, 0.0975,  ..., 0.0230, 0.0230, 0.0230],\n","          [0.0386, 0.0460, 0.0496,  ..., 0.0473, 0.0473, 0.0473],\n","          [0.1051, 0.0338, 0.0398,  ..., 0.0512, 0.0512, 0.0512],\n","          ...,\n","          [0.0692, 0.0321, 0.0895,  ..., 0.0537, 0.0537, 0.0537],\n","          [0.0692, 0.0321, 0.0895,  ..., 0.0537, 0.0537, 0.0537],\n","          [0.0692, 0.0321, 0.0895,  ..., 0.0537, 0.0537, 0.0537]],\n","\n","         ...,\n","\n","         [[0.0455, 0.0508, 0.0402,  ..., 0.0590, 0.0590, 0.0590],\n","          [0.0814, 0.0290, 0.0319,  ..., 0.0674, 0.0674, 0.0674],\n","          [0.0349, 0.0587, 0.0514,  ..., 0.0417, 0.0417, 0.0417],\n","          ...,\n","          [0.0652, 0.0405, 0.0530,  ..., 0.0572, 0.0572, 0.0572],\n","          [0.0652, 0.0405, 0.0530,  ..., 0.0572, 0.0572, 0.0572],\n","          [0.0652, 0.0405, 0.0530,  ..., 0.0572, 0.0572, 0.0572]],\n","\n","         [[0.0487, 0.0595, 0.0476,  ..., 0.0415, 0.0415, 0.0415],\n","          [0.0290, 0.0504, 0.0307,  ..., 0.0412, 0.0412, 0.0412],\n","          [0.0282, 0.0511, 0.0315,  ..., 0.0356, 0.0356, 0.0356],\n","          ...,\n","          [0.0492, 0.0427, 0.0641,  ..., 0.0474, 0.0474, 0.0474],\n","          [0.0492, 0.0427, 0.0641,  ..., 0.0474, 0.0474, 0.0474],\n","          [0.0492, 0.0427, 0.0641,  ..., 0.0474, 0.0474, 0.0474]],\n","\n","         [[0.0393, 0.0570, 0.0206,  ..., 0.0848, 0.0848, 0.0848],\n","          [0.0670, 0.0320, 0.0958,  ..., 0.0398, 0.0398, 0.0398],\n","          [0.0573, 0.0747, 0.0200,  ..., 0.0373, 0.0373, 0.0373],\n","          ...,\n","          [0.0662, 0.0547, 0.0562,  ..., 0.0554, 0.0554, 0.0554],\n","          [0.0662, 0.0547, 0.0562,  ..., 0.0554, 0.0554, 0.0554],\n","          [0.0662, 0.0547, 0.0562,  ..., 0.0554, 0.0554, 0.0554]]],\n","\n","\n","        [[[0.0448, 0.0962, 0.0321,  ..., 0.0481, 0.0481, 0.0481],\n","          [0.1054, 0.0670, 0.0658,  ..., 0.0393, 0.0393, 0.0393],\n","          [0.0568, 0.0531, 0.0555,  ..., 0.0481, 0.0481, 0.0481],\n","          ...,\n","          [0.0448, 0.0470, 0.0608,  ..., 0.0522, 0.0522, 0.0522],\n","          [0.0448, 0.0470, 0.0608,  ..., 0.0522, 0.0522, 0.0522],\n","          [0.0448, 0.0470, 0.0608,  ..., 0.0522, 0.0522, 0.0522]],\n","\n","         [[0.0374, 0.0577, 0.0307,  ..., 0.0518, 0.0518, 0.0518],\n","          [0.0631, 0.0350, 0.0617,  ..., 0.0501, 0.0501, 0.0501],\n","          [0.0294, 0.0439, 0.0783,  ..., 0.0488, 0.0488, 0.0488],\n","          ...,\n","          [0.0425, 0.0480, 0.0443,  ..., 0.0514, 0.0514, 0.0514],\n","          [0.0425, 0.0480, 0.0443,  ..., 0.0514, 0.0514, 0.0514],\n","          [0.0425, 0.0480, 0.0443,  ..., 0.0514, 0.0514, 0.0514]],\n","\n","         [[0.0437, 0.0334, 0.0391,  ..., 0.0519, 0.0519, 0.0519],\n","          [0.0488, 0.0814, 0.0542,  ..., 0.0440, 0.0440, 0.0440],\n","          [0.0501, 0.0318, 0.0825,  ..., 0.0447, 0.0447, 0.0447],\n","          ...,\n","          [0.0575, 0.0708, 0.0226,  ..., 0.0474, 0.0474, 0.0474],\n","          [0.0575, 0.0708, 0.0226,  ..., 0.0474, 0.0474, 0.0474],\n","          [0.0575, 0.0708, 0.0226,  ..., 0.0474, 0.0474, 0.0474]],\n","\n","         ...,\n","\n","         [[0.0469, 0.0750, 0.1087,  ..., 0.0356, 0.0356, 0.0356],\n","          [0.0293, 0.0446, 0.0575,  ..., 0.0501, 0.0501, 0.0501],\n","          [0.0835, 0.0643, 0.0798,  ..., 0.0411, 0.0411, 0.0411],\n","          ...,\n","          [0.0523, 0.0527, 0.0464,  ..., 0.0517, 0.0517, 0.0517],\n","          [0.0523, 0.0527, 0.0464,  ..., 0.0517, 0.0517, 0.0517],\n","          [0.0523, 0.0527, 0.0464,  ..., 0.0517, 0.0517, 0.0517]],\n","\n","         [[0.0727, 0.0450, 0.0565,  ..., 0.0458, 0.0458, 0.0458],\n","          [0.0398, 0.0503, 0.0465,  ..., 0.0479, 0.0479, 0.0479],\n","          [0.0383, 0.0697, 0.0498,  ..., 0.0496, 0.0496, 0.0496],\n","          ...,\n","          [0.0709, 0.0489, 0.0332,  ..., 0.0503, 0.0503, 0.0503],\n","          [0.0709, 0.0489, 0.0332,  ..., 0.0503, 0.0503, 0.0503],\n","          [0.0709, 0.0489, 0.0332,  ..., 0.0503, 0.0503, 0.0503]],\n","\n","         [[0.0337, 0.0271, 0.0436,  ..., 0.0531, 0.0531, 0.0531],\n","          [0.0715, 0.0451, 0.0586,  ..., 0.0476, 0.0476, 0.0476],\n","          [0.0418, 0.0292, 0.0200,  ..., 0.0549, 0.0549, 0.0549],\n","          ...,\n","          [0.0422, 0.0407, 0.0562,  ..., 0.0505, 0.0505, 0.0505],\n","          [0.0422, 0.0407, 0.0562,  ..., 0.0505, 0.0505, 0.0505],\n","          [0.0422, 0.0407, 0.0562,  ..., 0.0505, 0.0505, 0.0505]]],\n","\n","\n","        [[[0.0545, 0.0579, 0.0308,  ..., 0.0556, 0.0556, 0.0556],\n","          [0.0496, 0.0766, 0.0332,  ..., 0.0478, 0.0478, 0.0478],\n","          [0.0369, 0.0302, 0.0854,  ..., 0.0601, 0.0601, 0.0601],\n","          ...,\n","          [0.0433, 0.0493, 0.0197,  ..., 0.0547, 0.0547, 0.0547],\n","          [0.0433, 0.0493, 0.0197,  ..., 0.0547, 0.0547, 0.0547],\n","          [0.0433, 0.0493, 0.0197,  ..., 0.0547, 0.0547, 0.0547]],\n","\n","         [[0.0601, 0.0586, 0.0526,  ..., 0.0479, 0.0479, 0.0479],\n","          [0.0245, 0.0489, 0.0598,  ..., 0.0527, 0.0527, 0.0527],\n","          [0.0470, 0.0857, 0.0404,  ..., 0.0396, 0.0396, 0.0396],\n","          ...,\n","          [0.0393, 0.0532, 0.0425,  ..., 0.0515, 0.0515, 0.0515],\n","          [0.0393, 0.0532, 0.0425,  ..., 0.0515, 0.0515, 0.0515],\n","          [0.0393, 0.0532, 0.0425,  ..., 0.0515, 0.0515, 0.0515]],\n","\n","         [[0.0383, 0.0371, 0.0477,  ..., 0.0609, 0.0609, 0.0609],\n","          [0.0285, 0.0324, 0.0574,  ..., 0.0474, 0.0474, 0.0474],\n","          [0.0500, 0.0511, 0.0407,  ..., 0.0461, 0.0461, 0.0461],\n","          ...,\n","          [0.0917, 0.0496, 0.0555,  ..., 0.0472, 0.0472, 0.0472],\n","          [0.0917, 0.0496, 0.0555,  ..., 0.0472, 0.0472, 0.0472],\n","          [0.0917, 0.0496, 0.0555,  ..., 0.0472, 0.0472, 0.0472]],\n","\n","         ...,\n","\n","         [[0.0395, 0.0369, 0.0213,  ..., 0.0566, 0.0566, 0.0566],\n","          [0.0565, 0.0429, 0.0406,  ..., 0.0488, 0.0488, 0.0488],\n","          [0.0484, 0.0645, 0.0524,  ..., 0.0440, 0.0440, 0.0440],\n","          ...,\n","          [0.0392, 0.0365, 0.0558,  ..., 0.0573, 0.0573, 0.0573],\n","          [0.0392, 0.0365, 0.0558,  ..., 0.0573, 0.0573, 0.0573],\n","          [0.0392, 0.0365, 0.0558,  ..., 0.0573, 0.0573, 0.0573]],\n","\n","         [[0.0357, 0.0261, 0.0545,  ..., 0.0693, 0.0693, 0.0693],\n","          [0.0382, 0.0758, 0.0609,  ..., 0.0443, 0.0443, 0.0443],\n","          [0.0500, 0.0325, 0.0283,  ..., 0.0574, 0.0574, 0.0574],\n","          ...,\n","          [0.0434, 0.0376, 0.0712,  ..., 0.0478, 0.0478, 0.0478],\n","          [0.0434, 0.0376, 0.0712,  ..., 0.0478, 0.0478, 0.0478],\n","          [0.0434, 0.0376, 0.0712,  ..., 0.0478, 0.0478, 0.0478]],\n","\n","         [[0.0507, 0.0364, 0.0567,  ..., 0.0550, 0.0550, 0.0550],\n","          [0.0499, 0.0409, 0.0490,  ..., 0.0474, 0.0474, 0.0474],\n","          [0.0527, 0.0399, 0.0517,  ..., 0.0581, 0.0581, 0.0581],\n","          ...,\n","          [0.0345, 0.0666, 0.0350,  ..., 0.0470, 0.0470, 0.0470],\n","          [0.0345, 0.0666, 0.0350,  ..., 0.0470, 0.0470, 0.0470],\n","          [0.0345, 0.0666, 0.0350,  ..., 0.0470, 0.0470, 0.0470]]],\n","\n","\n","        ...,\n","\n","\n","        [[[0.0390, 0.0451, 0.0542,  ..., 0.0394, 0.0783, 0.0407],\n","          [0.0898, 0.0362, 0.0636,  ..., 0.0338, 0.0478, 0.0692],\n","          [0.0445, 0.0423, 0.0665,  ..., 0.0760, 0.0402, 0.0387],\n","          ...,\n","          [0.0495, 0.0356, 0.1048,  ..., 0.0765, 0.0600, 0.0488],\n","          [0.0584, 0.0618, 0.0317,  ..., 0.0296, 0.0365, 0.0496],\n","          [0.0554, 0.0449, 0.0259,  ..., 0.0515, 0.0628, 0.0387]],\n","\n","         [[0.0637, 0.0618, 0.0348,  ..., 0.0971, 0.0576, 0.0418],\n","          [0.0579, 0.0492, 0.0534,  ..., 0.0546, 0.0320, 0.0286],\n","          [0.0599, 0.0305, 0.0680,  ..., 0.0299, 0.0610, 0.0625],\n","          ...,\n","          [0.0494, 0.0635, 0.0352,  ..., 0.0811, 0.0837, 0.0533],\n","          [0.0571, 0.0326, 0.0742,  ..., 0.0497, 0.0313, 0.0366],\n","          [0.0359, 0.0400, 0.0633,  ..., 0.0421, 0.0385, 0.0315]],\n","\n","         [[0.0241, 0.0506, 0.0709,  ..., 0.1161, 0.0307, 0.0592],\n","          [0.0644, 0.0430, 0.0431,  ..., 0.0757, 0.0594, 0.0483],\n","          [0.0693, 0.0575, 0.0596,  ..., 0.0494, 0.0465, 0.0762],\n","          ...,\n","          [0.0495, 0.0426, 0.0332,  ..., 0.0453, 0.0426, 0.0719],\n","          [0.0793, 0.0354, 0.0422,  ..., 0.0477, 0.0706, 0.0291],\n","          [0.0391, 0.0317, 0.0672,  ..., 0.0408, 0.0431, 0.0559]],\n","\n","         ...,\n","\n","         [[0.0304, 0.0496, 0.0449,  ..., 0.0646, 0.0511, 0.0310],\n","          [0.0497, 0.0565, 0.0293,  ..., 0.0924, 0.0560, 0.0590],\n","          [0.0715, 0.0729, 0.0562,  ..., 0.0555, 0.0319, 0.0518],\n","          ...,\n","          [0.0314, 0.0463, 0.0626,  ..., 0.0644, 0.0987, 0.0189],\n","          [0.0633, 0.0527, 0.0226,  ..., 0.0501, 0.0371, 0.0602],\n","          [0.0376, 0.0700, 0.0409,  ..., 0.0577, 0.0572, 0.0434]],\n","\n","         [[0.0472, 0.0543, 0.0685,  ..., 0.0532, 0.0389, 0.0689],\n","          [0.0463, 0.0507, 0.0738,  ..., 0.0469, 0.0451, 0.0452],\n","          [0.0819, 0.0609, 0.0494,  ..., 0.0560, 0.0404, 0.0412],\n","          ...,\n","          [0.0639, 0.0670, 0.0325,  ..., 0.0449, 0.0361, 0.0639],\n","          [0.0630, 0.0409, 0.1068,  ..., 0.0264, 0.0448, 0.0413],\n","          [0.0364, 0.0685, 0.0351,  ..., 0.0544, 0.0266, 0.0559]],\n","\n","         [[0.0370, 0.0631, 0.0298,  ..., 0.0530, 0.0433, 0.0794],\n","          [0.0424, 0.0407, 0.0421,  ..., 0.0526, 0.0360, 0.0516],\n","          [0.0675, 0.0767, 0.0254,  ..., 0.0532, 0.0331, 0.0285],\n","          ...,\n","          [0.0532, 0.0512, 0.0369,  ..., 0.0808, 0.0211, 0.0535],\n","          [0.0415, 0.0693, 0.0624,  ..., 0.0521, 0.0261, 0.0528],\n","          [0.0467, 0.0532, 0.0485,  ..., 0.0346, 0.0355, 0.0626]]],\n","\n","\n","        [[[0.0405, 0.0752, 0.0359,  ..., 0.0466, 0.0466, 0.0466],\n","          [0.0285, 0.0179, 0.0389,  ..., 0.0671, 0.0671, 0.0671],\n","          [0.0520, 0.0531, 0.0448,  ..., 0.0540, 0.0540, 0.0540],\n","          ...,\n","          [0.0431, 0.0671, 0.0386,  ..., 0.0603, 0.0603, 0.0603],\n","          [0.0431, 0.0671, 0.0386,  ..., 0.0603, 0.0603, 0.0603],\n","          [0.0431, 0.0671, 0.0386,  ..., 0.0603, 0.0603, 0.0603]],\n","\n","         [[0.0379, 0.0942, 0.0385,  ..., 0.0609, 0.0609, 0.0609],\n","          [0.0450, 0.0718, 0.0641,  ..., 0.0482, 0.0482, 0.0482],\n","          [0.0210, 0.0536, 0.0853,  ..., 0.0476, 0.0476, 0.0476],\n","          ...,\n","          [0.0319, 0.0714, 0.0253,  ..., 0.0467, 0.0467, 0.0467],\n","          [0.0319, 0.0714, 0.0253,  ..., 0.0467, 0.0467, 0.0467],\n","          [0.0319, 0.0714, 0.0253,  ..., 0.0467, 0.0467, 0.0467]],\n","\n","         [[0.0739, 0.0373, 0.0727,  ..., 0.0541, 0.0541, 0.0541],\n","          [0.0412, 0.0320, 0.0303,  ..., 0.0583, 0.0583, 0.0583],\n","          [0.0252, 0.0934, 0.0453,  ..., 0.0418, 0.0418, 0.0418],\n","          ...,\n","          [0.0519, 0.0426, 0.0801,  ..., 0.0484, 0.0484, 0.0484],\n","          [0.0519, 0.0426, 0.0801,  ..., 0.0484, 0.0484, 0.0484],\n","          [0.0519, 0.0426, 0.0801,  ..., 0.0484, 0.0484, 0.0484]],\n","\n","         ...,\n","\n","         [[0.0381, 0.0404, 0.0725,  ..., 0.0435, 0.0435, 0.0435],\n","          [0.0486, 0.0347, 0.0446,  ..., 0.0547, 0.0547, 0.0547],\n","          [0.0319, 0.0805, 0.0594,  ..., 0.0553, 0.0553, 0.0553],\n","          ...,\n","          [0.0360, 0.0738, 0.0462,  ..., 0.0550, 0.0550, 0.0550],\n","          [0.0360, 0.0738, 0.0462,  ..., 0.0550, 0.0550, 0.0550],\n","          [0.0360, 0.0738, 0.0462,  ..., 0.0550, 0.0550, 0.0550]],\n","\n","         [[0.0972, 0.0205, 0.0555,  ..., 0.0520, 0.0520, 0.0520],\n","          [0.0627, 0.0464, 0.0313,  ..., 0.0418, 0.0418, 0.0418],\n","          [0.0468, 0.0613, 0.0546,  ..., 0.0380, 0.0380, 0.0380],\n","          ...,\n","          [0.0673, 0.0692, 0.0527,  ..., 0.0489, 0.0489, 0.0489],\n","          [0.0673, 0.0692, 0.0527,  ..., 0.0489, 0.0489, 0.0489],\n","          [0.0673, 0.0692, 0.0527,  ..., 0.0489, 0.0489, 0.0489]],\n","\n","         [[0.0301, 0.0506, 0.0473,  ..., 0.0395, 0.0395, 0.0395],\n","          [0.0371, 0.0524, 0.0360,  ..., 0.0641, 0.0641, 0.0641],\n","          [0.0338, 0.0508, 0.0298,  ..., 0.0497, 0.0497, 0.0497],\n","          ...,\n","          [0.0226, 0.0346, 0.0711,  ..., 0.0555, 0.0555, 0.0555],\n","          [0.0226, 0.0346, 0.0711,  ..., 0.0555, 0.0555, 0.0555],\n","          [0.0226, 0.0346, 0.0711,  ..., 0.0555, 0.0555, 0.0555]]],\n","\n","\n","        [[[0.0359, 0.0194, 0.0494,  ..., 0.0686, 0.0686, 0.0686],\n","          [0.0423, 0.0182, 0.0827,  ..., 0.0682, 0.0682, 0.0682],\n","          [0.0709, 0.0486, 0.1014,  ..., 0.0456, 0.0456, 0.0456],\n","          ...,\n","          [0.0439, 0.0601, 0.0269,  ..., 0.0540, 0.0540, 0.0540],\n","          [0.0439, 0.0601, 0.0269,  ..., 0.0540, 0.0540, 0.0540],\n","          [0.0439, 0.0601, 0.0269,  ..., 0.0540, 0.0540, 0.0540]],\n","\n","         [[0.0475, 0.0419, 0.0398,  ..., 0.0379, 0.0379, 0.0379],\n","          [0.0410, 0.0717, 0.0374,  ..., 0.0481, 0.0481, 0.0481],\n","          [0.0748, 0.0602, 0.0320,  ..., 0.0397, 0.0397, 0.0397],\n","          ...,\n","          [0.0785, 0.0654, 0.0526,  ..., 0.0427, 0.0427, 0.0427],\n","          [0.0785, 0.0654, 0.0526,  ..., 0.0427, 0.0427, 0.0427],\n","          [0.0785, 0.0654, 0.0526,  ..., 0.0427, 0.0427, 0.0427]],\n","\n","         [[0.0160, 0.0250, 0.0430,  ..., 0.0742, 0.0742, 0.0742],\n","          [0.0603, 0.0264, 0.0516,  ..., 0.0481, 0.0481, 0.0481],\n","          [0.0322, 0.0616, 0.0644,  ..., 0.0473, 0.0473, 0.0473],\n","          ...,\n","          [0.0381, 0.0435, 0.0436,  ..., 0.0494, 0.0494, 0.0494],\n","          [0.0381, 0.0435, 0.0436,  ..., 0.0494, 0.0494, 0.0494],\n","          [0.0381, 0.0435, 0.0436,  ..., 0.0494, 0.0494, 0.0494]],\n","\n","         ...,\n","\n","         [[0.0416, 0.0426, 0.0413,  ..., 0.0557, 0.0557, 0.0557],\n","          [0.0499, 0.0343, 0.0688,  ..., 0.0540, 0.0540, 0.0540],\n","          [0.0587, 0.0376, 0.0430,  ..., 0.0595, 0.0595, 0.0595],\n","          ...,\n","          [0.0414, 0.0700, 0.0222,  ..., 0.0522, 0.0522, 0.0522],\n","          [0.0414, 0.0700, 0.0222,  ..., 0.0522, 0.0522, 0.0522],\n","          [0.0414, 0.0700, 0.0222,  ..., 0.0522, 0.0522, 0.0522]],\n","\n","         [[0.0457, 0.0404, 0.0265,  ..., 0.0557, 0.0557, 0.0557],\n","          [0.0555, 0.0496, 0.0793,  ..., 0.0447, 0.0447, 0.0447],\n","          [0.0351, 0.0291, 0.0360,  ..., 0.0551, 0.0551, 0.0551],\n","          ...,\n","          [0.0338, 0.0637, 0.0867,  ..., 0.0450, 0.0450, 0.0450],\n","          [0.0338, 0.0637, 0.0867,  ..., 0.0450, 0.0450, 0.0450],\n","          [0.0338, 0.0637, 0.0867,  ..., 0.0450, 0.0450, 0.0450]],\n","\n","         [[0.0413, 0.0381, 0.0440,  ..., 0.0694, 0.0694, 0.0694],\n","          [0.0368, 0.0461, 0.0687,  ..., 0.0563, 0.0563, 0.0563],\n","          [0.0685, 0.0376, 0.0565,  ..., 0.0319, 0.0319, 0.0319],\n","          ...,\n","          [0.0531, 0.0307, 0.0624,  ..., 0.0493, 0.0493, 0.0493],\n","          [0.0531, 0.0307, 0.0624,  ..., 0.0493, 0.0493, 0.0493],\n","          [0.0531, 0.0307, 0.0624,  ..., 0.0493, 0.0493, 0.0493]]]],\n","       grad_fn=<SoftmaxBackward0>)\n","torch.Size([10, 8, 20, 20])\n"]}]},{"cell_type":"code","metadata":{"id":"7megouWpgCck","colab":{"base_uri":"https://localhost:8080/"},"outputId":"42371834-84ab-41fe-f752-73426c96a529","executionInfo":{"status":"ok","timestamp":1661994755591,"user_tz":-540,"elapsed":412,"user":{"displayName":"김수빈","userId":"13390164251685497519"}}},"source":["attn_values = torch.matmul(attn_dists, v)  # (B, num_heads, L, d_k)\n","\n","print(attn_values.shape)"],"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([10, 8, 20, 64])\n"]}]},{"cell_type":"markdown","metadata":{"id":"LmSTaymdg-P_"},"source":["### 각 head의 결과물 병합"]},{"cell_type":"markdown","metadata":{"id":"YSdQZCk0hCNd"},"source":["각 head의 결과물을 concat하고 동일 차원으로 linear projection합니다."]},{"cell_type":"code","metadata":{"id":"eaK0bpMGhQZ2","colab":{"base_uri":"https://localhost:8080/"},"outputId":"0cb26a42-bf76-401a-fadd-33de79f730b8","executionInfo":{"status":"ok","timestamp":1661995228698,"user_tz":-540,"elapsed":428,"user":{"displayName":"김수빈","userId":"13390164251685497519"}}},"source":["attn_values = attn_values.transpose(1, 2)  # (B, L, num_heads, d_k)\n","attn_values = attn_values.contiguous().view(batch_size, -1, d_model)  # (B, L, d_model)\n","\n","print(attn_values.shape)"],"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([10, 20, 512])\n"]}]},{"cell_type":"code","metadata":{"id":"LTng_2SXhdH1","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1661995230240,"user_tz":-540,"elapsed":2,"user":{"displayName":"김수빈","userId":"13390164251685497519"}},"outputId":"9a95430b-6b74-4177-d26e-b97a2aff0d0a"},"source":["# w_0 : (d_model, d_model)\n","# 서로 다른 의미로 foucsing 된 각 head의 self-attention 정보들을 합쳐주는 역할 수행\n","outputs = w_0(attn_values)\n","\n","print(outputs)\n","print(outputs.shape)"],"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[[ 0.1229,  0.0424, -0.1839,  ...,  0.1533, -0.1914,  0.0713],\n","         [ 0.1242,  0.0896, -0.1488,  ...,  0.1904, -0.1882,  0.1177],\n","         [ 0.1240,  0.0640, -0.1278,  ...,  0.1041, -0.1799,  0.1308],\n","         ...,\n","         [ 0.1488,  0.0530, -0.1911,  ...,  0.1184, -0.1888,  0.1129],\n","         [ 0.1488,  0.0530, -0.1911,  ...,  0.1184, -0.1888,  0.1129],\n","         [ 0.1488,  0.0530, -0.1911,  ...,  0.1184, -0.1888,  0.1129]],\n","\n","        [[ 0.2737,  0.3290, -0.6661,  ...,  0.0740, -0.2633,  0.0933],\n","         [ 0.2659,  0.3476, -0.6701,  ...,  0.1794, -0.3123,  0.0368],\n","         [ 0.2912,  0.3260, -0.7091,  ...,  0.1564, -0.2516,  0.0575],\n","         ...,\n","         [ 0.3064,  0.3258, -0.7044,  ...,  0.1412, -0.3043,  0.0909],\n","         [ 0.3064,  0.3258, -0.7044,  ...,  0.1412, -0.3043,  0.0909],\n","         [ 0.3064,  0.3258, -0.7044,  ...,  0.1412, -0.3043,  0.0909]],\n","\n","        [[ 0.2208,  0.2642, -0.4533,  ...,  0.1547, -0.1889,  0.0525],\n","         [ 0.1644,  0.2737, -0.3783,  ...,  0.1014, -0.1761,  0.0328],\n","         [ 0.2120,  0.2733, -0.4215,  ...,  0.1235, -0.1820,  0.0706],\n","         ...,\n","         [ 0.1912,  0.2609, -0.4708,  ...,  0.0938, -0.1809,  0.0256],\n","         [ 0.1912,  0.2609, -0.4708,  ...,  0.0938, -0.1809,  0.0256],\n","         [ 0.1912,  0.2609, -0.4708,  ...,  0.0938, -0.1809,  0.0256]],\n","\n","        ...,\n","\n","        [[-0.0704,  0.1532, -0.0353,  ..., -0.0491,  0.0008, -0.0548],\n","         [-0.0312,  0.1248, -0.0422,  ..., -0.0496, -0.0561, -0.0312],\n","         [-0.0519,  0.1458, -0.0477,  ..., -0.0803,  0.0106, -0.0454],\n","         ...,\n","         [-0.0874,  0.1779, -0.0335,  ..., -0.0777, -0.0382, -0.0491],\n","         [-0.0654,  0.1312, -0.0501,  ..., -0.1078, -0.0365, -0.0401],\n","         [-0.0603,  0.1230, -0.0510,  ..., -0.1005,  0.0016, -0.0239]],\n","\n","        [[ 0.1205,  0.1648, -0.1884,  ...,  0.1066, -0.1254,  0.1160],\n","         [ 0.1226,  0.1978, -0.2274,  ...,  0.1424, -0.1399,  0.0833],\n","         [ 0.1350,  0.1920, -0.1891,  ...,  0.1583, -0.1301,  0.1074],\n","         ...,\n","         [ 0.1444,  0.1353, -0.1647,  ...,  0.0970, -0.1187,  0.0804],\n","         [ 0.1444,  0.1353, -0.1647,  ...,  0.0970, -0.1187,  0.0804],\n","         [ 0.1444,  0.1353, -0.1647,  ...,  0.0970, -0.1187,  0.0804]],\n","\n","        [[ 0.1144,  0.2858, -0.4520,  ...,  0.2025, -0.1948,  0.0281],\n","         [ 0.0563,  0.2222, -0.4398,  ...,  0.1354, -0.2405, -0.0234],\n","         [ 0.0343,  0.1859, -0.3820,  ...,  0.1388, -0.2388, -0.0323],\n","         ...,\n","         [ 0.0269,  0.1630, -0.3377,  ...,  0.0916, -0.2480, -0.0382],\n","         [ 0.0269,  0.1630, -0.3377,  ...,  0.0916, -0.2480, -0.0382],\n","         [ 0.0269,  0.1630, -0.3377,  ...,  0.0916, -0.2480, -0.0382]]],\n","       grad_fn=<ViewBackward0>)\n","torch.Size([10, 20, 512])\n"]}]},{"cell_type":"markdown","metadata":{"id":"goX70VKqhxQH"},"source":["## Req. 2-2 Multi-head self-attention 모듈 클래스 구현"]},{"cell_type":"markdown","metadata":{"id":"WtNyV7mMj7V_"},"source":["위의 과정을 모두 합쳐 하나의 Multi-head attention 모듈 class를 구현하겠습니다.\n","\n","아래 코드의 TODO 부분을 채워주세요."]},{"cell_type":"code","metadata":{"id":"U_kNhOTrkBHm","executionInfo":{"status":"ok","timestamp":1661997348403,"user_tz":-540,"elapsed":474,"user":{"displayName":"김수빈","userId":"13390164251685497519"}}},"source":["class MultiheadAttention(nn.Module):\n","  def __init__(self):\n","    super(MultiheadAttention, self).__init__()\n","\n","    # Q, K, V learnable matrices\n","    self.w_q = nn.Linear(d_model, d_model)\n","    self.w_k = nn.Linear(d_model, d_model)\n","    self.w_v = nn.Linear(d_model, d_model)\n","\n","    # Linear projection for concatenated outputs\n","    self.w_0 = nn.Linear(d_model, d_model)\n","\n","  # scaled-dot product attention\n","  def self_attention(self, q, k, v):\n","    attn_scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(d_k)  # (B, num_heads, L, L)\n","    attn_dists = F.softmax(attn_scores, dim=-1)  # (B, num_heads, L, L)\n","\n","    attn_values = torch.matmul(attn_dists, v)  # (B, num_heads, L, d_k)\n","\n","    return attn_values\n","\n","  def forward(self, q, k, v):\n","    batch_size = q.shape[0]\n","\n","    # linear projection\n","    ################################################################################\n","    # TODO 1: Implement the forward pass for linear projection.                #\n","    ################################################################################\n","    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n","    q = w_q(batch_emb)  # (B, L, d_model)\n","    k = w_k(batch_emb)  # (B, L, d_model)\n","    v = w_v(batch_emb)  # (B, L, d_model)\n","\n","    # head만큼 쪼개준다\n","    ################################################################################\n","    # TODO 2: Implement the forward pass for \bsplit head.                #\n","    ################################################################################\n","    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n","\n","    q = q.view(batch_size, -1, num_heads, d_k)  # (B, L, num_heads, d_k)\n","    k = k.view(batch_size, -1, num_heads, d_k)  # (B, L, num_heads, d_k)\n","    v = v.view(batch_size, -1, num_heads, d_k)  # (B, L, num_heads, d_k)\n","\n","\n","    # 각 head가 (L, d_k)의 matrix를 담당하도록 만든다\n","    q = q.transpose(1, 2)  # (B, num_heads, L, d_k)\n","    k = k.transpose(1, 2)  # (B, num_heads, L, d_k)\n","    v = v.transpose(1, 2)  # (B, num_heads, L, d_k)\n","\n","    attn_values = self.self_attention(q, k, v)  # (B, num_heads, L, d_k)\n","    attn_values = attn_values.transpose(1, 2).contiguous().view(batch_size, -1, d_model)  # (B, L, num_heads, d_k) => (B, L, d_model)\n","\n","    return self.w_0(attn_values)"],"execution_count":19,"outputs":[]},{"cell_type":"code","metadata":{"id":"jYLuu_9alQxT","executionInfo":{"status":"ok","timestamp":1661997874148,"user_tz":-540,"elapsed":2,"user":{"displayName":"김수빈","userId":"13390164251685497519"}}},"source":["multihead_attn = MultiheadAttention()\n","\n","outputs = multihead_attn(batch_emb, batch_emb, batch_emb)  # (B, L, d_model)"],"execution_count":26,"outputs":[]},{"cell_type":"code","metadata":{"id":"KMiXlYjSlTfB","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1661997875854,"user_tz":-540,"elapsed":2,"user":{"displayName":"김수빈","userId":"13390164251685497519"}},"outputId":"f898d872-2b7a-4ce2-a213-d8f184138d31"},"source":["print(outputs)\n","print(outputs.shape)  # (batch_size, length, d_model)"],"execution_count":27,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[[-0.0099, -0.1381, -0.1413,  ..., -0.1032,  0.0060, -0.0400],\n","         [-0.0237, -0.1111, -0.1210,  ..., -0.0751,  0.0841, -0.0179],\n","         [-0.0218, -0.1023, -0.1356,  ..., -0.0731,  0.0263, -0.0382],\n","         ...,\n","         [-0.0377, -0.1805, -0.1478,  ..., -0.0802,  0.0496, -0.0135],\n","         [-0.0377, -0.1805, -0.1478,  ..., -0.0802,  0.0496, -0.0135],\n","         [-0.0377, -0.1805, -0.1478,  ..., -0.0802,  0.0496, -0.0135]],\n","\n","        [[-0.0976, -0.2689, -0.4722,  ...,  0.1124, -0.1993, -0.1764],\n","         [-0.1066, -0.2482, -0.3760,  ...,  0.1282, -0.2924, -0.1509],\n","         [-0.0958, -0.2800, -0.4454,  ...,  0.1305, -0.2678, -0.1799],\n","         ...,\n","         [-0.1230, -0.3100, -0.4888,  ...,  0.1760, -0.2491, -0.2121],\n","         [-0.1230, -0.3100, -0.4888,  ...,  0.1760, -0.2491, -0.2121],\n","         [-0.1230, -0.3100, -0.4888,  ...,  0.1760, -0.2491, -0.2121]],\n","\n","        [[-0.0346, -0.1738, -0.3469,  ...,  0.1012, -0.0318, -0.1792],\n","         [-0.0029, -0.1760, -0.2743,  ...,  0.0967, -0.0283, -0.1231],\n","         [ 0.0271, -0.2145, -0.3250,  ...,  0.1296, -0.0313, -0.1637],\n","         ...,\n","         [ 0.0324, -0.1980, -0.3016,  ...,  0.1378, -0.0439, -0.1317],\n","         [ 0.0324, -0.1980, -0.3016,  ...,  0.1378, -0.0439, -0.1317],\n","         [ 0.0324, -0.1980, -0.3016,  ...,  0.1378, -0.0439, -0.1317]],\n","\n","        ...,\n","\n","        [[ 0.1503,  0.1269,  0.1150,  ..., -0.0136,  0.1538,  0.0587],\n","         [ 0.1264,  0.1217,  0.1609,  ...,  0.0159,  0.1783,  0.1215],\n","         [ 0.1536,  0.1833,  0.2042,  ...,  0.0243,  0.1337,  0.0639],\n","         ...,\n","         [ 0.1674,  0.1373,  0.1256,  ...,  0.0274,  0.1545,  0.0601],\n","         [ 0.1306,  0.0942,  0.0957,  ...,  0.0062,  0.1842,  0.0699],\n","         [ 0.1903,  0.1231,  0.0850,  ..., -0.0093,  0.1136,  0.1426]],\n","\n","        [[-0.0061, -0.0422, -0.1077,  ...,  0.0014, -0.0816, -0.0371],\n","         [ 0.0174, -0.0427, -0.0993,  ..., -0.0263, -0.0345, -0.0382],\n","         [-0.0044, -0.0378, -0.0536,  ...,  0.0391, -0.0592, -0.0282],\n","         ...,\n","         [-0.0046, -0.0768, -0.0885,  ...,  0.0336, -0.1306, -0.0048],\n","         [-0.0046, -0.0768, -0.0885,  ...,  0.0336, -0.1306, -0.0048],\n","         [-0.0046, -0.0768, -0.0885,  ...,  0.0336, -0.1306, -0.0048]],\n","\n","        [[-0.0179, -0.1455, -0.2029,  ..., -0.0104,  0.0461, -0.1245],\n","         [ 0.0163, -0.1293, -0.1706,  ..., -0.0006, -0.0279, -0.0667],\n","         [ 0.0216, -0.1453, -0.1149,  ...,  0.0029, -0.0167, -0.1014],\n","         ...,\n","         [ 0.0358, -0.1562, -0.1996,  ...,  0.0403, -0.0573, -0.0326],\n","         [ 0.0358, -0.1562, -0.1996,  ...,  0.0403, -0.0573, -0.0326],\n","         [ 0.0358, -0.1562, -0.1996,  ...,  0.0403, -0.0573, -0.0326]]],\n","       grad_fn=<ViewBackward0>)\n","torch.Size([10, 20, 512])\n"]}]},{"cell_type":"code","metadata":{"id":"OTku1fySVR3L"},"source":[],"execution_count":null,"outputs":[]}]}