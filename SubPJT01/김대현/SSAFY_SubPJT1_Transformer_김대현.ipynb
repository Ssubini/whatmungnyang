{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Transformer"
      ],
      "metadata": {
        "id": "GiWulKAdmAHX"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9KsBGZpKkWki"
      },
      "source": [
        "Transformer의 핵심 구조인 Multi-head Attention을 구현하는 실습입니다.\n",
        "1. Multi-head attention 및 self-attention 구현.\n",
        "2. 각 과정에서 일어나는 연산과 input/output 형태 이해."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8qRU5DFY2OM8"
      },
      "source": [
        "### 필요 패키지 import"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lDtMioSQQ1bB"
      },
      "source": [
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "from tqdm import tqdm\n",
        "\n",
        "import torch\n",
        "import math"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Req. 2-1 Multi-head self-attention 구조 익히기"
      ],
      "metadata": {
        "id": "HH0VdC4uJJVG"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QBiZObgRep_Q"
      },
      "source": [
        "### **데이터 전처리**\n",
        "vocab_size 100인 가상의 시퀀스 데이터를 생성합니다. \n",
        "\n",
        "각 데이터에 할당된 숫자는 tokenizing과 정수화가 이뤄진 형태입니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e9ULZIqTenSc"
      },
      "source": [
        "pad_id = 0\n",
        "vocab_size = 100\n",
        "\n",
        "data = [\n",
        "  [62, 13, 47, 39, 78, 33, 56, 13, 39, 29, 44, 86, 71, 36, 18, 75],\n",
        "  [60, 96, 51, 32, 90],\n",
        "  [35, 45, 48, 65, 91, 99, 92, 10, 3, 21, 54],\n",
        "  [75, 51],\n",
        "  [66, 88, 98, 47],\n",
        "  [21, 39, 10, 64, 21],\n",
        "  [98],\n",
        "  [77, 65, 51, 77, 19, 15, 35, 19, 23, 97, 50, 46, 53, 42, 45, 91, 66, 3, 43, 10],\n",
        "  [70, 64, 98, 25, 99, 53, 4, 13, 69, 62, 66, 76, 15, 75, 45, 34],\n",
        "  [20, 64, 81, 35, 76, 85, 1, 62, 8, 45, 99, 77, 19, 43]\n",
        "]"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Hx3mcivgMyH"
      },
      "source": [
        "# 길이 맞춰주기 위해 패딩합니다.\n",
        "def padding(data):\n",
        "  max_len = len(max(data, key=len))\n",
        "  print(f\"Maximum sequence length: {max_len}\")\n",
        "\n",
        "  for i, seq in enumerate(tqdm(data)):\n",
        "    if len(seq) < max_len:\n",
        "      data[i] = seq + [pad_id] * (max_len - len(seq))\n",
        "\n",
        "  return data, max_len"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s3e8FiNvgX60",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1de722f4-46ea-4eaf-b5b0-dc1dfb9fd48e"
      },
      "source": [
        "data, max_len = padding(data)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Maximum sequence length: 20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10/10 [00:00<00:00, 60787.01it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hwPSIWYugaN0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0d3be19d-aa88-46e5-a518-08c85aa0192a"
      },
      "source": [
        "data"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[62, 13, 47, 39, 78, 33, 56, 13, 39, 29, 44, 86, 71, 36, 18, 75, 0, 0, 0, 0],\n",
              " [60, 96, 51, 32, 90, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              " [35, 45, 48, 65, 91, 99, 92, 10, 3, 21, 54, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              " [75, 51, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              " [66, 88, 98, 47, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              " [21, 39, 10, 64, 21, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              " [98, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              " [77,\n",
              "  65,\n",
              "  51,\n",
              "  77,\n",
              "  19,\n",
              "  15,\n",
              "  35,\n",
              "  19,\n",
              "  23,\n",
              "  97,\n",
              "  50,\n",
              "  46,\n",
              "  53,\n",
              "  42,\n",
              "  45,\n",
              "  91,\n",
              "  66,\n",
              "  3,\n",
              "  43,\n",
              "  10],\n",
              " [70, 64, 98, 25, 99, 53, 4, 13, 69, 62, 66, 76, 15, 75, 45, 34, 0, 0, 0, 0],\n",
              " [20, 64, 81, 35, 76, 85, 1, 62, 8, 45, 99, 77, 19, 43, 0, 0, 0, 0, 0, 0]]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rwqjACx8iidc"
      },
      "source": [
        "### Hyperparameter 세팅 및 embedding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p-Ngp2nWimS8"
      },
      "source": [
        "d_model = 512  # model의 hidden size\n",
        "num_heads = 8  # head의 개수\n",
        "\n",
        "# d_model이 입력을 projection 시킬 임베딩 space의 차원이므로, num_heads로 나누어 떨어져야 한다."
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GJMi2Xsni5uq"
      },
      "source": [
        "embedding = nn.Embedding(vocab_size, d_model)\n",
        "\n",
        "# B: batch size, L: maximum sequence length\n",
        "batch = torch.LongTensor(data)  # (B, L)\n",
        "batch_emb = embedding(batch)  # (B, L, d_model)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3tLCUQwojcUb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "56dfb674-bacf-407a-a8c7-934d2ed64823"
      },
      "source": [
        "print(batch_emb)\n",
        "print(batch_emb.shape)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[-0.0872, -0.1837,  0.0663,  ...,  0.8223, -0.9755, -1.6482],\n",
            "         [ 1.3381, -0.0301,  0.9966,  ...,  0.1041,  1.3338,  0.3834],\n",
            "         [-0.0453, -0.3511, -1.3705,  ...,  0.1287, -0.0406, -0.7384],\n",
            "         ...,\n",
            "         [ 1.5341, -0.5658,  2.5760,  ...,  0.7668, -0.1331,  0.3681],\n",
            "         [ 1.5341, -0.5658,  2.5760,  ...,  0.7668, -0.1331,  0.3681],\n",
            "         [ 1.5341, -0.5658,  2.5760,  ...,  0.7668, -0.1331,  0.3681]],\n",
            "\n",
            "        [[-0.7317, -0.6956, -0.3393,  ...,  0.2617,  0.4112,  1.0699],\n",
            "         [-1.3148,  0.3945,  0.4143,  ...,  0.1664,  0.1821,  1.4388],\n",
            "         [ 1.3474, -1.1592, -1.2798,  ..., -1.7441, -1.1508, -0.2235],\n",
            "         ...,\n",
            "         [ 1.5341, -0.5658,  2.5760,  ...,  0.7668, -0.1331,  0.3681],\n",
            "         [ 1.5341, -0.5658,  2.5760,  ...,  0.7668, -0.1331,  0.3681],\n",
            "         [ 1.5341, -0.5658,  2.5760,  ...,  0.7668, -0.1331,  0.3681]],\n",
            "\n",
            "        [[ 0.7209, -0.6180,  0.4204,  ..., -0.5001,  1.5654, -0.5699],\n",
            "         [-0.2522, -0.5939,  1.1027,  ..., -1.1114, -1.3982,  0.8216],\n",
            "         [-0.4109,  0.4789, -1.4745,  ..., -0.9504,  0.0611, -2.1262],\n",
            "         ...,\n",
            "         [ 1.5341, -0.5658,  2.5760,  ...,  0.7668, -0.1331,  0.3681],\n",
            "         [ 1.5341, -0.5658,  2.5760,  ...,  0.7668, -0.1331,  0.3681],\n",
            "         [ 1.5341, -0.5658,  2.5760,  ...,  0.7668, -0.1331,  0.3681]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ 0.7952,  0.4445,  3.3253,  ...,  0.7706,  0.0509, -0.7833],\n",
            "         [ 2.3148, -1.2907,  0.1254,  ...,  1.5628,  0.6447,  0.5549],\n",
            "         [ 1.3474, -1.1592, -1.2798,  ..., -1.7441, -1.1508, -0.2235],\n",
            "         ...,\n",
            "         [-0.7182, -1.1243,  0.1490,  ..., -1.2322,  0.4389,  0.5082],\n",
            "         [ 0.8501,  0.7613, -0.2618,  ..., -0.9247,  1.0535, -0.6274],\n",
            "         [ 0.1335,  0.2925,  0.1223,  ..., -0.2677,  0.6837, -0.6557]],\n",
            "\n",
            "        [[ 0.5267, -0.8709,  1.3957,  ..., -0.2206,  0.9985, -0.1920],\n",
            "         [-0.5124, -0.4889,  1.4555,  ...,  0.3127, -0.8305,  0.1494],\n",
            "         [ 0.5221,  0.9231,  1.4898,  ..., -0.3050, -0.8551, -0.9102],\n",
            "         ...,\n",
            "         [ 1.5341, -0.5658,  2.5760,  ...,  0.7668, -0.1331,  0.3681],\n",
            "         [ 1.5341, -0.5658,  2.5760,  ...,  0.7668, -0.1331,  0.3681],\n",
            "         [ 1.5341, -0.5658,  2.5760,  ...,  0.7668, -0.1331,  0.3681]],\n",
            "\n",
            "        [[ 0.8461, -0.3599,  0.5343,  ...,  1.0004, -0.3411,  1.2046],\n",
            "         [-0.5124, -0.4889,  1.4555,  ...,  0.3127, -0.8305,  0.1494],\n",
            "         [ 1.4639,  1.4778,  1.3955,  ...,  1.0402, -0.8592,  0.5164],\n",
            "         ...,\n",
            "         [ 1.5341, -0.5658,  2.5760,  ...,  0.7668, -0.1331,  0.3681],\n",
            "         [ 1.5341, -0.5658,  2.5760,  ...,  0.7668, -0.1331,  0.3681],\n",
            "         [ 1.5341, -0.5658,  2.5760,  ...,  0.7668, -0.1331,  0.3681]]],\n",
            "       grad_fn=<EmbeddingBackward0>)\n",
            "torch.Size([10, 20, 512])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s0Lhx892gmi3"
      },
      "source": [
        "### Linear projection & 여러 head로 나누기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "urXMBRnRgqvw"
      },
      "source": [
        "Multi-head attention 내에서 쓰이는 linear projection matrix들을 정의합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9DWKDqgCgfMk"
      },
      "source": [
        "w_q = nn.Linear(d_model, d_model)\n",
        "w_k = nn.Linear(d_model, d_model)\n",
        "w_v = nn.Linear(d_model, d_model)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tcLuhda7m-Lm"
      },
      "source": [
        "w_0 = nn.Linear(d_model, d_model)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H-vSL7PwnV6k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "91b0722f-2ff5-46ca-a0c4-06858d703068"
      },
      "source": [
        "q = w_q(batch_emb)  # (B, L, d_model)\n",
        "k = w_k(batch_emb)  # (B, L, d_model)\n",
        "v = w_v(batch_emb)  # (B, L, d_model)\n",
        "\n",
        "print(q.shape)\n",
        "print(k.shape)\n",
        "print(v.shape)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([10, 20, 512])\n",
            "torch.Size([10, 20, 512])\n",
            "torch.Size([10, 20, 512])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wnvlum-LnF1T"
      },
      "source": [
        "Q, k, v를 `num_head`개의 차원 분할된 여러 vector로 만듭니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sXcYLZYvJT_1"
      },
      "source": [
        "- 이론적으로는 multi-head attention을 수행하면 input을 각각 다른 head 개수만큼의 Wq, Wk, Wv로 linear transformation 해서 각각 여러번의 attention 수행한 후 concat 한 후 linear transformation 수행해준다\n",
        "- 구현에서는 Wq, Wk, Wv 한 개씩\n",
        "- 실제 `attention is all you need` 논문의 구현 예시는 Query vector 한개를 dim으로 쪼개서 진행한다"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_tiOKAv9nEli",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "575fe5da-68dc-4f78-8b36-753d5c56f1ea"
      },
      "source": [
        "batch_size = q.shape[0]\n",
        "d_k = d_model // num_heads\n",
        "\n",
        "# num_heads * d_k로 쪼갠다\n",
        "q = q.view(batch_size, -1, num_heads, d_k)  # (B, L, num_heads, d_k)\n",
        "k = k.view(batch_size, -1, num_heads, d_k)  # (B, L, num_heads, d_k)\n",
        "v = v.view(batch_size, -1, num_heads, d_k)  # (B, L, num_heads, d_k)\n",
        "\n",
        "print(q.shape)\n",
        "print(k.shape)\n",
        "print(v.shape)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([10, 20, 8, 64])\n",
            "torch.Size([10, 20, 8, 64])\n",
            "torch.Size([10, 20, 8, 64])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5tNb2isfn5Cx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "df473ab1-eb09-455d-8582-34a2f8cd266a"
      },
      "source": [
        "# num_heads를 밖으로 뺌으로써\n",
        "# 각 head가 (L, d_k) 만큼의 matrix를 가지고 self-attention 수행\n",
        "\n",
        "q = q.transpose(1, 2)  # (B, num_heads, L, d_k)\n",
        "k = k.transpose(1, 2)  # (B, num_heads, L, d_k)\n",
        "v = v.transpose(1, 2)  # (B, num_heads, L, d_k)\n",
        "\n",
        "print(q.shape)\n",
        "print(k.shape)\n",
        "print(v.shape)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([10, 8, 20, 64])\n",
            "torch.Size([10, 8, 20, 64])\n",
            "torch.Size([10, 8, 20, 64])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NWrDA5_Sofad"
      },
      "source": [
        "### Scaled dot-product self-attention 구현"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w52C4k3Wfl8m"
      },
      "source": [
        "각 head에서 실행되는 self-attetion 과정입니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A5waKr0Hfi2K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "031a87f3-5cb5-4893-aa27-0df86892c842"
      },
      "source": [
        "# shape - (L, L)\n",
        "# 같은 sequence 내에 서로 다른 token들에게 얼마나 가중치를 두고 attention을 해야하는가\n",
        "attn_scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(d_k)  # (B, num_heads, L, L)\n",
        "# softmax - row-wise이기 때문에 dim은 -1\n",
        "attn_dists = F.softmax(attn_scores, dim=-1)  # (B, num_heads, L, L)\n",
        "\n",
        "print(attn_dists)\n",
        "print(attn_dists.shape)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[[0.0479, 0.0324, 0.0634,  ..., 0.0532, 0.0532, 0.0532],\n",
            "          [0.0255, 0.0495, 0.0866,  ..., 0.0334, 0.0334, 0.0334],\n",
            "          [0.0432, 0.0314, 0.0736,  ..., 0.0565, 0.0565, 0.0565],\n",
            "          ...,\n",
            "          [0.0279, 0.0610, 0.0734,  ..., 0.0376, 0.0376, 0.0376],\n",
            "          [0.0279, 0.0610, 0.0734,  ..., 0.0376, 0.0376, 0.0376],\n",
            "          [0.0279, 0.0610, 0.0734,  ..., 0.0376, 0.0376, 0.0376]],\n",
            "\n",
            "         [[0.0579, 0.0379, 0.0388,  ..., 0.0512, 0.0512, 0.0512],\n",
            "          [0.0386, 0.0420, 0.0434,  ..., 0.0562, 0.0562, 0.0562],\n",
            "          [0.0514, 0.0505, 0.0615,  ..., 0.0619, 0.0619, 0.0619],\n",
            "          ...,\n",
            "          [0.0661, 0.0207, 0.0630,  ..., 0.0719, 0.0719, 0.0719],\n",
            "          [0.0661, 0.0207, 0.0630,  ..., 0.0719, 0.0719, 0.0719],\n",
            "          [0.0661, 0.0207, 0.0630,  ..., 0.0719, 0.0719, 0.0719]],\n",
            "\n",
            "         [[0.0352, 0.0536, 0.0844,  ..., 0.0333, 0.0333, 0.0333],\n",
            "          [0.0407, 0.0493, 0.0892,  ..., 0.0477, 0.0477, 0.0477],\n",
            "          [0.0278, 0.0740, 0.0298,  ..., 0.0422, 0.0422, 0.0422],\n",
            "          ...,\n",
            "          [0.0290, 0.0996, 0.0428,  ..., 0.0337, 0.0337, 0.0337],\n",
            "          [0.0290, 0.0996, 0.0428,  ..., 0.0337, 0.0337, 0.0337],\n",
            "          [0.0290, 0.0996, 0.0428,  ..., 0.0337, 0.0337, 0.0337]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[0.0280, 0.0458, 0.0278,  ..., 0.0680, 0.0680, 0.0680],\n",
            "          [0.0387, 0.0762, 0.0664,  ..., 0.0463, 0.0463, 0.0463],\n",
            "          [0.0260, 0.0585, 0.0893,  ..., 0.0468, 0.0468, 0.0468],\n",
            "          ...,\n",
            "          [0.0356, 0.0523, 0.0689,  ..., 0.0447, 0.0447, 0.0447],\n",
            "          [0.0356, 0.0523, 0.0689,  ..., 0.0447, 0.0447, 0.0447],\n",
            "          [0.0356, 0.0523, 0.0689,  ..., 0.0447, 0.0447, 0.0447]],\n",
            "\n",
            "         [[0.0356, 0.0443, 0.0431,  ..., 0.0669, 0.0669, 0.0669],\n",
            "          [0.0499, 0.0491, 0.0398,  ..., 0.0551, 0.0551, 0.0551],\n",
            "          [0.0337, 0.0306, 0.0507,  ..., 0.0632, 0.0632, 0.0632],\n",
            "          ...,\n",
            "          [0.0351, 0.0501, 0.0554,  ..., 0.0575, 0.0575, 0.0575],\n",
            "          [0.0351, 0.0501, 0.0554,  ..., 0.0575, 0.0575, 0.0575],\n",
            "          [0.0351, 0.0501, 0.0554,  ..., 0.0575, 0.0575, 0.0575]],\n",
            "\n",
            "         [[0.0565, 0.0538, 0.0541,  ..., 0.0554, 0.0554, 0.0554],\n",
            "          [0.0483, 0.0620, 0.0557,  ..., 0.0319, 0.0319, 0.0319],\n",
            "          [0.0417, 0.0679, 0.0495,  ..., 0.0411, 0.0411, 0.0411],\n",
            "          ...,\n",
            "          [0.0381, 0.0399, 0.0308,  ..., 0.0400, 0.0400, 0.0400],\n",
            "          [0.0381, 0.0399, 0.0308,  ..., 0.0400, 0.0400, 0.0400],\n",
            "          [0.0381, 0.0399, 0.0308,  ..., 0.0400, 0.0400, 0.0400]]],\n",
            "\n",
            "\n",
            "        [[[0.0404, 0.0707, 0.0774,  ..., 0.0462, 0.0462, 0.0462],\n",
            "          [0.0418, 0.0302, 0.0590,  ..., 0.0521, 0.0521, 0.0521],\n",
            "          [0.0534, 0.0538, 0.0531,  ..., 0.0451, 0.0451, 0.0451],\n",
            "          ...,\n",
            "          [0.0958, 0.0731, 0.0356,  ..., 0.0446, 0.0446, 0.0446],\n",
            "          [0.0958, 0.0731, 0.0356,  ..., 0.0446, 0.0446, 0.0446],\n",
            "          [0.0958, 0.0731, 0.0356,  ..., 0.0446, 0.0446, 0.0446]],\n",
            "\n",
            "         [[0.0265, 0.0239, 0.0234,  ..., 0.0568, 0.0568, 0.0568],\n",
            "          [0.0400, 0.0754, 0.0419,  ..., 0.0445, 0.0445, 0.0445],\n",
            "          [0.0638, 0.0710, 0.0349,  ..., 0.0474, 0.0474, 0.0474],\n",
            "          ...,\n",
            "          [0.0252, 0.0334, 0.0374,  ..., 0.0538, 0.0538, 0.0538],\n",
            "          [0.0252, 0.0334, 0.0374,  ..., 0.0538, 0.0538, 0.0538],\n",
            "          [0.0252, 0.0334, 0.0374,  ..., 0.0538, 0.0538, 0.0538]],\n",
            "\n",
            "         [[0.0705, 0.0413, 0.0459,  ..., 0.0498, 0.0498, 0.0498],\n",
            "          [0.0554, 0.0634, 0.0558,  ..., 0.0474, 0.0474, 0.0474],\n",
            "          [0.0462, 0.0529, 0.0470,  ..., 0.0502, 0.0502, 0.0502],\n",
            "          ...,\n",
            "          [0.0646, 0.0515, 0.1207,  ..., 0.0441, 0.0441, 0.0441],\n",
            "          [0.0646, 0.0515, 0.1207,  ..., 0.0441, 0.0441, 0.0441],\n",
            "          [0.0646, 0.0515, 0.1207,  ..., 0.0441, 0.0441, 0.0441]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[0.0802, 0.0393, 0.0551,  ..., 0.0495, 0.0495, 0.0495],\n",
            "          [0.0509, 0.0895, 0.0429,  ..., 0.0498, 0.0498, 0.0498],\n",
            "          [0.0612, 0.0967, 0.0446,  ..., 0.0407, 0.0407, 0.0407],\n",
            "          ...,\n",
            "          [0.0464, 0.0515, 0.0692,  ..., 0.0503, 0.0503, 0.0503],\n",
            "          [0.0464, 0.0515, 0.0692,  ..., 0.0503, 0.0503, 0.0503],\n",
            "          [0.0464, 0.0515, 0.0692,  ..., 0.0503, 0.0503, 0.0503]],\n",
            "\n",
            "         [[0.0453, 0.0486, 0.0377,  ..., 0.0486, 0.0486, 0.0486],\n",
            "          [0.0437, 0.0535, 0.0299,  ..., 0.0521, 0.0521, 0.0521],\n",
            "          [0.0359, 0.0315, 0.0399,  ..., 0.0561, 0.0561, 0.0561],\n",
            "          ...,\n",
            "          [0.0465, 0.0338, 0.0338,  ..., 0.0513, 0.0513, 0.0513],\n",
            "          [0.0465, 0.0338, 0.0338,  ..., 0.0513, 0.0513, 0.0513],\n",
            "          [0.0465, 0.0338, 0.0338,  ..., 0.0513, 0.0513, 0.0513]],\n",
            "\n",
            "         [[0.0368, 0.0529, 0.0618,  ..., 0.0486, 0.0486, 0.0486],\n",
            "          [0.0518, 0.0239, 0.0579,  ..., 0.0487, 0.0487, 0.0487],\n",
            "          [0.0614, 0.0406, 0.0236,  ..., 0.0538, 0.0538, 0.0538],\n",
            "          ...,\n",
            "          [0.0980, 0.0574, 0.0560,  ..., 0.0471, 0.0471, 0.0471],\n",
            "          [0.0980, 0.0574, 0.0560,  ..., 0.0471, 0.0471, 0.0471],\n",
            "          [0.0980, 0.0574, 0.0560,  ..., 0.0471, 0.0471, 0.0471]]],\n",
            "\n",
            "\n",
            "        [[[0.0534, 0.0344, 0.0604,  ..., 0.0577, 0.0577, 0.0577],\n",
            "          [0.0536, 0.0280, 0.0406,  ..., 0.0551, 0.0551, 0.0551],\n",
            "          [0.0468, 0.0511, 0.0365,  ..., 0.0512, 0.0512, 0.0512],\n",
            "          ...,\n",
            "          [0.0670, 0.0705, 0.0508,  ..., 0.0421, 0.0421, 0.0421],\n",
            "          [0.0670, 0.0705, 0.0508,  ..., 0.0421, 0.0421, 0.0421],\n",
            "          [0.0670, 0.0705, 0.0508,  ..., 0.0421, 0.0421, 0.0421]],\n",
            "\n",
            "         [[0.0388, 0.0539, 0.0663,  ..., 0.0374, 0.0374, 0.0374],\n",
            "          [0.0386, 0.0384, 0.0322,  ..., 0.0427, 0.0427, 0.0427],\n",
            "          [0.0462, 0.0651, 0.0291,  ..., 0.0541, 0.0541, 0.0541],\n",
            "          ...,\n",
            "          [0.0428, 0.0453, 0.0650,  ..., 0.0578, 0.0578, 0.0578],\n",
            "          [0.0428, 0.0453, 0.0650,  ..., 0.0578, 0.0578, 0.0578],\n",
            "          [0.0428, 0.0453, 0.0650,  ..., 0.0578, 0.0578, 0.0578]],\n",
            "\n",
            "         [[0.0456, 0.0394, 0.0653,  ..., 0.0390, 0.0390, 0.0390],\n",
            "          [0.0333, 0.0377, 0.0464,  ..., 0.0495, 0.0495, 0.0495],\n",
            "          [0.0396, 0.1000, 0.0738,  ..., 0.0503, 0.0503, 0.0503],\n",
            "          ...,\n",
            "          [0.0490, 0.0476, 0.0995,  ..., 0.0349, 0.0349, 0.0349],\n",
            "          [0.0490, 0.0476, 0.0995,  ..., 0.0349, 0.0349, 0.0349],\n",
            "          [0.0490, 0.0476, 0.0995,  ..., 0.0349, 0.0349, 0.0349]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[0.0364, 0.0456, 0.0312,  ..., 0.0506, 0.0506, 0.0506],\n",
            "          [0.0486, 0.0386, 0.0220,  ..., 0.0592, 0.0592, 0.0592],\n",
            "          [0.0505, 0.0349, 0.0723,  ..., 0.0570, 0.0570, 0.0570],\n",
            "          ...,\n",
            "          [0.0581, 0.0431, 0.0440,  ..., 0.0505, 0.0505, 0.0505],\n",
            "          [0.0581, 0.0431, 0.0440,  ..., 0.0505, 0.0505, 0.0505],\n",
            "          [0.0581, 0.0431, 0.0440,  ..., 0.0505, 0.0505, 0.0505]],\n",
            "\n",
            "         [[0.0705, 0.0364, 0.0623,  ..., 0.0538, 0.0538, 0.0538],\n",
            "          [0.0479, 0.0491, 0.0359,  ..., 0.0367, 0.0367, 0.0367],\n",
            "          [0.0634, 0.0872, 0.0737,  ..., 0.0311, 0.0311, 0.0311],\n",
            "          ...,\n",
            "          [0.0651, 0.0381, 0.0518,  ..., 0.0571, 0.0571, 0.0571],\n",
            "          [0.0651, 0.0381, 0.0518,  ..., 0.0571, 0.0571, 0.0571],\n",
            "          [0.0651, 0.0381, 0.0518,  ..., 0.0571, 0.0571, 0.0571]],\n",
            "\n",
            "         [[0.0596, 0.0391, 0.0674,  ..., 0.0491, 0.0491, 0.0491],\n",
            "          [0.0318, 0.0441, 0.0523,  ..., 0.0481, 0.0481, 0.0481],\n",
            "          [0.0453, 0.0549, 0.0676,  ..., 0.0465, 0.0465, 0.0465],\n",
            "          ...,\n",
            "          [0.0516, 0.0453, 0.0361,  ..., 0.0434, 0.0434, 0.0434],\n",
            "          [0.0516, 0.0453, 0.0361,  ..., 0.0434, 0.0434, 0.0434],\n",
            "          [0.0516, 0.0453, 0.0361,  ..., 0.0434, 0.0434, 0.0434]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[0.0408, 0.0441, 0.0292,  ..., 0.0570, 0.0505, 0.0410],\n",
            "          [0.0673, 0.0411, 0.0675,  ..., 0.0595, 0.0498, 0.0640],\n",
            "          [0.0362, 0.0517, 0.0460,  ..., 0.0752, 0.0487, 0.0414],\n",
            "          ...,\n",
            "          [0.0379, 0.0186, 0.0960,  ..., 0.0628, 0.0358, 0.0403],\n",
            "          [0.0369, 0.0500, 0.0368,  ..., 0.0459, 0.0593, 0.0717],\n",
            "          [0.0522, 0.0495, 0.0633,  ..., 0.0383, 0.0351, 0.0547]],\n",
            "\n",
            "         [[0.0524, 0.0344, 0.0482,  ..., 0.0596, 0.0303, 0.0522],\n",
            "          [0.0426, 0.0484, 0.0636,  ..., 0.0398, 0.0329, 0.0472],\n",
            "          [0.0450, 0.0491, 0.0327,  ..., 0.0397, 0.0581, 0.0539],\n",
            "          ...,\n",
            "          [0.0511, 0.0501, 0.0836,  ..., 0.0385, 0.0426, 0.0239],\n",
            "          [0.0492, 0.0571, 0.0451,  ..., 0.0404, 0.0421, 0.0400],\n",
            "          [0.0349, 0.0246, 0.0758,  ..., 0.0452, 0.0315, 0.0422]],\n",
            "\n",
            "         [[0.0565, 0.0529, 0.0867,  ..., 0.0578, 0.0409, 0.0469],\n",
            "          [0.0429, 0.0326, 0.0583,  ..., 0.0890, 0.0619, 0.0541],\n",
            "          [0.0367, 0.0543, 0.0389,  ..., 0.0334, 0.0593, 0.0721],\n",
            "          ...,\n",
            "          [0.0431, 0.0327, 0.0434,  ..., 0.0684, 0.0405, 0.0577],\n",
            "          [0.0418, 0.0307, 0.0362,  ..., 0.0664, 0.0490, 0.0601],\n",
            "          [0.0389, 0.0437, 0.0238,  ..., 0.0539, 0.0746, 0.0491]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[0.0465, 0.0509, 0.0719,  ..., 0.0541, 0.0447, 0.0498],\n",
            "          [0.0220, 0.0441, 0.0421,  ..., 0.0566, 0.0633, 0.0573],\n",
            "          [0.0304, 0.0505, 0.0359,  ..., 0.0931, 0.0440, 0.0651],\n",
            "          ...,\n",
            "          [0.0629, 0.0402, 0.0372,  ..., 0.0237, 0.0493, 0.0543],\n",
            "          [0.0482, 0.0551, 0.0423,  ..., 0.0433, 0.0450, 0.0455],\n",
            "          [0.0477, 0.0429, 0.0779,  ..., 0.0441, 0.1027, 0.0383]],\n",
            "\n",
            "         [[0.0842, 0.0467, 0.0277,  ..., 0.0345, 0.0371, 0.0571],\n",
            "          [0.0798, 0.0140, 0.0316,  ..., 0.0325, 0.0265, 0.0412],\n",
            "          [0.0339, 0.0442, 0.0553,  ..., 0.0543, 0.0602, 0.0559],\n",
            "          ...,\n",
            "          [0.0430, 0.0389, 0.0374,  ..., 0.0487, 0.0492, 0.0336],\n",
            "          [0.0370, 0.0556, 0.0412,  ..., 0.0408, 0.0590, 0.0677],\n",
            "          [0.0635, 0.0396, 0.0581,  ..., 0.0699, 0.0373, 0.0558]],\n",
            "\n",
            "         [[0.0736, 0.0272, 0.0629,  ..., 0.0608, 0.0459, 0.0523],\n",
            "          [0.0531, 0.0355, 0.0418,  ..., 0.0299, 0.0419, 0.0408],\n",
            "          [0.0357, 0.0487, 0.0294,  ..., 0.0505, 0.0522, 0.0369],\n",
            "          ...,\n",
            "          [0.0247, 0.0350, 0.0396,  ..., 0.0757, 0.0458, 0.0594],\n",
            "          [0.0880, 0.0334, 0.0429,  ..., 0.0447, 0.0534, 0.0498],\n",
            "          [0.0501, 0.0497, 0.0508,  ..., 0.0556, 0.0558, 0.0477]]],\n",
            "\n",
            "\n",
            "        [[[0.0508, 0.0549, 0.0289,  ..., 0.0529, 0.0529, 0.0529],\n",
            "          [0.0172, 0.0634, 0.0467,  ..., 0.0385, 0.0385, 0.0385],\n",
            "          [0.0410, 0.0492, 0.0464,  ..., 0.0572, 0.0572, 0.0572],\n",
            "          ...,\n",
            "          [0.0654, 0.0151, 0.0573,  ..., 0.0323, 0.0323, 0.0323],\n",
            "          [0.0654, 0.0151, 0.0573,  ..., 0.0323, 0.0323, 0.0323],\n",
            "          [0.0654, 0.0151, 0.0573,  ..., 0.0323, 0.0323, 0.0323]],\n",
            "\n",
            "         [[0.0467, 0.0781, 0.0531,  ..., 0.0432, 0.0432, 0.0432],\n",
            "          [0.0441, 0.0405, 0.0474,  ..., 0.0563, 0.0563, 0.0563],\n",
            "          [0.0444, 0.0361, 0.0536,  ..., 0.0501, 0.0501, 0.0501],\n",
            "          ...,\n",
            "          [0.0581, 0.0713, 0.0463,  ..., 0.0632, 0.0632, 0.0632],\n",
            "          [0.0581, 0.0713, 0.0463,  ..., 0.0632, 0.0632, 0.0632],\n",
            "          [0.0581, 0.0713, 0.0463,  ..., 0.0632, 0.0632, 0.0632]],\n",
            "\n",
            "         [[0.0432, 0.0173, 0.0407,  ..., 0.0408, 0.0408, 0.0408],\n",
            "          [0.0463, 0.0284, 0.0475,  ..., 0.0769, 0.0769, 0.0769],\n",
            "          [0.0497, 0.0348, 0.0283,  ..., 0.0483, 0.0483, 0.0483],\n",
            "          ...,\n",
            "          [0.0562, 0.0448, 0.0324,  ..., 0.0379, 0.0379, 0.0379],\n",
            "          [0.0562, 0.0448, 0.0324,  ..., 0.0379, 0.0379, 0.0379],\n",
            "          [0.0562, 0.0448, 0.0324,  ..., 0.0379, 0.0379, 0.0379]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[0.0418, 0.0601, 0.0281,  ..., 0.0496, 0.0496, 0.0496],\n",
            "          [0.0423, 0.0259, 0.0529,  ..., 0.0427, 0.0427, 0.0427],\n",
            "          [0.0640, 0.0485, 0.0526,  ..., 0.0423, 0.0423, 0.0423],\n",
            "          ...,\n",
            "          [0.0507, 0.0414, 0.0451,  ..., 0.0455, 0.0455, 0.0455],\n",
            "          [0.0507, 0.0414, 0.0451,  ..., 0.0455, 0.0455, 0.0455],\n",
            "          [0.0507, 0.0414, 0.0451,  ..., 0.0455, 0.0455, 0.0455]],\n",
            "\n",
            "         [[0.0770, 0.0591, 0.0432,  ..., 0.0320, 0.0320, 0.0320],\n",
            "          [0.0264, 0.0397, 0.0962,  ..., 0.0661, 0.0661, 0.0661],\n",
            "          [0.0494, 0.0608, 0.0403,  ..., 0.0262, 0.0262, 0.0262],\n",
            "          ...,\n",
            "          [0.0561, 0.0339, 0.0604,  ..., 0.0546, 0.0546, 0.0546],\n",
            "          [0.0561, 0.0339, 0.0604,  ..., 0.0546, 0.0546, 0.0546],\n",
            "          [0.0561, 0.0339, 0.0604,  ..., 0.0546, 0.0546, 0.0546]],\n",
            "\n",
            "         [[0.0562, 0.0450, 0.0457,  ..., 0.0785, 0.0785, 0.0785],\n",
            "          [0.0626, 0.0722, 0.0402,  ..., 0.0526, 0.0526, 0.0526],\n",
            "          [0.0407, 0.0493, 0.0548,  ..., 0.0401, 0.0401, 0.0401],\n",
            "          ...,\n",
            "          [0.0361, 0.0598, 0.0509,  ..., 0.0439, 0.0439, 0.0439],\n",
            "          [0.0361, 0.0598, 0.0509,  ..., 0.0439, 0.0439, 0.0439],\n",
            "          [0.0361, 0.0598, 0.0509,  ..., 0.0439, 0.0439, 0.0439]]],\n",
            "\n",
            "\n",
            "        [[[0.0508, 0.0277, 0.0429,  ..., 0.0521, 0.0521, 0.0521],\n",
            "          [0.0424, 0.0684, 0.0276,  ..., 0.0415, 0.0415, 0.0415],\n",
            "          [0.0928, 0.0273, 0.0318,  ..., 0.0455, 0.0455, 0.0455],\n",
            "          ...,\n",
            "          [0.0523, 0.0168, 0.0570,  ..., 0.0359, 0.0359, 0.0359],\n",
            "          [0.0523, 0.0168, 0.0570,  ..., 0.0359, 0.0359, 0.0359],\n",
            "          [0.0523, 0.0168, 0.0570,  ..., 0.0359, 0.0359, 0.0359]],\n",
            "\n",
            "         [[0.0424, 0.0445, 0.0218,  ..., 0.0505, 0.0505, 0.0505],\n",
            "          [0.0751, 0.0353, 0.0400,  ..., 0.0491, 0.0491, 0.0491],\n",
            "          [0.0446, 0.0643, 0.0319,  ..., 0.0585, 0.0585, 0.0585],\n",
            "          ...,\n",
            "          [0.0419, 0.0721, 0.0295,  ..., 0.0639, 0.0639, 0.0639],\n",
            "          [0.0419, 0.0721, 0.0295,  ..., 0.0639, 0.0639, 0.0639],\n",
            "          [0.0419, 0.0721, 0.0295,  ..., 0.0639, 0.0639, 0.0639]],\n",
            "\n",
            "         [[0.0581, 0.0319, 0.0469,  ..., 0.0410, 0.0410, 0.0410],\n",
            "          [0.0348, 0.0294, 0.0440,  ..., 0.0794, 0.0794, 0.0794],\n",
            "          [0.0680, 0.0178, 0.0600,  ..., 0.0451, 0.0451, 0.0451],\n",
            "          ...,\n",
            "          [0.0391, 0.0459, 0.0490,  ..., 0.0388, 0.0388, 0.0388],\n",
            "          [0.0391, 0.0459, 0.0490,  ..., 0.0388, 0.0388, 0.0388],\n",
            "          [0.0391, 0.0459, 0.0490,  ..., 0.0388, 0.0388, 0.0388]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[0.0415, 0.0432, 0.0365,  ..., 0.0648, 0.0648, 0.0648],\n",
            "          [0.0311, 0.0287, 0.0287,  ..., 0.0475, 0.0475, 0.0475],\n",
            "          [0.0370, 0.0482, 0.0313,  ..., 0.0538, 0.0538, 0.0538],\n",
            "          ...,\n",
            "          [0.0358, 0.0414, 0.0628,  ..., 0.0456, 0.0456, 0.0456],\n",
            "          [0.0358, 0.0414, 0.0628,  ..., 0.0456, 0.0456, 0.0456],\n",
            "          [0.0358, 0.0414, 0.0628,  ..., 0.0456, 0.0456, 0.0456]],\n",
            "\n",
            "         [[0.0511, 0.0450, 0.0524,  ..., 0.0459, 0.0459, 0.0459],\n",
            "          [0.0327, 0.0404, 0.0355,  ..., 0.0672, 0.0672, 0.0672],\n",
            "          [0.0465, 0.0409, 0.0380,  ..., 0.0396, 0.0396, 0.0396],\n",
            "          ...,\n",
            "          [0.0332, 0.0329, 0.0580,  ..., 0.0530, 0.0530, 0.0530],\n",
            "          [0.0332, 0.0329, 0.0580,  ..., 0.0530, 0.0530, 0.0530],\n",
            "          [0.0332, 0.0329, 0.0580,  ..., 0.0530, 0.0530, 0.0530]],\n",
            "\n",
            "         [[0.0505, 0.0435, 0.0371,  ..., 0.0456, 0.0456, 0.0456],\n",
            "          [0.0501, 0.0745, 0.0396,  ..., 0.0542, 0.0542, 0.0542],\n",
            "          [0.0325, 0.0651, 0.0356,  ..., 0.0445, 0.0445, 0.0445],\n",
            "          ...,\n",
            "          [0.0799, 0.0557, 0.0716,  ..., 0.0408, 0.0408, 0.0408],\n",
            "          [0.0799, 0.0557, 0.0716,  ..., 0.0408, 0.0408, 0.0408],\n",
            "          [0.0799, 0.0557, 0.0716,  ..., 0.0408, 0.0408, 0.0408]]]],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "torch.Size([10, 8, 20, 20])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7megouWpgCck",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1228e5f5-e161-4c1f-f00b-77202357f519"
      },
      "source": [
        "attn_values = torch.matmul(attn_dists, v)  # (B, num_heads, L, d_k)\n",
        "\n",
        "print(attn_values.shape)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([10, 8, 20, 64])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LmSTaymdg-P_"
      },
      "source": [
        "### 각 head의 결과물 병합"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YSdQZCk0hCNd"
      },
      "source": [
        "각 head의 결과물을 concat하고 동일 차원으로 linear projection합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eaK0bpMGhQZ2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1de5b229-865b-451c-eac9-aa1fafc949af"
      },
      "source": [
        "attn_values = attn_values.transpose(1, 2)  # (B, L, num_heads, d_k)\n",
        "attn_values = attn_values.contiguous().view(batch_size, -1, d_model)  # (B, L, d_model)\n",
        "\n",
        "print(attn_values.shape)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([10, 20, 512])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LTng_2SXhdH1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ef888de0-e5c7-4bf2-cfb6-ba8a7caa17d4"
      },
      "source": [
        "# w_0 : (d_model, d_model)\n",
        "# 서로 다른 의미로 foucsing 된 각 head의 self-attention 정보들을 합쳐주는 역할 수행\n",
        "outputs = w_0(attn_values)\n",
        "\n",
        "print(outputs)\n",
        "print(outputs.shape)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[ 1.5717e-01, -3.1369e-01, -5.9587e-02,  ...,  3.9965e-02,\n",
            "          -4.1008e-02, -2.3296e-01],\n",
            "         [ 1.0249e-01, -2.2069e-01, -4.2081e-02,  ...,  1.3163e-02,\n",
            "          -2.9182e-02, -2.5673e-01],\n",
            "         [ 4.8828e-02, -2.7699e-01, -2.4381e-02,  ..., -5.3123e-02,\n",
            "          -1.4591e-02, -2.0055e-01],\n",
            "         ...,\n",
            "         [ 7.6182e-02, -3.0714e-01, -1.0364e-02,  ...,  2.2050e-02,\n",
            "           2.0028e-02, -2.4123e-01],\n",
            "         [ 7.6182e-02, -3.0714e-01, -1.0364e-02,  ...,  2.2050e-02,\n",
            "           2.0028e-02, -2.4123e-01],\n",
            "         [ 7.6182e-02, -3.0714e-01, -1.0364e-02,  ...,  2.2050e-02,\n",
            "           2.0028e-02, -2.4123e-01]],\n",
            "\n",
            "        [[ 4.7528e-01, -9.8734e-02, -1.9330e-01,  ...,  1.0549e-02,\n",
            "           1.8985e-02, -3.3002e-01],\n",
            "         [ 4.8786e-01, -1.0205e-01, -1.7467e-01,  ...,  5.6268e-02,\n",
            "          -3.6668e-03, -2.8469e-01],\n",
            "         [ 5.0575e-01, -7.6409e-02, -1.5484e-01,  ...,  3.6863e-02,\n",
            "          -5.6308e-02, -3.6636e-01],\n",
            "         ...,\n",
            "         [ 4.8573e-01, -1.3130e-01, -1.8828e-01,  ...,  4.8626e-02,\n",
            "           1.9914e-02, -3.3793e-01],\n",
            "         [ 4.8573e-01, -1.3130e-01, -1.8828e-01,  ...,  4.8626e-02,\n",
            "           1.9914e-02, -3.3793e-01],\n",
            "         [ 4.8573e-01, -1.3130e-01, -1.8828e-01,  ...,  4.8626e-02,\n",
            "           1.9914e-02, -3.3793e-01]],\n",
            "\n",
            "        [[ 2.1039e-01, -8.5283e-02,  7.8081e-02,  ...,  1.3901e-01,\n",
            "           9.4758e-03, -1.6371e-01],\n",
            "         [ 2.5819e-01, -6.8984e-02,  4.5212e-02,  ...,  1.2937e-01,\n",
            "           1.1984e-02, -1.2719e-01],\n",
            "         [ 2.1938e-01, -6.9504e-02, -9.7599e-03,  ...,  9.2356e-02,\n",
            "           7.1791e-02, -1.7334e-01],\n",
            "         ...,\n",
            "         [ 2.0911e-01, -1.0859e-01, -2.6780e-02,  ...,  1.3801e-01,\n",
            "           6.5761e-02, -2.2416e-01],\n",
            "         [ 2.0911e-01, -1.0859e-01, -2.6780e-02,  ...,  1.3801e-01,\n",
            "           6.5761e-02, -2.2416e-01],\n",
            "         [ 2.0911e-01, -1.0859e-01, -2.6780e-02,  ...,  1.3801e-01,\n",
            "           6.5761e-02, -2.2416e-01]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[-8.7676e-02,  1.3950e-01,  4.1693e-02,  ...,  9.9050e-02,\n",
            "          -1.3241e-01, -5.2287e-02],\n",
            "         [-8.8521e-02,  1.1259e-01,  3.1594e-02,  ...,  1.4962e-01,\n",
            "          -8.4670e-02, -6.6508e-02],\n",
            "         [-6.6009e-02,  1.1437e-01,  5.1078e-02,  ...,  1.6806e-01,\n",
            "          -1.1069e-01, -4.8880e-02],\n",
            "         ...,\n",
            "         [-1.0173e-01,  1.5733e-01,  4.8436e-03,  ...,  1.1841e-01,\n",
            "          -9.2197e-02, -5.1776e-02],\n",
            "         [-9.4837e-02,  1.5516e-01,  7.0330e-02,  ...,  2.1509e-01,\n",
            "          -6.7642e-02, -7.1312e-02],\n",
            "         [-9.6608e-02,  1.0137e-01,  3.2663e-02,  ...,  1.0572e-01,\n",
            "          -1.0037e-01, -2.6238e-02]],\n",
            "\n",
            "        [[ 1.4310e-01, -3.7920e-02, -5.5593e-02,  ...,  1.3770e-01,\n",
            "           2.1242e-02, -9.2416e-02],\n",
            "         [ 1.7323e-01,  5.4815e-02, -9.1305e-02,  ...,  8.3027e-02,\n",
            "           4.8693e-02, -1.9237e-01],\n",
            "         [ 1.3553e-01,  1.7997e-02, -1.0340e-01,  ...,  1.1978e-01,\n",
            "           6.6533e-02, -1.0616e-01],\n",
            "         ...,\n",
            "         [ 1.4896e-01, -1.5391e-02, -1.0853e-01,  ...,  1.3591e-01,\n",
            "           6.5321e-02, -1.1262e-01],\n",
            "         [ 1.4896e-01, -1.5391e-02, -1.0853e-01,  ...,  1.3591e-01,\n",
            "           6.5321e-02, -1.1262e-01],\n",
            "         [ 1.4896e-01, -1.5391e-02, -1.0853e-01,  ...,  1.3591e-01,\n",
            "           6.5321e-02, -1.1262e-01]],\n",
            "\n",
            "        [[ 1.8515e-01,  4.3142e-03, -2.0083e-01,  ...,  3.8259e-02,\n",
            "           4.3632e-02, -1.0088e-01],\n",
            "         [ 1.8725e-01,  7.6248e-02, -2.6800e-01,  ...,  7.0880e-04,\n",
            "           9.4508e-03, -1.6518e-01],\n",
            "         [ 1.8161e-01,  4.1189e-02, -2.0936e-01,  ...,  2.8578e-02,\n",
            "           4.5781e-02, -1.3038e-01],\n",
            "         ...,\n",
            "         [ 1.8679e-01,  1.8291e-02, -2.3631e-01,  ...,  9.8314e-05,\n",
            "           4.7301e-02, -1.4593e-01],\n",
            "         [ 1.8679e-01,  1.8291e-02, -2.3631e-01,  ...,  9.8279e-05,\n",
            "           4.7301e-02, -1.4593e-01],\n",
            "         [ 1.8679e-01,  1.8291e-02, -2.3631e-01,  ...,  9.8279e-05,\n",
            "           4.7301e-02, -1.4593e-01]]], grad_fn=<ViewBackward0>)\n",
            "torch.Size([10, 20, 512])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "goX70VKqhxQH"
      },
      "source": [
        "## Req. 2-2 Multi-head self-attention 모듈 클래스 구현"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WtNyV7mMj7V_"
      },
      "source": [
        "위의 과정을 모두 합쳐 하나의 Multi-head attention 모듈 class를 구현하겠습니다.\n",
        "\n",
        "아래 코드의 TODO 부분을 채워주세요."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U_kNhOTrkBHm"
      },
      "source": [
        "class MultiheadAttention(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(MultiheadAttention, self).__init__()\n",
        "\n",
        "    # Q, K, V learnable matrices\n",
        "    self.w_q = nn.Linear(d_model, d_model)\n",
        "    self.w_k = nn.Linear(d_model, d_model)\n",
        "    self.w_v = nn.Linear(d_model, d_model)\n",
        "\n",
        "    # Linear projection for concatenated outputs\n",
        "    self.w_0 = nn.Linear(d_model, d_model)\n",
        "\n",
        "  # scaled-dot product attention\n",
        "  def self_attention(self, q, k, v):\n",
        "    attn_scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(d_k)  # (B, num_heads, L, L)\n",
        "    attn_dists = F.softmax(attn_scores, dim=-1)  # (B, num_heads, L, L)\n",
        "\n",
        "    attn_values = torch.matmul(attn_dists, v)  # (B, num_heads, L, d_k)\n",
        "\n",
        "    return attn_values\n",
        "\n",
        "  def forward(self, q, k, v):\n",
        "    batch_size = q.shape[0]\n",
        "\n",
        "    # linear projection\n",
        "    ################################################################################\n",
        "    # TODO 1: Implement the forward pass for linear projection.                #\n",
        "    ################################################################################\n",
        "    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "    q = self.w_q(q)  # (B, L, d_model)\n",
        "    k = self.w_k(k)  # (B, L, d_model)\n",
        "    v = self.w_v(v)  # (B, L, d_model)\n",
        "    # head만큼 쪼개준다\n",
        "    ################################################################################\n",
        "    # TODO 2: Implement the forward pass for \bsplit head.                #\n",
        "    ################################################################################\n",
        "    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "    q = q.view(batch_size, -1, num_heads, d_k)  # (B, L, num_heads, d_k)\n",
        "    k = k.view(batch_size, -1, num_heads, d_k)  # (B, L, num_heads, d_k)\n",
        "    v = v.view(batch_size, -1, num_heads, d_k)  # (B, L, num_heads, d_k)\n",
        "    # 각 head가 (L, d_k)의 matrix를 담당하도록 만든다\n",
        "    q = q.transpose(1, 2)  # (B, num_heads, L, d_k)\n",
        "    k = k.transpose(1, 2)  # (B, num_heads, L, d_k)\n",
        "    v = v.transpose(1, 2)  # (B, num_heads, L, d_k)\n",
        "\n",
        "    attn_values = self.self_attention(q, k, v)  # (B, num_heads, L, d_k)\n",
        "    attn_values = attn_values.transpose(1, 2).contiguous().view(batch_size, -1, d_model)  # (B, L, num_heads, d_k) => (B, L, d_model)\n",
        "\n",
        "    return self.w_0(attn_values)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jYLuu_9alQxT"
      },
      "source": [
        "multihead_attn = MultiheadAttention()\n",
        "\n",
        "outputs = multihead_attn(batch_emb, batch_emb, batch_emb)  # (B, L, d_model)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KMiXlYjSlTfB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b1316997-289c-49b3-e3d3-fff23e36007f"
      },
      "source": [
        "print(outputs)\n",
        "print(outputs.shape)  # (batch_size, length, d_model)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[-0.0110,  0.0470, -0.0620,  ..., -0.0875,  0.0084, -0.1096],\n",
            "         [ 0.0024,  0.0604, -0.0475,  ..., -0.0565,  0.0504, -0.1013],\n",
            "         [-0.0150, -0.0011, -0.0347,  ..., -0.0224,  0.0982, -0.1170],\n",
            "         ...,\n",
            "         [-0.0142,  0.1204, -0.0975,  ..., -0.1024, -0.0124, -0.1269],\n",
            "         [-0.0142,  0.1204, -0.0975,  ..., -0.1024, -0.0124, -0.1269],\n",
            "         [-0.0142,  0.1204, -0.0975,  ..., -0.1024, -0.0124, -0.1269]],\n",
            "\n",
            "        [[-0.2620, -0.0029, -0.0328,  ..., -0.2944, -0.1437, -0.1228],\n",
            "         [-0.2605, -0.1386, -0.0489,  ..., -0.2830, -0.1722, -0.0812],\n",
            "         [-0.2897, -0.0571, -0.0280,  ..., -0.3137, -0.1660, -0.1217],\n",
            "         ...,\n",
            "         [-0.3023, -0.0305, -0.0695,  ..., -0.3105, -0.2095, -0.1495],\n",
            "         [-0.3023, -0.0305, -0.0695,  ..., -0.3105, -0.2095, -0.1495],\n",
            "         [-0.3023, -0.0305, -0.0695,  ..., -0.3105, -0.2095, -0.1495]],\n",
            "\n",
            "        [[-0.1329, -0.0812, -0.1102,  ..., -0.2355, -0.2508, -0.1708],\n",
            "         [-0.0920, -0.0041, -0.0873,  ..., -0.2286, -0.1992, -0.0872],\n",
            "         [-0.1507, -0.0126, -0.0418,  ..., -0.1588, -0.2694, -0.1721],\n",
            "         ...,\n",
            "         [-0.1292, -0.0343, -0.0613,  ..., -0.2594, -0.2190, -0.1298],\n",
            "         [-0.1292, -0.0343, -0.0613,  ..., -0.2594, -0.2190, -0.1298],\n",
            "         [-0.1292, -0.0343, -0.0613,  ..., -0.2594, -0.2190, -0.1298]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ 0.1800, -0.0422, -0.1305,  ..., -0.1247,  0.1034,  0.0580],\n",
            "         [ 0.1711, -0.0221, -0.1020,  ..., -0.0909,  0.1033,  0.0581],\n",
            "         [ 0.1205, -0.0310, -0.0722,  ..., -0.0823,  0.0432,  0.0796],\n",
            "         ...,\n",
            "         [ 0.1712, -0.0014, -0.0276,  ..., -0.0539,  0.1190,  0.0932],\n",
            "         [ 0.1461, -0.0123, -0.0845,  ..., -0.0550,  0.1158,  0.0735],\n",
            "         [ 0.1727, -0.0457, -0.0380,  ..., -0.1075,  0.0681,  0.1187]],\n",
            "\n",
            "        [[ 0.0011, -0.1310,  0.0595,  ..., -0.0173,  0.0384, -0.0118],\n",
            "         [-0.0035, -0.0364,  0.1089,  ..., -0.0309,  0.0511, -0.0192],\n",
            "         [ 0.0161, -0.1059,  0.0831,  ..., -0.0287,  0.0025,  0.0098],\n",
            "         ...,\n",
            "         [ 0.0133, -0.0204,  0.0387,  ..., -0.0698, -0.0116, -0.0690],\n",
            "         [ 0.0133, -0.0204,  0.0387,  ..., -0.0698, -0.0116, -0.0690],\n",
            "         [ 0.0133, -0.0204,  0.0387,  ..., -0.0698, -0.0116, -0.0690]],\n",
            "\n",
            "        [[-0.0381,  0.0305, -0.0504,  ..., -0.1010, -0.1023, -0.1294],\n",
            "         [-0.0234,  0.0593, -0.0233,  ..., -0.1433, -0.1137, -0.0968],\n",
            "         [ 0.0165,  0.0011, -0.0450,  ..., -0.0932, -0.1120, -0.0896],\n",
            "         ...,\n",
            "         [ 0.0139,  0.0744, -0.0299,  ..., -0.1465, -0.1259, -0.1327],\n",
            "         [ 0.0139,  0.0744, -0.0299,  ..., -0.1465, -0.1259, -0.1327],\n",
            "         [ 0.0139,  0.0744, -0.0299,  ..., -0.1465, -0.1259, -0.1327]]],\n",
            "       grad_fn=<ViewBackward0>)\n",
            "torch.Size([10, 20, 512])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OTku1fySVR3L"
      },
      "source": [],
      "execution_count": 20,
      "outputs": []
    }
  ]
}