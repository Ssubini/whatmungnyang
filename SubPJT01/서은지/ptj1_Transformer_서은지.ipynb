{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Transformer"
      ],
      "metadata": {
        "id": "GiWulKAdmAHX"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9KsBGZpKkWki"
      },
      "source": [
        "Transformer의 핵심 구조인 Multi-head Attention을 구현하는 실습입니다.\n",
        "1. Multi-head attention 및 self-attention 구현.\n",
        "2. 각 과정에서 일어나는 연산과 input/output 형태 이해."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8qRU5DFY2OM8"
      },
      "source": [
        "### 필요 패키지 import"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lDtMioSQQ1bB"
      },
      "source": [
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "from tqdm import tqdm\n",
        "\n",
        "import torch\n",
        "import math"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Req. 2-1 Multi-head self-attention 구조 익히기"
      ],
      "metadata": {
        "id": "HH0VdC4uJJVG"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QBiZObgRep_Q"
      },
      "source": [
        "### **데이터 전처리**\n",
        "vocab_size 100인 가상의 시퀀스 데이터를 생성합니다. \n",
        "\n",
        "각 데이터에 할당된 숫자는 tokenizing과 정수화가 이뤄진 형태입니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e9ULZIqTenSc"
      },
      "source": [
        "pad_id = 0\n",
        "vocab_size = 100\n",
        "\n",
        "data = [\n",
        "  [62, 13, 47, 39, 78, 33, 56, 13, 39, 29, 44, 86, 71, 36, 18, 75],\n",
        "  [60, 96, 51, 32, 90],\n",
        "  [35, 45, 48, 65, 91, 99, 92, 10, 3, 21, 54],\n",
        "  [75, 51],\n",
        "  [66, 88, 98, 47],\n",
        "  [21, 39, 10, 64, 21],\n",
        "  [98],\n",
        "  [77, 65, 51, 77, 19, 15, 35, 19, 23, 97, 50, 46, 53, 42, 45, 91, 66, 3, 43, 10],\n",
        "  [70, 64, 98, 25, 99, 53, 4, 13, 69, 62, 66, 76, 15, 75, 45, 34],\n",
        "  [20, 64, 81, 35, 76, 85, 1, 62, 8, 45, 99, 77, 19, 43]\n",
        "]"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Hx3mcivgMyH"
      },
      "source": [
        "# 길이 맞춰주기 위해 패딩합니다.\n",
        "def padding(data):\n",
        "  max_len = len(max(data, key=len))\n",
        "  print(f\"Maximum sequence length: {max_len}\")\n",
        "\n",
        "  for i, seq in enumerate(tqdm(data)):\n",
        "    if len(seq) < max_len:\n",
        "      data[i] = seq + [pad_id] * (max_len - len(seq))\n",
        "\n",
        "  return data, max_len"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s3e8FiNvgX60",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd6f9e91-2eee-49d8-d7b5-632ef42864c6"
      },
      "source": [
        "data, max_len = padding(data)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Maximum sequence length: 20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10/10 [00:00<00:00, 55924.05it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hwPSIWYugaN0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "27acf5a0-37b7-4257-919a-4acdc4780183"
      },
      "source": [
        "data"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[62, 13, 47, 39, 78, 33, 56, 13, 39, 29, 44, 86, 71, 36, 18, 75, 0, 0, 0, 0],\n",
              " [60, 96, 51, 32, 90, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              " [35, 45, 48, 65, 91, 99, 92, 10, 3, 21, 54, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              " [75, 51, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              " [66, 88, 98, 47, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              " [21, 39, 10, 64, 21, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              " [98, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              " [77,\n",
              "  65,\n",
              "  51,\n",
              "  77,\n",
              "  19,\n",
              "  15,\n",
              "  35,\n",
              "  19,\n",
              "  23,\n",
              "  97,\n",
              "  50,\n",
              "  46,\n",
              "  53,\n",
              "  42,\n",
              "  45,\n",
              "  91,\n",
              "  66,\n",
              "  3,\n",
              "  43,\n",
              "  10],\n",
              " [70, 64, 98, 25, 99, 53, 4, 13, 69, 62, 66, 76, 15, 75, 45, 34, 0, 0, 0, 0],\n",
              " [20, 64, 81, 35, 76, 85, 1, 62, 8, 45, 99, 77, 19, 43, 0, 0, 0, 0, 0, 0]]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rwqjACx8iidc"
      },
      "source": [
        "### Hyperparameter 세팅 및 embedding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p-Ngp2nWimS8"
      },
      "source": [
        "d_model = 512  # model의 hidden size\n",
        "num_heads = 8  # head의 개수\n",
        "\n",
        "# d_model이 입력을 projection 시킬 임베딩 space의 차원이므로, num_heads로 나누어 떨어져야 한다."
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GJMi2Xsni5uq"
      },
      "source": [
        "embedding = nn.Embedding(vocab_size, d_model)\n",
        "\n",
        "# B: batch size, L: maximum sequence length\n",
        "batch = torch.LongTensor(data)  # (B, L)\n",
        "batch_emb = embedding(batch)  # (B, L, d_model)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3tLCUQwojcUb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "77f640e1-484d-4dfc-9cd0-dda74c2c0449"
      },
      "source": [
        "print(batch_emb)\n",
        "print(batch_emb.shape)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[ 0.0227, -0.7169, -1.0666,  ..., -0.0915, -2.4909,  0.6048],\n",
            "         [-0.0783, -0.1332,  0.2458,  ...,  0.9972,  0.4791, -0.1637],\n",
            "         [ 0.1512,  1.6798,  0.1060,  ...,  2.1160,  0.1851, -0.0871],\n",
            "         ...,\n",
            "         [-1.3291, -0.6499,  1.4460,  ..., -0.6595, -1.7504,  0.1500],\n",
            "         [-1.3291, -0.6499,  1.4460,  ..., -0.6595, -1.7504,  0.1500],\n",
            "         [-1.3291, -0.6499,  1.4460,  ..., -0.6595, -1.7504,  0.1500]],\n",
            "\n",
            "        [[-0.5863,  0.5547, -1.3158,  ..., -1.4122, -0.7298, -0.5086],\n",
            "         [ 0.1425,  1.7762, -0.1879,  ..., -0.2370, -0.6788, -1.2901],\n",
            "         [ 0.3872,  1.2684, -0.4120,  ...,  2.0639, -0.2172, -0.9816],\n",
            "         ...,\n",
            "         [-1.3291, -0.6499,  1.4460,  ..., -0.6595, -1.7504,  0.1500],\n",
            "         [-1.3291, -0.6499,  1.4460,  ..., -0.6595, -1.7504,  0.1500],\n",
            "         [-1.3291, -0.6499,  1.4460,  ..., -0.6595, -1.7504,  0.1500]],\n",
            "\n",
            "        [[-1.1946, -0.2537, -1.5862,  ...,  0.1333, -0.1476,  0.0090],\n",
            "         [ 0.3754,  0.9026, -0.3693,  ...,  1.8796, -1.0774, -1.1831],\n",
            "         [ 0.4404,  0.6776, -0.7431,  ..., -0.7630, -1.2796, -1.3842],\n",
            "         ...,\n",
            "         [-1.3291, -0.6499,  1.4460,  ..., -0.6595, -1.7504,  0.1500],\n",
            "         [-1.3291, -0.6499,  1.4460,  ..., -0.6595, -1.7504,  0.1500],\n",
            "         [-1.3291, -0.6499,  1.4460,  ..., -0.6595, -1.7504,  0.1500]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[-0.3855, -0.1759, -2.1743,  ...,  1.3177, -0.5530, -0.7240],\n",
            "         [-0.8694, -0.0243,  0.0070,  ..., -0.5680,  0.0265, -0.8710],\n",
            "         [ 0.3872,  1.2684, -0.4120,  ...,  2.0639, -0.2172, -0.9816],\n",
            "         ...,\n",
            "         [-0.1331,  0.4934,  0.7522,  ...,  0.2884,  0.0656, -0.9304],\n",
            "         [-0.2386,  1.5670,  0.3516,  ...,  0.0628, -1.2352, -0.1293],\n",
            "         [ 1.1446, -0.0990,  0.4120,  ...,  0.3632,  1.6393,  1.3598]],\n",
            "\n",
            "        [[-0.4882, -0.0728,  1.6378,  ...,  0.1711, -0.3273,  1.7690],\n",
            "         [ 0.9135,  1.4997, -1.4185,  ..., -1.5729,  1.4091, -0.2535],\n",
            "         [ 1.0480,  1.9376, -1.0472,  ..., -0.9004, -0.7585,  0.1575],\n",
            "         ...,\n",
            "         [-1.3291, -0.6499,  1.4460,  ..., -0.6595, -1.7504,  0.1500],\n",
            "         [-1.3291, -0.6499,  1.4460,  ..., -0.6595, -1.7504,  0.1500],\n",
            "         [-1.3291, -0.6499,  1.4460,  ..., -0.6595, -1.7504,  0.1500]],\n",
            "\n",
            "        [[-0.3145, -0.6628, -1.5134,  ..., -1.6062,  0.4141, -0.8649],\n",
            "         [ 0.9135,  1.4997, -1.4185,  ..., -1.5729,  1.4091, -0.2535],\n",
            "         [ 0.8499, -0.8006,  0.4083,  ..., -0.3489, -1.1552, -0.0490],\n",
            "         ...,\n",
            "         [-1.3291, -0.6499,  1.4460,  ..., -0.6595, -1.7504,  0.1500],\n",
            "         [-1.3291, -0.6499,  1.4460,  ..., -0.6595, -1.7504,  0.1500],\n",
            "         [-1.3291, -0.6499,  1.4460,  ..., -0.6595, -1.7504,  0.1500]]],\n",
            "       grad_fn=<EmbeddingBackward0>)\n",
            "torch.Size([10, 20, 512])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s0Lhx892gmi3"
      },
      "source": [
        "### Linear projection & 여러 head로 나누기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "urXMBRnRgqvw"
      },
      "source": [
        "Multi-head attention 내에서 쓰이는 linear projection matrix들을 정의합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9DWKDqgCgfMk"
      },
      "source": [
        "w_q = nn.Linear(d_model, d_model)\n",
        "w_k = nn.Linear(d_model, d_model)\n",
        "w_v = nn.Linear(d_model, d_model)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tcLuhda7m-Lm"
      },
      "source": [
        "w_0 = nn.Linear(d_model, d_model)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H-vSL7PwnV6k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e638d577-e4fe-4e21-cb56-e95696488a06"
      },
      "source": [
        "q = w_q(batch_emb)  # (B, L, d_model)\n",
        "k = w_k(batch_emb)  # (B, L, d_model)\n",
        "v = w_v(batch_emb)  # (B, L, d_model)\n",
        "\n",
        "print(q.shape)\n",
        "print(k.shape)\n",
        "print(v.shape)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([10, 20, 512])\n",
            "torch.Size([10, 20, 512])\n",
            "torch.Size([10, 20, 512])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wnvlum-LnF1T"
      },
      "source": [
        "Q, k, v를 `num_head`개의 차원 분할된 여러 vector로 만듭니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sXcYLZYvJT_1"
      },
      "source": [
        "- 이론적으로는 multi-head attention을 수행하면 input을 각각 다른 head 개수만큼의 Wq, Wk, Wv로 linear transformation 해서 각각 여러번의 attention 수행한 후 concat 한 후 linear transformation 수행해준다\n",
        "- 구현에서는 Wq, Wk, Wv 한 개씩\n",
        "- 실제 `attention is all you need` 논문의 구현 예시는 Query vector 한개를 dim으로 쪼개서 진행한다"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_tiOKAv9nEli",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dc2c3503-a34b-47b4-8b52-69da3a8fd9db"
      },
      "source": [
        "batch_size = q.shape[0]\n",
        "d_k = d_model // num_heads\n",
        "\n",
        "# num_heads * d_k로 쪼갠다\n",
        "q = q.view(batch_size, -1, num_heads, d_k)  # (B, L, num_heads, d_k)\n",
        "k = k.view(batch_size, -1, num_heads, d_k)  # (B, L, num_heads, d_k)\n",
        "v = v.view(batch_size, -1, num_heads, d_k)  # (B, L, num_heads, d_k)\n",
        "\n",
        "print(q.shape)\n",
        "print(k.shape)\n",
        "print(v.shape)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([10, 20, 8, 64])\n",
            "torch.Size([10, 20, 8, 64])\n",
            "torch.Size([10, 20, 8, 64])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5tNb2isfn5Cx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4c65468a-efc3-4941-d088-2e0ccc77ae03"
      },
      "source": [
        "# num_heads를 밖으로 뺌으로써\n",
        "# 각 head가 (L, d_k) 만큼의 matrix를 가지고 self-attention 수행\n",
        "\n",
        "q = q.transpose(1, 2)  # (B, num_heads, L, d_k)\n",
        "k = k.transpose(1, 2)  # (B, num_heads, L, d_k)\n",
        "v = v.transpose(1, 2)  # (B, num_heads, L, d_k)\n",
        "\n",
        "print(q.shape)\n",
        "print(k.shape)\n",
        "print(v.shape)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([10, 8, 20, 64])\n",
            "torch.Size([10, 8, 20, 64])\n",
            "torch.Size([10, 8, 20, 64])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NWrDA5_Sofad"
      },
      "source": [
        "### Scaled dot-product self-attention 구현"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w52C4k3Wfl8m"
      },
      "source": [
        "각 head에서 실행되는 self-attetion 과정입니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A5waKr0Hfi2K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bf97c90d-0f57-4dd9-b438-b63ec36ec60b"
      },
      "source": [
        "# shape - (L, L)\n",
        "# 같은 sequence 내에 서로 다른 token들에게 얼마나 가중치를 두고 attention을 해야하는가\n",
        "attn_scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(d_k)  # (B, num_heads, L, L)\n",
        "# softmax - row-wise이기 때문에 dim은 -1\n",
        "attn_dists = F.softmax(attn_scores, dim=-1)  # (B, num_heads, L, L)\n",
        "\n",
        "print(attn_dists)\n",
        "print(attn_dists.shape)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[[0.0308, 0.0823, 0.0586,  ..., 0.0437, 0.0437, 0.0437],\n",
            "          [0.0589, 0.0795, 0.0593,  ..., 0.0325, 0.0325, 0.0325],\n",
            "          [0.0417, 0.0448, 0.0348,  ..., 0.0507, 0.0507, 0.0507],\n",
            "          ...,\n",
            "          [0.0412, 0.0508, 0.0277,  ..., 0.0433, 0.0433, 0.0433],\n",
            "          [0.0412, 0.0508, 0.0277,  ..., 0.0433, 0.0433, 0.0433],\n",
            "          [0.0412, 0.0508, 0.0277,  ..., 0.0433, 0.0433, 0.0433]],\n",
            "\n",
            "         [[0.0502, 0.0701, 0.0431,  ..., 0.0451, 0.0451, 0.0451],\n",
            "          [0.0365, 0.0530, 0.0504,  ..., 0.0483, 0.0483, 0.0483],\n",
            "          [0.0667, 0.0509, 0.0410,  ..., 0.0611, 0.0611, 0.0611],\n",
            "          ...,\n",
            "          [0.0341, 0.0499, 0.0515,  ..., 0.0365, 0.0365, 0.0365],\n",
            "          [0.0341, 0.0499, 0.0515,  ..., 0.0365, 0.0365, 0.0365],\n",
            "          [0.0341, 0.0499, 0.0515,  ..., 0.0365, 0.0365, 0.0365]],\n",
            "\n",
            "         [[0.0909, 0.0403, 0.0583,  ..., 0.0514, 0.0514, 0.0514],\n",
            "          [0.0405, 0.0422, 0.0721,  ..., 0.0436, 0.0436, 0.0436],\n",
            "          [0.0615, 0.0378, 0.0598,  ..., 0.0308, 0.0308, 0.0308],\n",
            "          ...,\n",
            "          [0.0798, 0.0452, 0.0674,  ..., 0.0484, 0.0484, 0.0484],\n",
            "          [0.0798, 0.0452, 0.0674,  ..., 0.0484, 0.0484, 0.0484],\n",
            "          [0.0798, 0.0452, 0.0674,  ..., 0.0484, 0.0484, 0.0484]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[0.0409, 0.0379, 0.0407,  ..., 0.0579, 0.0579, 0.0579],\n",
            "          [0.0442, 0.0558, 0.0519,  ..., 0.0500, 0.0500, 0.0500],\n",
            "          [0.0565, 0.0682, 0.0502,  ..., 0.0328, 0.0328, 0.0328],\n",
            "          ...,\n",
            "          [0.0390, 0.0455, 0.0245,  ..., 0.0386, 0.0386, 0.0386],\n",
            "          [0.0390, 0.0455, 0.0245,  ..., 0.0386, 0.0386, 0.0386],\n",
            "          [0.0390, 0.0455, 0.0245,  ..., 0.0386, 0.0386, 0.0386]],\n",
            "\n",
            "         [[0.0274, 0.0443, 0.0397,  ..., 0.0370, 0.0370, 0.0370],\n",
            "          [0.0563, 0.0529, 0.0407,  ..., 0.0381, 0.0381, 0.0381],\n",
            "          [0.0422, 0.0425, 0.0390,  ..., 0.0497, 0.0497, 0.0497],\n",
            "          ...,\n",
            "          [0.0652, 0.0650, 0.0530,  ..., 0.0518, 0.0518, 0.0518],\n",
            "          [0.0652, 0.0650, 0.0530,  ..., 0.0518, 0.0518, 0.0518],\n",
            "          [0.0652, 0.0650, 0.0530,  ..., 0.0518, 0.0518, 0.0518]],\n",
            "\n",
            "         [[0.0313, 0.0300, 0.0569,  ..., 0.0387, 0.0387, 0.0387],\n",
            "          [0.0687, 0.0646, 0.0758,  ..., 0.0444, 0.0444, 0.0444],\n",
            "          [0.0646, 0.0377, 0.0552,  ..., 0.0640, 0.0640, 0.0640],\n",
            "          ...,\n",
            "          [0.0465, 0.0384, 0.0543,  ..., 0.0529, 0.0529, 0.0529],\n",
            "          [0.0465, 0.0384, 0.0543,  ..., 0.0529, 0.0529, 0.0529],\n",
            "          [0.0465, 0.0384, 0.0543,  ..., 0.0529, 0.0529, 0.0529]]],\n",
            "\n",
            "\n",
            "        [[[0.0437, 0.1071, 0.0467,  ..., 0.0410, 0.0410, 0.0410],\n",
            "          [0.0374, 0.0568, 0.0773,  ..., 0.0501, 0.0501, 0.0501],\n",
            "          [0.0651, 0.0950, 0.0928,  ..., 0.0414, 0.0414, 0.0414],\n",
            "          ...,\n",
            "          [0.0439, 0.0694, 0.0525,  ..., 0.0449, 0.0449, 0.0449],\n",
            "          [0.0439, 0.0694, 0.0525,  ..., 0.0449, 0.0449, 0.0449],\n",
            "          [0.0439, 0.0694, 0.0525,  ..., 0.0449, 0.0449, 0.0449]],\n",
            "\n",
            "         [[0.0368, 0.0411, 0.0384,  ..., 0.0558, 0.0558, 0.0558],\n",
            "          [0.0678, 0.0438, 0.0490,  ..., 0.0503, 0.0503, 0.0503],\n",
            "          [0.0676, 0.1063, 0.0683,  ..., 0.0386, 0.0386, 0.0386],\n",
            "          ...,\n",
            "          [0.1088, 0.0620, 0.0746,  ..., 0.0448, 0.0448, 0.0448],\n",
            "          [0.1088, 0.0620, 0.0746,  ..., 0.0448, 0.0448, 0.0448],\n",
            "          [0.1088, 0.0620, 0.0746,  ..., 0.0448, 0.0448, 0.0448]],\n",
            "\n",
            "         [[0.0555, 0.0501, 0.0648,  ..., 0.0505, 0.0505, 0.0505],\n",
            "          [0.0679, 0.0408, 0.0468,  ..., 0.0511, 0.0511, 0.0511],\n",
            "          [0.0594, 0.0597, 0.0598,  ..., 0.0437, 0.0437, 0.0437],\n",
            "          ...,\n",
            "          [0.0394, 0.0407, 0.0677,  ..., 0.0484, 0.0484, 0.0484],\n",
            "          [0.0394, 0.0407, 0.0677,  ..., 0.0484, 0.0484, 0.0484],\n",
            "          [0.0394, 0.0407, 0.0677,  ..., 0.0484, 0.0484, 0.0484]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[0.0517, 0.0302, 0.0726,  ..., 0.0506, 0.0506, 0.0506],\n",
            "          [0.0558, 0.0485, 0.0438,  ..., 0.0501, 0.0501, 0.0501],\n",
            "          [0.0700, 0.0201, 0.0995,  ..., 0.0499, 0.0499, 0.0499],\n",
            "          ...,\n",
            "          [0.0889, 0.0799, 0.0393,  ..., 0.0463, 0.0463, 0.0463],\n",
            "          [0.0889, 0.0799, 0.0393,  ..., 0.0463, 0.0463, 0.0463],\n",
            "          [0.0889, 0.0799, 0.0393,  ..., 0.0463, 0.0463, 0.0463]],\n",
            "\n",
            "         [[0.0351, 0.0247, 0.0282,  ..., 0.0574, 0.0574, 0.0574],\n",
            "          [0.0414, 0.0317, 0.0590,  ..., 0.0484, 0.0484, 0.0484],\n",
            "          [0.0321, 0.0627, 0.0223,  ..., 0.0540, 0.0540, 0.0540],\n",
            "          ...,\n",
            "          [0.0247, 0.0721, 0.0478,  ..., 0.0496, 0.0496, 0.0496],\n",
            "          [0.0247, 0.0721, 0.0478,  ..., 0.0496, 0.0496, 0.0496],\n",
            "          [0.0247, 0.0721, 0.0478,  ..., 0.0496, 0.0496, 0.0496]],\n",
            "\n",
            "         [[0.0608, 0.0597, 0.0950,  ..., 0.0435, 0.0435, 0.0435],\n",
            "          [0.0531, 0.0187, 0.0322,  ..., 0.0532, 0.0532, 0.0532],\n",
            "          [0.0933, 0.0748, 0.0469,  ..., 0.0411, 0.0411, 0.0411],\n",
            "          ...,\n",
            "          [0.0489, 0.0624, 0.0869,  ..., 0.0466, 0.0466, 0.0466],\n",
            "          [0.0489, 0.0624, 0.0869,  ..., 0.0466, 0.0466, 0.0466],\n",
            "          [0.0489, 0.0624, 0.0869,  ..., 0.0466, 0.0466, 0.0466]]],\n",
            "\n",
            "\n",
            "        [[[0.0571, 0.0249, 0.0658,  ..., 0.0524, 0.0524, 0.0524],\n",
            "          [0.0315, 0.0201, 0.0265,  ..., 0.0564, 0.0564, 0.0564],\n",
            "          [0.0531, 0.0816, 0.0650,  ..., 0.0469, 0.0469, 0.0469],\n",
            "          ...,\n",
            "          [0.0354, 0.0570, 0.0628,  ..., 0.0470, 0.0470, 0.0470],\n",
            "          [0.0354, 0.0570, 0.0628,  ..., 0.0470, 0.0470, 0.0470],\n",
            "          [0.0354, 0.0570, 0.0628,  ..., 0.0470, 0.0470, 0.0470]],\n",
            "\n",
            "         [[0.0481, 0.0362, 0.0733,  ..., 0.0455, 0.0455, 0.0455],\n",
            "          [0.0718, 0.0642, 0.0374,  ..., 0.0428, 0.0428, 0.0428],\n",
            "          [0.0387, 0.0361, 0.0690,  ..., 0.0405, 0.0405, 0.0405],\n",
            "          ...,\n",
            "          [0.0579, 0.1030, 0.0685,  ..., 0.0363, 0.0363, 0.0363],\n",
            "          [0.0579, 0.1030, 0.0685,  ..., 0.0363, 0.0363, 0.0363],\n",
            "          [0.0579, 0.1030, 0.0685,  ..., 0.0363, 0.0363, 0.0363]],\n",
            "\n",
            "         [[0.0762, 0.0469, 0.0893,  ..., 0.0345, 0.0345, 0.0345],\n",
            "          [0.0215, 0.0544, 0.0348,  ..., 0.0614, 0.0614, 0.0614],\n",
            "          [0.0398, 0.0366, 0.0477,  ..., 0.0631, 0.0631, 0.0631],\n",
            "          ...,\n",
            "          [0.0697, 0.0361, 0.0462,  ..., 0.0448, 0.0448, 0.0448],\n",
            "          [0.0697, 0.0361, 0.0462,  ..., 0.0448, 0.0448, 0.0448],\n",
            "          [0.0697, 0.0361, 0.0462,  ..., 0.0448, 0.0448, 0.0448]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[0.0700, 0.0812, 0.0488,  ..., 0.0342, 0.0342, 0.0342],\n",
            "          [0.0449, 0.0542, 0.0448,  ..., 0.0453, 0.0453, 0.0453],\n",
            "          [0.0816, 0.0536, 0.0483,  ..., 0.0415, 0.0415, 0.0415],\n",
            "          ...,\n",
            "          [0.0377, 0.0417, 0.0533,  ..., 0.0430, 0.0430, 0.0430],\n",
            "          [0.0377, 0.0417, 0.0533,  ..., 0.0430, 0.0430, 0.0430],\n",
            "          [0.0377, 0.0417, 0.0533,  ..., 0.0430, 0.0430, 0.0430]],\n",
            "\n",
            "         [[0.0556, 0.0240, 0.0255,  ..., 0.0602, 0.0602, 0.0602],\n",
            "          [0.0445, 0.0440, 0.0715,  ..., 0.0500, 0.0500, 0.0500],\n",
            "          [0.0681, 0.0335, 0.0344,  ..., 0.0512, 0.0512, 0.0512],\n",
            "          ...,\n",
            "          [0.0237, 0.0222, 0.0676,  ..., 0.0487, 0.0487, 0.0487],\n",
            "          [0.0237, 0.0222, 0.0676,  ..., 0.0487, 0.0487, 0.0487],\n",
            "          [0.0237, 0.0222, 0.0676,  ..., 0.0487, 0.0487, 0.0487]],\n",
            "\n",
            "         [[0.0394, 0.0516, 0.0369,  ..., 0.0478, 0.0478, 0.0478],\n",
            "          [0.0263, 0.0293, 0.0642,  ..., 0.0573, 0.0573, 0.0573],\n",
            "          [0.0675, 0.0421, 0.0628,  ..., 0.0494, 0.0494, 0.0494],\n",
            "          ...,\n",
            "          [0.0448, 0.0722, 0.0471,  ..., 0.0518, 0.0518, 0.0518],\n",
            "          [0.0448, 0.0722, 0.0471,  ..., 0.0518, 0.0518, 0.0518],\n",
            "          [0.0448, 0.0722, 0.0471,  ..., 0.0518, 0.0518, 0.0518]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[0.0316, 0.0633, 0.0661,  ..., 0.0307, 0.0584, 0.0474],\n",
            "          [0.0426, 0.0386, 0.0554,  ..., 0.0488, 0.0586, 0.0653],\n",
            "          [0.0340, 0.0880, 0.0831,  ..., 0.0329, 0.0364, 0.0390],\n",
            "          ...,\n",
            "          [0.0383, 0.0510, 0.0529,  ..., 0.0654, 0.0296, 0.0488],\n",
            "          [0.0530, 0.0471, 0.0458,  ..., 0.0293, 0.0369, 0.0867],\n",
            "          [0.0463, 0.0564, 0.0407,  ..., 0.0439, 0.0360, 0.0472]],\n",
            "\n",
            "         [[0.0710, 0.0539, 0.0288,  ..., 0.0375, 0.0314, 0.0462],\n",
            "          [0.0360, 0.0300, 0.0767,  ..., 0.0572, 0.0898, 0.0397],\n",
            "          [0.0628, 0.0602, 0.0654,  ..., 0.0451, 0.0631, 0.0580],\n",
            "          ...,\n",
            "          [0.0439, 0.0913, 0.0445,  ..., 0.0201, 0.0735, 0.0404],\n",
            "          [0.0428, 0.0496, 0.0407,  ..., 0.0373, 0.0727, 0.0496],\n",
            "          [0.0869, 0.0336, 0.0258,  ..., 0.0387, 0.0560, 0.0451]],\n",
            "\n",
            "         [[0.0456, 0.0428, 0.0490,  ..., 0.0416, 0.0346, 0.0562],\n",
            "          [0.0395, 0.0497, 0.0300,  ..., 0.0313, 0.0475, 0.0548],\n",
            "          [0.0476, 0.0372, 0.0524,  ..., 0.0306, 0.0380, 0.0563],\n",
            "          ...,\n",
            "          [0.0606, 0.0384, 0.0396,  ..., 0.0488, 0.0656, 0.0670],\n",
            "          [0.0316, 0.0412, 0.0361,  ..., 0.0261, 0.0326, 0.0430],\n",
            "          [0.0538, 0.0700, 0.0476,  ..., 0.0390, 0.0567, 0.0417]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[0.0838, 0.0481, 0.0574,  ..., 0.0312, 0.0261, 0.0259],\n",
            "          [0.0332, 0.0435, 0.1228,  ..., 0.0767, 0.0489, 0.0561],\n",
            "          [0.0292, 0.0445, 0.1216,  ..., 0.0308, 0.0574, 0.0509],\n",
            "          ...,\n",
            "          [0.0291, 0.0309, 0.0410,  ..., 0.1046, 0.0349, 0.0743],\n",
            "          [0.0210, 0.0446, 0.1080,  ..., 0.0459, 0.0534, 0.0343],\n",
            "          [0.0484, 0.0403, 0.0478,  ..., 0.0668, 0.0893, 0.0327]],\n",
            "\n",
            "         [[0.0420, 0.0540, 0.0343,  ..., 0.0355, 0.0684, 0.0448],\n",
            "          [0.0417, 0.0558, 0.0345,  ..., 0.0922, 0.0509, 0.0662],\n",
            "          [0.0573, 0.0536, 0.0252,  ..., 0.0523, 0.0532, 0.0332],\n",
            "          ...,\n",
            "          [0.0923, 0.0294, 0.0635,  ..., 0.0285, 0.0428, 0.0328],\n",
            "          [0.0549, 0.0382, 0.0306,  ..., 0.0337, 0.0323, 0.0437],\n",
            "          [0.0830, 0.0418, 0.0550,  ..., 0.0336, 0.0409, 0.0488]],\n",
            "\n",
            "         [[0.0357, 0.0942, 0.0445,  ..., 0.0228, 0.0566, 0.0537],\n",
            "          [0.0486, 0.0864, 0.0480,  ..., 0.0266, 0.0313, 0.0312],\n",
            "          [0.0366, 0.0412, 0.0356,  ..., 0.0479, 0.0475, 0.0247],\n",
            "          ...,\n",
            "          [0.0726, 0.0840, 0.0612,  ..., 0.0248, 0.0527, 0.0587],\n",
            "          [0.0549, 0.0437, 0.0275,  ..., 0.0546, 0.0339, 0.0635],\n",
            "          [0.0721, 0.0572, 0.0386,  ..., 0.0373, 0.0556, 0.0507]]],\n",
            "\n",
            "\n",
            "        [[[0.0422, 0.0594, 0.0581,  ..., 0.0627, 0.0627, 0.0627],\n",
            "          [0.0475, 0.0758, 0.0514,  ..., 0.0463, 0.0463, 0.0463],\n",
            "          [0.0168, 0.0736, 0.0497,  ..., 0.0541, 0.0541, 0.0541],\n",
            "          ...,\n",
            "          [0.0377, 0.0589, 0.0655,  ..., 0.0433, 0.0433, 0.0433],\n",
            "          [0.0377, 0.0589, 0.0655,  ..., 0.0433, 0.0433, 0.0433],\n",
            "          [0.0377, 0.0589, 0.0655,  ..., 0.0433, 0.0433, 0.0433]],\n",
            "\n",
            "         [[0.0381, 0.0430, 0.0385,  ..., 0.0493, 0.0493, 0.0493],\n",
            "          [0.0364, 0.0393, 0.0364,  ..., 0.0453, 0.0453, 0.0453],\n",
            "          [0.0312, 0.0520, 0.0386,  ..., 0.0607, 0.0607, 0.0607],\n",
            "          ...,\n",
            "          [0.0623, 0.0701, 0.0441,  ..., 0.0348, 0.0348, 0.0348],\n",
            "          [0.0623, 0.0701, 0.0441,  ..., 0.0348, 0.0348, 0.0348],\n",
            "          [0.0623, 0.0701, 0.0441,  ..., 0.0348, 0.0348, 0.0348]],\n",
            "\n",
            "         [[0.0587, 0.0515, 0.0348,  ..., 0.0698, 0.0698, 0.0698],\n",
            "          [0.0419, 0.0592, 0.0291,  ..., 0.0633, 0.0633, 0.0633],\n",
            "          [0.0309, 0.0344, 0.0325,  ..., 0.0342, 0.0342, 0.0342],\n",
            "          ...,\n",
            "          [0.0432, 0.0656, 0.0501,  ..., 0.0399, 0.0399, 0.0399],\n",
            "          [0.0432, 0.0656, 0.0501,  ..., 0.0399, 0.0399, 0.0399],\n",
            "          [0.0432, 0.0656, 0.0501,  ..., 0.0399, 0.0399, 0.0399]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[0.0443, 0.0359, 0.0634,  ..., 0.0426, 0.0426, 0.0426],\n",
            "          [0.0355, 0.0318, 0.0283,  ..., 0.0477, 0.0477, 0.0477],\n",
            "          [0.0424, 0.0267, 0.0975,  ..., 0.0414, 0.0414, 0.0414],\n",
            "          ...,\n",
            "          [0.0796, 0.0332, 0.0705,  ..., 0.0412, 0.0412, 0.0412],\n",
            "          [0.0796, 0.0332, 0.0705,  ..., 0.0412, 0.0412, 0.0412],\n",
            "          [0.0796, 0.0332, 0.0705,  ..., 0.0412, 0.0412, 0.0412]],\n",
            "\n",
            "         [[0.0588, 0.0465, 0.0357,  ..., 0.0426, 0.0426, 0.0426],\n",
            "          [0.0426, 0.0356, 0.0480,  ..., 0.0729, 0.0729, 0.0729],\n",
            "          [0.0662, 0.0487, 0.0231,  ..., 0.0470, 0.0470, 0.0470],\n",
            "          ...,\n",
            "          [0.0445, 0.0462, 0.0834,  ..., 0.0436, 0.0436, 0.0436],\n",
            "          [0.0445, 0.0462, 0.0834,  ..., 0.0436, 0.0436, 0.0436],\n",
            "          [0.0445, 0.0462, 0.0834,  ..., 0.0436, 0.0436, 0.0436]],\n",
            "\n",
            "         [[0.0423, 0.0447, 0.0628,  ..., 0.0447, 0.0447, 0.0447],\n",
            "          [0.0405, 0.0515, 0.0462,  ..., 0.0634, 0.0634, 0.0634],\n",
            "          [0.0804, 0.0400, 0.0475,  ..., 0.0593, 0.0593, 0.0593],\n",
            "          ...,\n",
            "          [0.0412, 0.0582, 0.0428,  ..., 0.0480, 0.0480, 0.0480],\n",
            "          [0.0412, 0.0582, 0.0428,  ..., 0.0480, 0.0480, 0.0480],\n",
            "          [0.0412, 0.0582, 0.0428,  ..., 0.0480, 0.0480, 0.0480]]],\n",
            "\n",
            "\n",
            "        [[[0.0541, 0.0516, 0.0659,  ..., 0.0372, 0.0372, 0.0372],\n",
            "          [0.0637, 0.0728, 0.0419,  ..., 0.0445, 0.0445, 0.0445],\n",
            "          [0.0454, 0.0404, 0.0421,  ..., 0.0507, 0.0507, 0.0507],\n",
            "          ...,\n",
            "          [0.0512, 0.0605, 0.0622,  ..., 0.0445, 0.0445, 0.0445],\n",
            "          [0.0512, 0.0605, 0.0622,  ..., 0.0445, 0.0445, 0.0445],\n",
            "          [0.0512, 0.0605, 0.0622,  ..., 0.0445, 0.0445, 0.0445]],\n",
            "\n",
            "         [[0.0340, 0.0356, 0.0481,  ..., 0.0625, 0.0625, 0.0625],\n",
            "          [0.0632, 0.0399, 0.0737,  ..., 0.0460, 0.0460, 0.0460],\n",
            "          [0.0343, 0.0590, 0.0425,  ..., 0.0557, 0.0557, 0.0557],\n",
            "          ...,\n",
            "          [0.0597, 0.0755, 0.0399,  ..., 0.0375, 0.0375, 0.0375],\n",
            "          [0.0597, 0.0755, 0.0399,  ..., 0.0375, 0.0375, 0.0375],\n",
            "          [0.0597, 0.0755, 0.0399,  ..., 0.0375, 0.0375, 0.0375]],\n",
            "\n",
            "         [[0.0962, 0.0499, 0.0673,  ..., 0.0432, 0.0432, 0.0432],\n",
            "          [0.0435, 0.0530, 0.0283,  ..., 0.0567, 0.0567, 0.0567],\n",
            "          [0.0744, 0.0712, 0.0596,  ..., 0.0434, 0.0434, 0.0434],\n",
            "          ...,\n",
            "          [0.0500, 0.0690, 0.0800,  ..., 0.0420, 0.0420, 0.0420],\n",
            "          [0.0500, 0.0690, 0.0800,  ..., 0.0420, 0.0420, 0.0420],\n",
            "          [0.0500, 0.0690, 0.0800,  ..., 0.0420, 0.0420, 0.0420]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[0.0482, 0.0457, 0.0375,  ..., 0.0370, 0.0370, 0.0370],\n",
            "          [0.0661, 0.0301, 0.0400,  ..., 0.0452, 0.0452, 0.0452],\n",
            "          [0.0360, 0.0682, 0.0725,  ..., 0.0573, 0.0573, 0.0573],\n",
            "          ...,\n",
            "          [0.0348, 0.0381, 0.0497,  ..., 0.0473, 0.0473, 0.0473],\n",
            "          [0.0348, 0.0381, 0.0497,  ..., 0.0473, 0.0473, 0.0473],\n",
            "          [0.0348, 0.0381, 0.0497,  ..., 0.0473, 0.0473, 0.0473]],\n",
            "\n",
            "         [[0.0571, 0.0530, 0.0591,  ..., 0.0474, 0.0474, 0.0474],\n",
            "          [0.0456, 0.0334, 0.0259,  ..., 0.0683, 0.0683, 0.0683],\n",
            "          [0.0265, 0.0464, 0.0380,  ..., 0.0469, 0.0469, 0.0469],\n",
            "          ...,\n",
            "          [0.0591, 0.0512, 0.0672,  ..., 0.0484, 0.0484, 0.0484],\n",
            "          [0.0591, 0.0512, 0.0672,  ..., 0.0484, 0.0484, 0.0484],\n",
            "          [0.0591, 0.0512, 0.0672,  ..., 0.0484, 0.0484, 0.0484]],\n",
            "\n",
            "         [[0.0476, 0.0573, 0.0390,  ..., 0.0407, 0.0407, 0.0407],\n",
            "          [0.0700, 0.0440, 0.0334,  ..., 0.0542, 0.0542, 0.0542],\n",
            "          [0.0598, 0.0495, 0.0310,  ..., 0.0506, 0.0506, 0.0506],\n",
            "          ...,\n",
            "          [0.0844, 0.0599, 0.0466,  ..., 0.0494, 0.0494, 0.0494],\n",
            "          [0.0844, 0.0599, 0.0466,  ..., 0.0494, 0.0494, 0.0494],\n",
            "          [0.0844, 0.0599, 0.0466,  ..., 0.0494, 0.0494, 0.0494]]]],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "torch.Size([10, 8, 20, 20])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7megouWpgCck",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bf90e4bb-aa63-4cd2-eb07-c85f520651f7"
      },
      "source": [
        "attn_values = torch.matmul(attn_dists, v)  # (B, num_heads, L, d_k)\n",
        "\n",
        "print(attn_values.shape)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([10, 8, 20, 64])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LmSTaymdg-P_"
      },
      "source": [
        "### 각 head의 결과물 병합"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YSdQZCk0hCNd"
      },
      "source": [
        "각 head의 결과물을 concat하고 동일 차원으로 linear projection합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eaK0bpMGhQZ2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bec7664a-bac8-46e6-f4af-1b56ec417ef6"
      },
      "source": [
        "attn_values = attn_values.transpose(1, 2)  # (B, L, num_heads, d_k)\n",
        "attn_values = attn_values.contiguous().view(batch_size, -1, d_model)  # (B, L, d_model)\n",
        "\n",
        "print(attn_values.shape)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([10, 20, 512])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LTng_2SXhdH1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0b24680f-4e95-44a5-f4be-c543c3f92f57"
      },
      "source": [
        "# w_0 : (d_model, d_model)\n",
        "# 서로 다른 의미로 foucsing 된 각 head의 self-attention 정보들을 합쳐주는 역할 수행\n",
        "outputs = w_0(attn_values)\n",
        "\n",
        "print(outputs)\n",
        "print(outputs.shape)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[ 0.1001, -0.0317,  0.0495,  ..., -0.0762,  0.0256, -0.0998],\n",
            "         [ 0.1160, -0.0281,  0.0075,  ..., -0.0090,  0.0475, -0.0746],\n",
            "         [ 0.0866, -0.0017,  0.0315,  ...,  0.0158,  0.0203, -0.1333],\n",
            "         ...,\n",
            "         [ 0.1087,  0.0687,  0.0247,  ..., -0.0050,  0.0923, -0.0934],\n",
            "         [ 0.1087,  0.0687,  0.0247,  ..., -0.0050,  0.0923, -0.0934],\n",
            "         [ 0.1087,  0.0687,  0.0247,  ..., -0.0050,  0.0923, -0.0934]],\n",
            "\n",
            "        [[-0.0061,  0.0622,  0.1436,  ..., -0.0452, -0.2533, -0.3990],\n",
            "         [ 0.0024,  0.0518,  0.1457,  ...,  0.0029, -0.2689, -0.4206],\n",
            "         [ 0.0319,  0.1129,  0.1208,  ..., -0.0017, -0.1423, -0.3227],\n",
            "         ...,\n",
            "         [ 0.0271,  0.1041,  0.1683,  ...,  0.0197, -0.1913, -0.3267],\n",
            "         [ 0.0271,  0.1041,  0.1683,  ...,  0.0197, -0.1913, -0.3267],\n",
            "         [ 0.0271,  0.1041,  0.1683,  ...,  0.0197, -0.1913, -0.3267]],\n",
            "\n",
            "        [[-0.0612, -0.0607,  0.1817,  ..., -0.0835, -0.1291, -0.2943],\n",
            "         [-0.0128, -0.0667,  0.1512,  ..., -0.0257, -0.1286, -0.2856],\n",
            "         [ 0.0247, -0.0602,  0.1614,  ...,  0.0027, -0.1457, -0.2763],\n",
            "         ...,\n",
            "         [-0.0229, -0.0617,  0.1636,  ..., -0.0558, -0.0475, -0.2752],\n",
            "         [-0.0229, -0.0617,  0.1636,  ..., -0.0558, -0.0475, -0.2752],\n",
            "         [-0.0229, -0.0617,  0.1636,  ..., -0.0558, -0.0475, -0.2752]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[-0.0339, -0.1310, -0.0674,  ...,  0.0201,  0.0772,  0.0108],\n",
            "         [-0.0271, -0.1329, -0.0641,  ..., -0.0366,  0.1400, -0.0295],\n",
            "         [-0.0907, -0.1445, -0.0914,  ..., -0.0765,  0.0975, -0.0424],\n",
            "         ...,\n",
            "         [-0.0150, -0.1288, -0.0610,  ..., -0.1070,  0.1113, -0.0352],\n",
            "         [-0.0374, -0.1634, -0.1441,  ..., -0.1095,  0.1029, -0.0479],\n",
            "         [-0.0276, -0.1175, -0.0532,  ..., -0.0635,  0.1166, -0.0067]],\n",
            "\n",
            "        [[ 0.0336,  0.0830,  0.0044,  ...,  0.0159, -0.1077, -0.1622],\n",
            "         [ 0.0232,  0.0529,  0.0572,  ...,  0.0210, -0.0637, -0.1140],\n",
            "         [ 0.0062,  0.1218,  0.0445,  ..., -0.0103, -0.0939, -0.1436],\n",
            "         ...,\n",
            "         [ 0.0624,  0.1181, -0.0050,  ..., -0.0115, -0.0606, -0.1258],\n",
            "         [ 0.0624,  0.1181, -0.0050,  ..., -0.0115, -0.0606, -0.1258],\n",
            "         [ 0.0624,  0.1181, -0.0050,  ..., -0.0115, -0.0606, -0.1258]],\n",
            "\n",
            "        [[-0.0188, -0.1240,  0.0061,  ..., -0.0362, -0.0945, -0.0704],\n",
            "         [ 0.0242, -0.0182,  0.1004,  ..., -0.0146,  0.0024, -0.0435],\n",
            "         [-0.0243, -0.0980,  0.0300,  ..., -0.0773, -0.0401, -0.0659],\n",
            "         ...,\n",
            "         [ 0.0142, -0.0721,  0.1005,  ..., -0.0589,  0.0174, -0.0603],\n",
            "         [ 0.0142, -0.0721,  0.1005,  ..., -0.0589,  0.0174, -0.0603],\n",
            "         [ 0.0142, -0.0721,  0.1005,  ..., -0.0589,  0.0174, -0.0603]]],\n",
            "       grad_fn=<ViewBackward0>)\n",
            "torch.Size([10, 20, 512])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "goX70VKqhxQH"
      },
      "source": [
        "## Req. 2-2 Multi-head self-attention 모듈 클래스 구현"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "XzO-ui28VEca"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WtNyV7mMj7V_"
      },
      "source": [
        "위의 과정을 모두 합쳐 하나의 Multi-head attention 모듈 class를 구현하겠습니다.\n",
        "\n",
        "아래 코드의 TODO 부분을 채워주세요."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U_kNhOTrkBHm"
      },
      "source": [
        "class MultiheadAttention(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(MultiheadAttention, self).__init__()\n",
        "\n",
        "    # Q, K, V learnable matrices\n",
        "    self.w_q = nn.Linear(d_model, d_model)\n",
        "    self.w_k = nn.Linear(d_model, d_model)\n",
        "    self.w_v = nn.Linear(d_model, d_model)\n",
        "\n",
        "    # Linear projection for concatenated outputs\n",
        "    self.w_0 = nn.Linear(d_model, d_model)\n",
        "\n",
        "  # scaled-dot product attention\n",
        "  def self_attention(self, q, k, v):\n",
        "    attn_scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(d_k)  # (B, num_heads, L, L)\n",
        "    attn_dists = F.softmax(attn_scores, dim=-1)  # (B, num_heads, L, L)\n",
        "\n",
        "    attn_values = torch.matmul(attn_dists, v)  # (B, num_heads, L, d_k)\n",
        "\n",
        "    return attn_values\n",
        "\n",
        "  def forward(self, q, k, v):\n",
        "    batch_size = q.shape[0]\n",
        "\n",
        "    # linear projection\n",
        "    ################################################################################\n",
        "    # TODO 1: Implement the forward pass for linear projection.                #\n",
        "    ################################################################################\n",
        "    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "    q = self.w_q(q)\n",
        "    k = self.w_k(k)\n",
        "    v = self.w_v(v)\n",
        "\n",
        "    print(q.shape)\n",
        "    print(k.shape)\n",
        "    print(v.shape)\n",
        "\n",
        "    # head만큼 쪼개준다\n",
        "    ################################################################################\n",
        "    # TODO 2: Implement the forward pass for \bsplit head.                #\n",
        "    ################################################################################\n",
        "    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "    q = q.view(batch_size, -1, num_heads, d_k)\n",
        "    k = k.view(batch_size, -1, num_heads, d_k)\n",
        "    v = v.view(batch_size, -1, num_heads, d_k)\n",
        "\n",
        "    print(q.shape)\n",
        "    print(k.shape)\n",
        "    print(v.shape)\n",
        "\n",
        "    # 각 head가 (L, d_k)의 matrix를 담당하도록 만든다\n",
        "    q = q.transpose(1, 2)  # (B, num_heads, L, d_k)\n",
        "    k = k.transpose(1, 2)  # (B, num_heads, L, d_k)\n",
        "    v = v.transpose(1, 2)  # (B, num_heads, L, d_k)\n",
        "\n",
        "    attn_values = self.self_attention(q, k, v)  # (B, num_heads, L, d_k)\n",
        "    attn_values = attn_values.transpose(1, 2).contiguous().view(batch_size, -1, d_model)  # (B, L, num_heads, d_k) => (B, L, d_model)\n",
        "\n",
        "    return self.w_0(attn_values)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jYLuu_9alQxT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c0818015-89e7-4a1a-df94-8e036774d3a4"
      },
      "source": [
        "multihead_attn = MultiheadAttention()\n",
        "\n",
        "outputs = multihead_attn(batch_emb, batch_emb, batch_emb)  # (B, L, d_model)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([10, 20, 512])\n",
            "torch.Size([10, 20, 512])\n",
            "torch.Size([10, 20, 512])\n",
            "torch.Size([10, 20, 8, 64])\n",
            "torch.Size([10, 20, 8, 64])\n",
            "torch.Size([10, 20, 8, 64])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KMiXlYjSlTfB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1a6e5dac-9366-4e2c-c381-cfb5af9a998d"
      },
      "source": [
        "print(outputs)\n",
        "print(outputs.shape)  # (batch_size, length, d_model)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[-0.1155,  0.0739,  0.0743,  ..., -0.0894, -0.0701, -0.0931],\n",
            "         [-0.1487,  0.1092,  0.0011,  ..., -0.0477, -0.0676, -0.0509],\n",
            "         [-0.1373,  0.1096,  0.0160,  ..., -0.0412, -0.0627, -0.0777],\n",
            "         ...,\n",
            "         [-0.0853,  0.0809,  0.0524,  ..., -0.0196, -0.0607, -0.1230],\n",
            "         [-0.0853,  0.0809,  0.0524,  ..., -0.0196, -0.0607, -0.1230],\n",
            "         [-0.0853,  0.0809,  0.0524,  ..., -0.0196, -0.0607, -0.1230]],\n",
            "\n",
            "        [[ 0.1945,  0.3697,  0.2906,  ...,  0.3264,  0.0169, -0.4615],\n",
            "         [ 0.1712,  0.4035,  0.2827,  ...,  0.3155,  0.0780, -0.4422],\n",
            "         [ 0.1881,  0.4075,  0.2655,  ...,  0.2364,  0.0469, -0.4114],\n",
            "         ...,\n",
            "         [ 0.1928,  0.3302,  0.3160,  ...,  0.3036,  0.0672, -0.4063],\n",
            "         [ 0.1928,  0.3302,  0.3160,  ...,  0.3036,  0.0672, -0.4063],\n",
            "         [ 0.1928,  0.3302,  0.3160,  ...,  0.3036,  0.0672, -0.4063]],\n",
            "\n",
            "        [[ 0.0447,  0.1728,  0.1319,  ...,  0.2321, -0.1352, -0.3282],\n",
            "         [ 0.0408,  0.2158,  0.1147,  ...,  0.3296, -0.0822, -0.2838],\n",
            "         [ 0.0946,  0.1886,  0.1622,  ...,  0.3471, -0.0921, -0.3583],\n",
            "         ...,\n",
            "         [ 0.0841,  0.1897,  0.1943,  ...,  0.3504, -0.0177, -0.3570],\n",
            "         [ 0.0841,  0.1897,  0.1943,  ...,  0.3504, -0.0177, -0.3570],\n",
            "         [ 0.0841,  0.1897,  0.1943,  ...,  0.3504, -0.0177, -0.3570]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ 0.2237, -0.0599,  0.0947,  ...,  0.1281, -0.1957, -0.1633],\n",
            "         [ 0.1451, -0.0112,  0.0396,  ...,  0.1582, -0.1187, -0.1392],\n",
            "         [ 0.2218, -0.0306,  0.1155,  ...,  0.1989, -0.2006, -0.2217],\n",
            "         ...,\n",
            "         [ 0.1884,  0.0043,  0.0511,  ...,  0.1871, -0.1521, -0.1194],\n",
            "         [ 0.1519, -0.0253,  0.0492,  ...,  0.2193, -0.1864, -0.1556],\n",
            "         [ 0.1543, -0.0129,  0.0598,  ...,  0.1861, -0.1407, -0.1736]],\n",
            "\n",
            "        [[ 0.0176,  0.2179, -0.0065,  ...,  0.1545, -0.1176, -0.1049],\n",
            "         [-0.0646,  0.1760, -0.0045,  ...,  0.1099, -0.1486, -0.0823],\n",
            "         [ 0.0302,  0.2145,  0.0430,  ...,  0.1523, -0.1398, -0.1155],\n",
            "         ...,\n",
            "         [ 0.0179,  0.1655,  0.0066,  ...,  0.1217, -0.0991, -0.1074],\n",
            "         [ 0.0179,  0.1655,  0.0066,  ...,  0.1217, -0.0991, -0.1074],\n",
            "         [ 0.0179,  0.1655,  0.0066,  ...,  0.1217, -0.0991, -0.1074]],\n",
            "\n",
            "        [[ 0.0116,  0.2050, -0.0431,  ...,  0.2213, -0.0508, -0.1938],\n",
            "         [ 0.0292,  0.1790, -0.0052,  ...,  0.2318, -0.0730, -0.1562],\n",
            "         [ 0.0413,  0.2020,  0.0042,  ...,  0.2635, -0.0465, -0.2071],\n",
            "         ...,\n",
            "         [ 0.0528,  0.1705,  0.0177,  ...,  0.2187, -0.0155, -0.2179],\n",
            "         [ 0.0528,  0.1705,  0.0177,  ...,  0.2187, -0.0155, -0.2179],\n",
            "         [ 0.0528,  0.1705,  0.0177,  ...,  0.2187, -0.0155, -0.2179]]],\n",
            "       grad_fn=<ViewBackward0>)\n",
            "torch.Size([10, 20, 512])\n"
          ]
        }
      ]
    }
  ]
}