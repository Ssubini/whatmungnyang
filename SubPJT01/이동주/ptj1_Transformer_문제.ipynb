{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Transformer"
      ],
      "metadata": {
        "id": "GiWulKAdmAHX"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9KsBGZpKkWki"
      },
      "source": [
        "Transformer의 핵심 구조인 Multi-head Attention을 구현하는 실습입니다.\n",
        "1. Multi-head attention 및 self-attention 구현.\n",
        "2. 각 과정에서 일어나는 연산과 input/output 형태 이해."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8qRU5DFY2OM8"
      },
      "source": [
        "### 필요 패키지 import"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lDtMioSQQ1bB"
      },
      "source": [
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "from tqdm import tqdm\n",
        "\n",
        "import torch\n",
        "import math"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Req. 2-1 Multi-head self-attention 구조 익히기"
      ],
      "metadata": {
        "id": "HH0VdC4uJJVG"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QBiZObgRep_Q"
      },
      "source": [
        "### **데이터 전처리**\n",
        "vocab_size 100인 가상의 시퀀스 데이터를 생성합니다. \n",
        "\n",
        "각 데이터에 할당된 숫자는 tokenizing과 정수화가 이뤄진 형태입니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e9ULZIqTenSc"
      },
      "source": [
        "pad_id = 0\n",
        "vocab_size = 100\n",
        "\n",
        "data = [\n",
        "  [62, 13, 47, 39, 78, 33, 56, 13, 39, 29, 44, 86, 71, 36, 18, 75],\n",
        "  [60, 96, 51, 32, 90],\n",
        "  [35, 45, 48, 65, 91, 99, 92, 10, 3, 21, 54],\n",
        "  [75, 51],\n",
        "  [66, 88, 98, 47],\n",
        "  [21, 39, 10, 64, 21],\n",
        "  [98],\n",
        "  [77, 65, 51, 77, 19, 15, 35, 19, 23, 97, 50, 46, 53, 42, 45, 91, 66, 3, 43, 10],\n",
        "  [70, 64, 98, 25, 99, 53, 4, 13, 69, 62, 66, 76, 15, 75, 45, 34],\n",
        "  [20, 64, 81, 35, 76, 85, 1, 62, 8, 45, 99, 77, 19, 43]\n",
        "]"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Hx3mcivgMyH"
      },
      "source": [
        "# 길이 맞춰주기 위해 패딩합니다.\n",
        "def padding(data):\n",
        "  max_len = len(max(data, key=len))\n",
        "  print(f\"Maximum sequence length: {max_len}\")\n",
        "\n",
        "  for i, seq in enumerate(tqdm(data)):\n",
        "    if len(seq) < max_len:\n",
        "      data[i] = seq + [pad_id] * (max_len - len(seq))\n",
        "\n",
        "  return data, max_len"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s3e8FiNvgX60",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e401083f-28d4-4e5e-b866-feea21369897"
      },
      "source": [
        "data, max_len = padding(data)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Maximum sequence length: 20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10/10 [00:00<00:00, 54050.31it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hwPSIWYugaN0",
        "outputId": "a01b1b05-a00f-4a3a-a19b-510e247b6bd4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "data"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[62, 13, 47, 39, 78, 33, 56, 13, 39, 29, 44, 86, 71, 36, 18, 75, 0, 0, 0, 0],\n",
              " [60, 96, 51, 32, 90, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              " [35, 45, 48, 65, 91, 99, 92, 10, 3, 21, 54, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              " [75, 51, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              " [66, 88, 98, 47, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              " [21, 39, 10, 64, 21, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              " [98, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              " [77,\n",
              "  65,\n",
              "  51,\n",
              "  77,\n",
              "  19,\n",
              "  15,\n",
              "  35,\n",
              "  19,\n",
              "  23,\n",
              "  97,\n",
              "  50,\n",
              "  46,\n",
              "  53,\n",
              "  42,\n",
              "  45,\n",
              "  91,\n",
              "  66,\n",
              "  3,\n",
              "  43,\n",
              "  10],\n",
              " [70, 64, 98, 25, 99, 53, 4, 13, 69, 62, 66, 76, 15, 75, 45, 34, 0, 0, 0, 0],\n",
              " [20, 64, 81, 35, 76, 85, 1, 62, 8, 45, 99, 77, 19, 43, 0, 0, 0, 0, 0, 0]]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rwqjACx8iidc"
      },
      "source": [
        "### Hyperparameter 세팅 및 embedding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p-Ngp2nWimS8"
      },
      "source": [
        "d_model = 512  # model의 hidden size\n",
        "num_heads = 8  # head의 개수\n",
        "\n",
        "# d_model이 입력을 projection 시킬 임베딩 space의 차원이므로, num_heads로 나누어 떨어져야 한다."
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GJMi2Xsni5uq"
      },
      "source": [
        "embedding = nn.Embedding(vocab_size, d_model)\n",
        "\n",
        "# B: batch size, L: maximum sequence length\n",
        "batch = torch.LongTensor(data)  # (B, L)\n",
        "batch_emb = embedding(batch)  # (B, L, d_model)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3tLCUQwojcUb"
      },
      "source": [
        "print(batch_emb)\n",
        "print(batch_emb.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s0Lhx892gmi3"
      },
      "source": [
        "### Linear projection & 여러 head로 나누기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "urXMBRnRgqvw"
      },
      "source": [
        "Multi-head attention 내에서 쓰이는 linear projection matrix들을 정의합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9DWKDqgCgfMk"
      },
      "source": [
        "w_q = nn.Linear(d_model, d_model)\n",
        "w_k = nn.Linear(d_model, d_model)\n",
        "w_v = nn.Linear(d_model, d_model)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tcLuhda7m-Lm"
      },
      "source": [
        "w_0 = nn.Linear(d_model, d_model)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H-vSL7PwnV6k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d83e7bd2-184c-486c-cdae-f3394e38ab7c"
      },
      "source": [
        "q = w_q(batch_emb)  # (B, L, d_model)\n",
        "k = w_k(batch_emb)  # (B, L, d_model)\n",
        "v = w_v(batch_emb)  # (B, L, d_model)\n",
        "\n",
        "print(q.shape)\n",
        "print(k.shape)\n",
        "print(v.shape)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([10, 20, 512])\n",
            "torch.Size([10, 20, 512])\n",
            "torch.Size([10, 20, 512])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wnvlum-LnF1T"
      },
      "source": [
        "Q, k, v를 `num_head`개의 차원 분할된 여러 vector로 만듭니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sXcYLZYvJT_1"
      },
      "source": [
        "- 이론적으로는 multi-head attention을 수행하면 input을 각각 다른 head 개수만큼의 Wq, Wk, Wv로 linear transformation 해서 각각 여러번의 attention 수행한 후 concat 한 후 linear transformation 수행해준다\n",
        "- 구현에서는 Wq, Wk, Wv 한 개씩\n",
        "- 실제 `attention is all you need` 논문의 구현 예시는 Query vector 한개를 dim으로 쪼개서 진행한다"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_tiOKAv9nEli",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "99ef5ef8-5f03-452f-a554-207681096c62"
      },
      "source": [
        "batch_size = q.shape[0]\n",
        "d_k = d_model // num_heads\n",
        "\n",
        "# num_heads * d_k로 쪼갠다\n",
        "q = q.view(batch_size, -1, num_heads, d_k)  # (B, L, num_heads, d_k)\n",
        "k = k.view(batch_size, -1, num_heads, d_k)  # (B, L, num_heads, d_k)\n",
        "v = v.view(batch_size, -1, num_heads, d_k)  # (B, L, num_heads, d_k)\n",
        "\n",
        "print(q.shape)\n",
        "print(k.shape)\n",
        "print(v.shape)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([10, 20, 8, 64])\n",
            "torch.Size([10, 20, 8, 64])\n",
            "torch.Size([10, 20, 8, 64])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5tNb2isfn5Cx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "feaaf88d-76dd-478d-ba04-9b86f37647fd"
      },
      "source": [
        "# num_heads를 밖으로 뺌으로써\n",
        "# 각 head가 (L, d_k) 만큼의 matrix를 가지고 self-attention 수행\n",
        "\n",
        "q = q.transpose(1, 2)  # (B, num_heads, L, d_k)\n",
        "k = k.transpose(1, 2)  # (B, num_heads, L, d_k)\n",
        "v = v.transpose(1, 2)  # (B, num_heads, L, d_k)\n",
        "\n",
        "print(q.shape)\n",
        "print(k.shape)\n",
        "print(v.shape)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([10, 8, 20, 64])\n",
            "torch.Size([10, 8, 20, 64])\n",
            "torch.Size([10, 8, 20, 64])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NWrDA5_Sofad"
      },
      "source": [
        "### Scaled dot-product self-attention 구현"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w52C4k3Wfl8m"
      },
      "source": [
        "각 head에서 실행되는 self-attetion 과정입니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A5waKr0Hfi2K",
        "outputId": "e8e1a375-1b53-4f34-8f11-075c5e2c27b9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# shape - (L, L)\n",
        "# 같은 sequence 내에 서로 다른 token들에게 얼마나 가중치를 두고 attention을 해야하는가\n",
        "attn_scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(d_k)  # (B, num_heads, L, L)\n",
        "# softmax - row-wise이기 때문에 dim은 -1\n",
        "attn_dists = F.softmax(attn_scores, dim=-1)  # (B, num_heads, L, L)\n",
        "\n",
        "print(attn_dists)\n",
        "print(attn_dists.shape)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[[0.0335, 0.0580, 0.0429,  ..., 0.0567, 0.0567, 0.0567],\n",
            "          [0.0499, 0.0504, 0.0512,  ..., 0.0403, 0.0403, 0.0403],\n",
            "          [0.0762, 0.0323, 0.0400,  ..., 0.0275, 0.0275, 0.0275],\n",
            "          ...,\n",
            "          [0.0284, 0.0381, 0.0700,  ..., 0.0796, 0.0796, 0.0796],\n",
            "          [0.0284, 0.0381, 0.0700,  ..., 0.0796, 0.0796, 0.0796],\n",
            "          [0.0284, 0.0381, 0.0700,  ..., 0.0796, 0.0796, 0.0796]],\n",
            "\n",
            "         [[0.0372, 0.0644, 0.0730,  ..., 0.0294, 0.0294, 0.0294],\n",
            "          [0.0342, 0.0600, 0.0369,  ..., 0.0643, 0.0643, 0.0643],\n",
            "          [0.0998, 0.0262, 0.0507,  ..., 0.0594, 0.0594, 0.0594],\n",
            "          ...,\n",
            "          [0.0438, 0.0348, 0.0384,  ..., 0.0808, 0.0808, 0.0808],\n",
            "          [0.0438, 0.0348, 0.0384,  ..., 0.0808, 0.0808, 0.0808],\n",
            "          [0.0438, 0.0348, 0.0384,  ..., 0.0808, 0.0808, 0.0808]],\n",
            "\n",
            "         [[0.0370, 0.0563, 0.0400,  ..., 0.0481, 0.0481, 0.0481],\n",
            "          [0.0383, 0.0723, 0.0657,  ..., 0.0256, 0.0256, 0.0256],\n",
            "          [0.0684, 0.0578, 0.0374,  ..., 0.0436, 0.0436, 0.0436],\n",
            "          ...,\n",
            "          [0.0352, 0.0539, 0.0295,  ..., 0.0502, 0.0502, 0.0502],\n",
            "          [0.0352, 0.0539, 0.0295,  ..., 0.0502, 0.0502, 0.0502],\n",
            "          [0.0352, 0.0539, 0.0295,  ..., 0.0502, 0.0502, 0.0502]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[0.0578, 0.0925, 0.0329,  ..., 0.0405, 0.0405, 0.0405],\n",
            "          [0.0339, 0.0871, 0.0334,  ..., 0.0402, 0.0402, 0.0402],\n",
            "          [0.0307, 0.0647, 0.0547,  ..., 0.0525, 0.0525, 0.0525],\n",
            "          ...,\n",
            "          [0.0405, 0.0432, 0.0374,  ..., 0.0345, 0.0345, 0.0345],\n",
            "          [0.0405, 0.0432, 0.0374,  ..., 0.0345, 0.0345, 0.0345],\n",
            "          [0.0405, 0.0432, 0.0374,  ..., 0.0345, 0.0345, 0.0345]],\n",
            "\n",
            "         [[0.0317, 0.0394, 0.0361,  ..., 0.0551, 0.0551, 0.0551],\n",
            "          [0.0840, 0.0390, 0.0364,  ..., 0.0643, 0.0643, 0.0643],\n",
            "          [0.0462, 0.0373, 0.0552,  ..., 0.0520, 0.0520, 0.0520],\n",
            "          ...,\n",
            "          [0.0480, 0.0365, 0.0708,  ..., 0.0585, 0.0585, 0.0585],\n",
            "          [0.0480, 0.0365, 0.0708,  ..., 0.0585, 0.0585, 0.0585],\n",
            "          [0.0480, 0.0365, 0.0708,  ..., 0.0585, 0.0585, 0.0585]],\n",
            "\n",
            "         [[0.0542, 0.0660, 0.0618,  ..., 0.0451, 0.0451, 0.0451],\n",
            "          [0.0170, 0.0599, 0.0293,  ..., 0.0710, 0.0710, 0.0710],\n",
            "          [0.0576, 0.0454, 0.0298,  ..., 0.0733, 0.0733, 0.0733],\n",
            "          ...,\n",
            "          [0.0312, 0.0497, 0.0318,  ..., 0.0527, 0.0527, 0.0527],\n",
            "          [0.0312, 0.0497, 0.0318,  ..., 0.0527, 0.0527, 0.0527],\n",
            "          [0.0312, 0.0497, 0.0318,  ..., 0.0527, 0.0527, 0.0527]]],\n",
            "\n",
            "\n",
            "        [[[0.0266, 0.0406, 0.0223,  ..., 0.0570, 0.0570, 0.0570],\n",
            "          [0.0415, 0.0522, 0.0436,  ..., 0.0520, 0.0520, 0.0520],\n",
            "          [0.0356, 0.0224, 0.0333,  ..., 0.0562, 0.0562, 0.0562],\n",
            "          ...,\n",
            "          [0.0296, 0.0249, 0.0303,  ..., 0.0558, 0.0558, 0.0558],\n",
            "          [0.0296, 0.0249, 0.0303,  ..., 0.0558, 0.0558, 0.0558],\n",
            "          [0.0296, 0.0249, 0.0303,  ..., 0.0558, 0.0558, 0.0558]],\n",
            "\n",
            "         [[0.0498, 0.0989, 0.0890,  ..., 0.0390, 0.0390, 0.0390],\n",
            "          [0.0392, 0.0334, 0.0472,  ..., 0.0505, 0.0505, 0.0505],\n",
            "          [0.0293, 0.0457, 0.0562,  ..., 0.0525, 0.0525, 0.0525],\n",
            "          ...,\n",
            "          [0.0218, 0.0275, 0.0303,  ..., 0.0567, 0.0567, 0.0567],\n",
            "          [0.0218, 0.0275, 0.0303,  ..., 0.0567, 0.0567, 0.0567],\n",
            "          [0.0218, 0.0275, 0.0303,  ..., 0.0567, 0.0567, 0.0567]],\n",
            "\n",
            "         [[0.0367, 0.0628, 0.0359,  ..., 0.0513, 0.0513, 0.0513],\n",
            "          [0.0499, 0.0480, 0.0385,  ..., 0.0497, 0.0497, 0.0497],\n",
            "          [0.0584, 0.0921, 0.1036,  ..., 0.0413, 0.0413, 0.0413],\n",
            "          ...,\n",
            "          [0.0552, 0.0429, 0.0166,  ..., 0.0546, 0.0546, 0.0546],\n",
            "          [0.0552, 0.0429, 0.0166,  ..., 0.0546, 0.0546, 0.0546],\n",
            "          [0.0552, 0.0429, 0.0166,  ..., 0.0546, 0.0546, 0.0546]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[0.0472, 0.0322, 0.0287,  ..., 0.0551, 0.0551, 0.0551],\n",
            "          [0.0282, 0.0772, 0.0368,  ..., 0.0462, 0.0462, 0.0462],\n",
            "          [0.0397, 0.0272, 0.0365,  ..., 0.0549, 0.0549, 0.0549],\n",
            "          ...,\n",
            "          [0.0553, 0.0595, 0.0559,  ..., 0.0452, 0.0452, 0.0452],\n",
            "          [0.0553, 0.0595, 0.0559,  ..., 0.0452, 0.0452, 0.0452],\n",
            "          [0.0553, 0.0595, 0.0559,  ..., 0.0452, 0.0452, 0.0452]],\n",
            "\n",
            "         [[0.0597, 0.0493, 0.0477,  ..., 0.0482, 0.0482, 0.0482],\n",
            "          [0.0323, 0.0228, 0.0603,  ..., 0.0510, 0.0510, 0.0510],\n",
            "          [0.0666, 0.0273, 0.0173,  ..., 0.0550, 0.0550, 0.0550],\n",
            "          ...,\n",
            "          [0.0346, 0.0358, 0.0328,  ..., 0.0527, 0.0527, 0.0527],\n",
            "          [0.0346, 0.0358, 0.0328,  ..., 0.0527, 0.0527, 0.0527],\n",
            "          [0.0346, 0.0358, 0.0328,  ..., 0.0527, 0.0527, 0.0527]],\n",
            "\n",
            "         [[0.0638, 0.1012, 0.0698,  ..., 0.0427, 0.0427, 0.0427],\n",
            "          [0.0531, 0.1176, 0.0906,  ..., 0.0403, 0.0403, 0.0403],\n",
            "          [0.0485, 0.0379, 0.0326,  ..., 0.0534, 0.0534, 0.0534],\n",
            "          ...,\n",
            "          [0.0283, 0.0588, 0.0530,  ..., 0.0502, 0.0502, 0.0502],\n",
            "          [0.0283, 0.0588, 0.0530,  ..., 0.0502, 0.0502, 0.0502],\n",
            "          [0.0283, 0.0588, 0.0530,  ..., 0.0502, 0.0502, 0.0502]]],\n",
            "\n",
            "\n",
            "        [[[0.0265, 0.0438, 0.0276,  ..., 0.0548, 0.0548, 0.0548],\n",
            "          [0.1021, 0.0569, 0.0875,  ..., 0.0343, 0.0343, 0.0343],\n",
            "          [0.0794, 0.0587, 0.0768,  ..., 0.0341, 0.0341, 0.0341],\n",
            "          ...,\n",
            "          [0.0215, 0.0495, 0.0522,  ..., 0.0654, 0.0654, 0.0654],\n",
            "          [0.0215, 0.0495, 0.0522,  ..., 0.0654, 0.0654, 0.0654],\n",
            "          [0.0215, 0.0495, 0.0522,  ..., 0.0654, 0.0654, 0.0654]],\n",
            "\n",
            "         [[0.0545, 0.0397, 0.0519,  ..., 0.0523, 0.0523, 0.0523],\n",
            "          [0.0560, 0.0232, 0.0321,  ..., 0.0642, 0.0642, 0.0642],\n",
            "          [0.0337, 0.0457, 0.0680,  ..., 0.0599, 0.0599, 0.0599],\n",
            "          ...,\n",
            "          [0.0804, 0.0395, 0.0190,  ..., 0.0646, 0.0646, 0.0646],\n",
            "          [0.0804, 0.0395, 0.0190,  ..., 0.0646, 0.0646, 0.0646],\n",
            "          [0.0804, 0.0395, 0.0190,  ..., 0.0646, 0.0646, 0.0646]],\n",
            "\n",
            "         [[0.0661, 0.0527, 0.0359,  ..., 0.0524, 0.0524, 0.0524],\n",
            "          [0.0616, 0.0469, 0.0269,  ..., 0.0529, 0.0529, 0.0529],\n",
            "          [0.0465, 0.0457, 0.0442,  ..., 0.0462, 0.0462, 0.0462],\n",
            "          ...,\n",
            "          [0.0414, 0.0599, 0.0739,  ..., 0.0530, 0.0530, 0.0530],\n",
            "          [0.0414, 0.0599, 0.0739,  ..., 0.0530, 0.0530, 0.0530],\n",
            "          [0.0414, 0.0599, 0.0739,  ..., 0.0530, 0.0530, 0.0530]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[0.0794, 0.0443, 0.0539,  ..., 0.0496, 0.0496, 0.0496],\n",
            "          [0.0230, 0.0217, 0.0271,  ..., 0.0678, 0.0678, 0.0678],\n",
            "          [0.0569, 0.0322, 0.0370,  ..., 0.0499, 0.0499, 0.0499],\n",
            "          ...,\n",
            "          [0.0377, 0.0848, 0.0647,  ..., 0.0367, 0.0367, 0.0367],\n",
            "          [0.0377, 0.0848, 0.0647,  ..., 0.0367, 0.0367, 0.0367],\n",
            "          [0.0377, 0.0848, 0.0647,  ..., 0.0367, 0.0367, 0.0367]],\n",
            "\n",
            "         [[0.0515, 0.0382, 0.0286,  ..., 0.0490, 0.0490, 0.0490],\n",
            "          [0.0444, 0.0427, 0.0796,  ..., 0.0484, 0.0484, 0.0484],\n",
            "          [0.0298, 0.0351, 0.0373,  ..., 0.0591, 0.0591, 0.0591],\n",
            "          ...,\n",
            "          [0.0279, 0.0601, 0.0386,  ..., 0.0563, 0.0563, 0.0563],\n",
            "          [0.0279, 0.0601, 0.0386,  ..., 0.0563, 0.0563, 0.0563],\n",
            "          [0.0279, 0.0601, 0.0386,  ..., 0.0563, 0.0563, 0.0563]],\n",
            "\n",
            "         [[0.0651, 0.0707, 0.0361,  ..., 0.0524, 0.0524, 0.0524],\n",
            "          [0.0598, 0.0743, 0.0472,  ..., 0.0485, 0.0485, 0.0485],\n",
            "          [0.1009, 0.0532, 0.0645,  ..., 0.0386, 0.0386, 0.0386],\n",
            "          ...,\n",
            "          [0.0301, 0.0392, 0.0539,  ..., 0.0541, 0.0541, 0.0541],\n",
            "          [0.0301, 0.0392, 0.0539,  ..., 0.0541, 0.0541, 0.0541],\n",
            "          [0.0301, 0.0392, 0.0539,  ..., 0.0541, 0.0541, 0.0541]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[0.0449, 0.0654, 0.0642,  ..., 0.0577, 0.0403, 0.0296],\n",
            "          [0.0348, 0.0263, 0.0256,  ..., 0.0421, 0.0770, 0.0373],\n",
            "          [0.0866, 0.0424, 0.0467,  ..., 0.0692, 0.0327, 0.0468],\n",
            "          ...,\n",
            "          [0.0352, 0.1067, 0.0631,  ..., 0.0501, 0.0474, 0.0718],\n",
            "          [0.0360, 0.0453, 0.0610,  ..., 0.0555, 0.0609, 0.0361],\n",
            "          [0.0556, 0.0451, 0.0622,  ..., 0.0988, 0.0730, 0.0309]],\n",
            "\n",
            "         [[0.0626, 0.0479, 0.0362,  ..., 0.0263, 0.0564, 0.0414],\n",
            "          [0.0481, 0.0426, 0.0374,  ..., 0.0504, 0.0937, 0.0467],\n",
            "          [0.0400, 0.0841, 0.0705,  ..., 0.0538, 0.0363, 0.0541],\n",
            "          ...,\n",
            "          [0.0659, 0.0612, 0.0655,  ..., 0.0342, 0.0499, 0.0395],\n",
            "          [0.0290, 0.0397, 0.0444,  ..., 0.0868, 0.0433, 0.0677],\n",
            "          [0.0613, 0.0249, 0.0558,  ..., 0.0338, 0.0709, 0.0935]],\n",
            "\n",
            "         [[0.0564, 0.0591, 0.0985,  ..., 0.0678, 0.0906, 0.0478],\n",
            "          [0.0354, 0.0489, 0.0567,  ..., 0.0538, 0.0290, 0.0599],\n",
            "          [0.0266, 0.0484, 0.0932,  ..., 0.0927, 0.0252, 0.0584],\n",
            "          ...,\n",
            "          [0.0853, 0.0612, 0.0629,  ..., 0.0475, 0.0570, 0.0489],\n",
            "          [0.0220, 0.0571, 0.0618,  ..., 0.0211, 0.1035, 0.0499],\n",
            "          [0.0461, 0.0567, 0.0458,  ..., 0.0342, 0.0730, 0.0555]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[0.0403, 0.0411, 0.0426,  ..., 0.0788, 0.0389, 0.0848],\n",
            "          [0.0863, 0.0471, 0.0420,  ..., 0.0364, 0.0868, 0.0377],\n",
            "          [0.0501, 0.0683, 0.0504,  ..., 0.0592, 0.0456, 0.0698],\n",
            "          ...,\n",
            "          [0.0375, 0.0479, 0.0634,  ..., 0.0429, 0.0622, 0.0327],\n",
            "          [0.0604, 0.0418, 0.0275,  ..., 0.0551, 0.0283, 0.0512],\n",
            "          [0.0290, 0.0305, 0.0301,  ..., 0.0570, 0.0917, 0.0575]],\n",
            "\n",
            "         [[0.0412, 0.0494, 0.0216,  ..., 0.0625, 0.0399, 0.0243],\n",
            "          [0.0469, 0.0336, 0.0319,  ..., 0.0315, 0.0544, 0.0184],\n",
            "          [0.0254, 0.0272, 0.0307,  ..., 0.0675, 0.0480, 0.0290],\n",
            "          ...,\n",
            "          [0.0528, 0.0621, 0.0592,  ..., 0.0230, 0.0878, 0.0425],\n",
            "          [0.0666, 0.0410, 0.0351,  ..., 0.0317, 0.0322, 0.0306],\n",
            "          [0.0459, 0.0834, 0.0317,  ..., 0.0285, 0.0474, 0.0545]],\n",
            "\n",
            "         [[0.0511, 0.0550, 0.0323,  ..., 0.0251, 0.0572, 0.0472],\n",
            "          [0.0324, 0.0489, 0.0709,  ..., 0.0564, 0.0733, 0.0372],\n",
            "          [0.0619, 0.0385, 0.0416,  ..., 0.0579, 0.0484, 0.0556],\n",
            "          ...,\n",
            "          [0.0316, 0.0410, 0.0498,  ..., 0.0422, 0.0436, 0.0520],\n",
            "          [0.0425, 0.0562, 0.0577,  ..., 0.0839, 0.0435, 0.0404],\n",
            "          [0.0309, 0.0367, 0.0989,  ..., 0.0600, 0.0387, 0.0470]]],\n",
            "\n",
            "\n",
            "        [[[0.0407, 0.0405, 0.0441,  ..., 0.0459, 0.0459, 0.0459],\n",
            "          [0.0404, 0.0751, 0.0573,  ..., 0.0339, 0.0339, 0.0339],\n",
            "          [0.0399, 0.0306, 0.0535,  ..., 0.0428, 0.0428, 0.0428],\n",
            "          ...,\n",
            "          [0.0506, 0.0385, 0.0413,  ..., 0.0814, 0.0814, 0.0814],\n",
            "          [0.0506, 0.0385, 0.0413,  ..., 0.0814, 0.0814, 0.0814],\n",
            "          [0.0506, 0.0385, 0.0413,  ..., 0.0814, 0.0814, 0.0814]],\n",
            "\n",
            "         [[0.0299, 0.0287, 0.0287,  ..., 0.0749, 0.0749, 0.0749],\n",
            "          [0.0509, 0.0819, 0.0413,  ..., 0.0619, 0.0619, 0.0619],\n",
            "          [0.0640, 0.0481, 0.0744,  ..., 0.0429, 0.0429, 0.0429],\n",
            "          ...,\n",
            "          [0.0873, 0.0481, 0.0524,  ..., 0.0735, 0.0735, 0.0735],\n",
            "          [0.0873, 0.0481, 0.0524,  ..., 0.0735, 0.0735, 0.0735],\n",
            "          [0.0873, 0.0481, 0.0524,  ..., 0.0735, 0.0735, 0.0735]],\n",
            "\n",
            "         [[0.0385, 0.0501, 0.0464,  ..., 0.0659, 0.0659, 0.0659],\n",
            "          [0.0458, 0.0208, 0.0589,  ..., 0.0454, 0.0454, 0.0454],\n",
            "          [0.0699, 0.0736, 0.0379,  ..., 0.0513, 0.0513, 0.0513],\n",
            "          ...,\n",
            "          [0.0295, 0.0333, 0.0507,  ..., 0.0542, 0.0542, 0.0542],\n",
            "          [0.0295, 0.0333, 0.0507,  ..., 0.0542, 0.0542, 0.0542],\n",
            "          [0.0295, 0.0333, 0.0507,  ..., 0.0542, 0.0542, 0.0542]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[0.0522, 0.0331, 0.0449,  ..., 0.0535, 0.0535, 0.0535],\n",
            "          [0.0587, 0.0553, 0.0437,  ..., 0.0374, 0.0374, 0.0374],\n",
            "          [0.0549, 0.0465, 0.0673,  ..., 0.0539, 0.0539, 0.0539],\n",
            "          ...,\n",
            "          [0.0785, 0.0618, 0.0635,  ..., 0.0346, 0.0346, 0.0346],\n",
            "          [0.0785, 0.0618, 0.0635,  ..., 0.0346, 0.0346, 0.0346],\n",
            "          [0.0785, 0.0618, 0.0635,  ..., 0.0346, 0.0346, 0.0346]],\n",
            "\n",
            "         [[0.0534, 0.0406, 0.0440,  ..., 0.0521, 0.0521, 0.0521],\n",
            "          [0.0470, 0.0596, 0.0540,  ..., 0.0498, 0.0498, 0.0498],\n",
            "          [0.0335, 0.0383, 0.0424,  ..., 0.0466, 0.0466, 0.0466],\n",
            "          ...,\n",
            "          [0.0378, 0.0557, 0.0428,  ..., 0.0649, 0.0649, 0.0649],\n",
            "          [0.0378, 0.0557, 0.0428,  ..., 0.0649, 0.0649, 0.0649],\n",
            "          [0.0378, 0.0557, 0.0428,  ..., 0.0649, 0.0649, 0.0649]],\n",
            "\n",
            "         [[0.0561, 0.0199, 0.0603,  ..., 0.0494, 0.0494, 0.0494],\n",
            "          [0.0659, 0.0662, 0.0581,  ..., 0.0378, 0.0378, 0.0378],\n",
            "          [0.0334, 0.0493, 0.0375,  ..., 0.0713, 0.0713, 0.0713],\n",
            "          ...,\n",
            "          [0.0731, 0.0552, 0.0244,  ..., 0.0549, 0.0549, 0.0549],\n",
            "          [0.0731, 0.0552, 0.0244,  ..., 0.0549, 0.0549, 0.0549],\n",
            "          [0.0731, 0.0552, 0.0244,  ..., 0.0549, 0.0549, 0.0549]]],\n",
            "\n",
            "\n",
            "        [[[0.0545, 0.0454, 0.0387,  ..., 0.0504, 0.0504, 0.0504],\n",
            "          [0.0491, 0.0746, 0.0521,  ..., 0.0337, 0.0337, 0.0337],\n",
            "          [0.0497, 0.0348, 0.0403,  ..., 0.0571, 0.0571, 0.0571],\n",
            "          ...,\n",
            "          [0.0312, 0.0353, 0.0822,  ..., 0.0747, 0.0747, 0.0747],\n",
            "          [0.0312, 0.0353, 0.0822,  ..., 0.0747, 0.0747, 0.0747],\n",
            "          [0.0312, 0.0353, 0.0822,  ..., 0.0747, 0.0747, 0.0747]],\n",
            "\n",
            "         [[0.0729, 0.0580, 0.0497,  ..., 0.0278, 0.0278, 0.0278],\n",
            "          [0.0394, 0.0844, 0.0344,  ..., 0.0638, 0.0638, 0.0638],\n",
            "          [0.0571, 0.0436, 0.0950,  ..., 0.0386, 0.0386, 0.0386],\n",
            "          ...,\n",
            "          [0.0419, 0.0428, 0.0448,  ..., 0.0654, 0.0654, 0.0654],\n",
            "          [0.0419, 0.0428, 0.0448,  ..., 0.0654, 0.0654, 0.0654],\n",
            "          [0.0419, 0.0428, 0.0448,  ..., 0.0654, 0.0654, 0.0654]],\n",
            "\n",
            "         [[0.0497, 0.0739, 0.0538,  ..., 0.0403, 0.0403, 0.0403],\n",
            "          [0.0402, 0.0202, 0.0395,  ..., 0.0440, 0.0440, 0.0440],\n",
            "          [0.0435, 0.0342, 0.0585,  ..., 0.0664, 0.0664, 0.0664],\n",
            "          ...,\n",
            "          [0.0475, 0.0352, 0.0435,  ..., 0.0573, 0.0573, 0.0573],\n",
            "          [0.0475, 0.0352, 0.0435,  ..., 0.0573, 0.0573, 0.0573],\n",
            "          [0.0475, 0.0352, 0.0435,  ..., 0.0573, 0.0573, 0.0573]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[0.0916, 0.0509, 0.0694,  ..., 0.0335, 0.0335, 0.0335],\n",
            "          [0.0465, 0.0548, 0.0889,  ..., 0.0371, 0.0371, 0.0371],\n",
            "          [0.0685, 0.0686, 0.0740,  ..., 0.0504, 0.0504, 0.0504],\n",
            "          ...,\n",
            "          [0.0495, 0.0580, 0.1209,  ..., 0.0324, 0.0324, 0.0324],\n",
            "          [0.0495, 0.0580, 0.1209,  ..., 0.0324, 0.0324, 0.0324],\n",
            "          [0.0495, 0.0580, 0.1209,  ..., 0.0324, 0.0324, 0.0324]],\n",
            "\n",
            "         [[0.0348, 0.0374, 0.0434,  ..., 0.0609, 0.0609, 0.0609],\n",
            "          [0.0327, 0.0568, 0.0490,  ..., 0.0474, 0.0474, 0.0474],\n",
            "          [0.0589, 0.0412, 0.0452,  ..., 0.0550, 0.0550, 0.0550],\n",
            "          ...,\n",
            "          [0.0506, 0.0481, 0.0600,  ..., 0.0561, 0.0561, 0.0561],\n",
            "          [0.0506, 0.0481, 0.0600,  ..., 0.0561, 0.0561, 0.0561],\n",
            "          [0.0506, 0.0481, 0.0600,  ..., 0.0561, 0.0561, 0.0561]],\n",
            "\n",
            "         [[0.0525, 0.0698, 0.0453,  ..., 0.0450, 0.0450, 0.0450],\n",
            "          [0.0531, 0.0701, 0.0558,  ..., 0.0401, 0.0401, 0.0401],\n",
            "          [0.0418, 0.0827, 0.0548,  ..., 0.0386, 0.0386, 0.0386],\n",
            "          ...,\n",
            "          [0.0411, 0.0679, 0.0391,  ..., 0.0675, 0.0675, 0.0675],\n",
            "          [0.0411, 0.0679, 0.0391,  ..., 0.0675, 0.0675, 0.0675],\n",
            "          [0.0411, 0.0679, 0.0391,  ..., 0.0675, 0.0675, 0.0675]]]],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "torch.Size([10, 8, 20, 20])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7megouWpgCck",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6262d131-29b6-4d6f-b902-b71e1eee9709"
      },
      "source": [
        "attn_values = torch.matmul(attn_dists, v)  # (B, num_heads, L, d_k)\n",
        "\n",
        "print(attn_values.shape)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([10, 8, 20, 64])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LmSTaymdg-P_"
      },
      "source": [
        "### 각 head의 결과물 병합"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YSdQZCk0hCNd"
      },
      "source": [
        "각 head의 결과물을 concat하고 동일 차원으로 linear projection합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eaK0bpMGhQZ2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e3922070-b8ca-4519-b130-ff2b858ec296"
      },
      "source": [
        "attn_values = attn_values.transpose(1, 2)  # (B, L, num_heads, d_k)\n",
        "attn_values = attn_values.contiguous().view(batch_size, -1, d_model)  # (B, L, d_model)\n",
        "\n",
        "print(attn_values.shape)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([10, 20, 512])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LTng_2SXhdH1",
        "outputId": "5ee10f68-7633-4176-93d7-f58302e209a6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# w_0 : (d_model, d_model)\n",
        "# 서로 다른 의미로 foucsing 된 각 head의 self-attention 정보들을 합쳐주는 역할 수행\n",
        "outputs = w_0(attn_values)\n",
        "\n",
        "print(outputs)\n",
        "print(outputs.shape)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[ 0.0106,  0.0115, -0.0005,  ..., -0.0712, -0.1098, -0.0659],\n",
            "         [ 0.0525,  0.0792, -0.0039,  ..., -0.0528, -0.0492, -0.0287],\n",
            "         [ 0.0274,  0.0840, -0.0084,  ..., -0.0343, -0.0747, -0.0851],\n",
            "         ...,\n",
            "         [ 0.0439,  0.0649,  0.0591,  ..., -0.0244, -0.1058,  0.0321],\n",
            "         [ 0.0439,  0.0649,  0.0591,  ..., -0.0244, -0.1058,  0.0321],\n",
            "         [ 0.0439,  0.0649,  0.0591,  ..., -0.0244, -0.1058,  0.0321]],\n",
            "\n",
            "        [[ 0.2657,  0.1159, -0.2594,  ..., -0.0963, -0.2405,  0.1822],\n",
            "         [ 0.2875,  0.1081, -0.2328,  ..., -0.0455, -0.2609,  0.1413],\n",
            "         [ 0.2616,  0.1483, -0.2944,  ..., -0.1036, -0.2274,  0.2030],\n",
            "         ...,\n",
            "         [ 0.3070,  0.1214, -0.2897,  ..., -0.0606, -0.2617,  0.1582],\n",
            "         [ 0.3070,  0.1214, -0.2897,  ..., -0.0606, -0.2617,  0.1582],\n",
            "         [ 0.3070,  0.1214, -0.2897,  ..., -0.0606, -0.2617,  0.1582]],\n",
            "\n",
            "        [[ 0.0650,  0.0982, -0.1668,  ..., -0.0170, -0.2075,  0.0626],\n",
            "         [ 0.0335,  0.1672, -0.1525,  ..., -0.0059, -0.1480,  0.0622],\n",
            "         [-0.0102,  0.1089, -0.2057,  ..., -0.0331, -0.1840,  0.0651],\n",
            "         ...,\n",
            "         [ 0.0916,  0.1162, -0.1722,  ...,  0.0624, -0.1383,  0.1105],\n",
            "         [ 0.0916,  0.1162, -0.1722,  ...,  0.0624, -0.1383,  0.1105],\n",
            "         [ 0.0916,  0.1162, -0.1722,  ...,  0.0624, -0.1383,  0.1105]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[-0.0982,  0.0216,  0.1450,  ...,  0.1087, -0.0815, -0.0729],\n",
            "         [-0.1110,  0.1384,  0.0187,  ...,  0.0602, -0.1846,  0.0226],\n",
            "         [-0.0664,  0.0757,  0.0829,  ...,  0.1397, -0.1752,  0.0305],\n",
            "         ...,\n",
            "         [-0.1426,  0.0988,  0.1088,  ...,  0.0696, -0.1202, -0.0710],\n",
            "         [-0.0641,  0.0233,  0.0810,  ...,  0.0199, -0.1118, -0.0061],\n",
            "         [-0.0720,  0.1306,  0.0809,  ...,  0.0901, -0.1157,  0.0277]],\n",
            "\n",
            "        [[ 0.0333,  0.0555,  0.0736,  ..., -0.0288, -0.1109,  0.0145],\n",
            "         [ 0.0424,  0.0079,  0.0770,  ..., -0.0811, -0.1167,  0.0515],\n",
            "         [-0.0133,  0.0172,  0.0578,  ..., -0.0627, -0.1216,  0.0116],\n",
            "         ...,\n",
            "         [ 0.0248,  0.0607,  0.0373,  ..., -0.0397, -0.0948,  0.0415],\n",
            "         [ 0.0248,  0.0607,  0.0373,  ..., -0.0397, -0.0948,  0.0415],\n",
            "         [ 0.0248,  0.0607,  0.0373,  ..., -0.0397, -0.0948,  0.0415]],\n",
            "\n",
            "        [[-0.0015,  0.0789, -0.0276,  ..., -0.0189, -0.0729,  0.0558],\n",
            "         [ 0.0446,  0.0999, -0.0148,  ...,  0.0412, -0.0429,  0.0326],\n",
            "         [ 0.0762,  0.0380, -0.0228,  ..., -0.0097, -0.1330,  0.0324],\n",
            "         ...,\n",
            "         [ 0.0442,  0.0557, -0.0662,  ...,  0.0189, -0.1235,  0.0663],\n",
            "         [ 0.0442,  0.0557, -0.0662,  ...,  0.0189, -0.1235,  0.0663],\n",
            "         [ 0.0442,  0.0557, -0.0662,  ...,  0.0189, -0.1235,  0.0663]]],\n",
            "       grad_fn=<ViewBackward0>)\n",
            "torch.Size([10, 20, 512])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "goX70VKqhxQH"
      },
      "source": [
        "## Req. 2-2 Multi-head self-attention 모듈 클래스 구현"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WtNyV7mMj7V_"
      },
      "source": [
        "위의 과정을 모두 합쳐 하나의 Multi-head attention 모듈 class를 구현하겠습니다.\n",
        "\n",
        "아래 코드의 TODO 부분을 채워주세요."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U_kNhOTrkBHm"
      },
      "source": [
        "class MultiheadAttention(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(MultiheadAttention, self).__init__()\n",
        "\n",
        "    # Q, K, V learnable matrices\n",
        "    self.w_q = nn.Linear(d_model, d_model)\n",
        "    self.w_k = nn.Linear(d_model, d_model)\n",
        "    self.w_v = nn.Linear(d_model, d_model)\n",
        "\n",
        "    # Linear projection for concatenated outputs\n",
        "    self.w_0 = nn.Linear(d_model, d_model)\n",
        "\n",
        "  # scaled-dot product attention\n",
        "  def self_attention(self, q, k, v):\n",
        "    attn_scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(d_k)  # (B, num_heads, L, L)\n",
        "    attn_dists = F.softmax(attn_scores, dim=-1)  # (B, num_heads, L, L)\n",
        "\n",
        "    attn_values = torch.matmul(attn_dists, v)  # (B, num_heads, L, d_k)\n",
        "\n",
        "    return attn_values\n",
        "\n",
        "  def forward(self, q, k, v):\n",
        "    batch_size = q.shape[0]\n",
        "\n",
        "    # linear projection\n",
        "    ################################################################################\n",
        "    # TODO 1: Implement the forward pass for linear projection.                #\n",
        "    ################################################################################\n",
        "    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "    q = self.w_q(q) \n",
        "    k = self.w_k(k) \n",
        "    v = self.w_v(v)  \n",
        "\n",
        "    # head만큼 쪼개준다\n",
        "    ################################################################################\n",
        "    # TODO 2: Implement the forward pass for \bsplit head.                #\n",
        "    ################################################################################\n",
        "    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "    q = q.view(batch_size, -1, num_heads, d_k)  \n",
        "    k = k.view(batch_size, -1, num_heads, d_k) \n",
        "    v = v.view(batch_size, -1, num_heads, d_k)  \n",
        "\n",
        "    # 각 head가 (L, d_k)의 matrix를 담당하도록 만든다\n",
        "    q = q.transpose(1, 2)  # (B, num_heads, L, d_k)\n",
        "    k = k.transpose(1, 2)  # (B, num_heads, L, d_k)\n",
        "    v = v.transpose(1, 2)  # (B, num_heads, L, d_k)\n",
        "\n",
        "    attn_values = self.self_attention(q, k, v)  # (B, num_heads, L, d_k)\n",
        "    attn_values = attn_values.transpose(1, 2).contiguous().view(batch_size, -1, d_model)  # (B, L, num_heads, d_k) => (B, L, d_model)\n",
        "\n",
        "    return self.w_0(attn_values)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jYLuu_9alQxT"
      },
      "source": [
        "multihead_attn = MultiheadAttention()\n",
        "\n",
        "outputs = multihead_attn(batch_emb, batch_emb, batch_emb)  # (B, L, d_model)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KMiXlYjSlTfB",
        "outputId": "e2c544d9-9588-4513-ca19-872cdee6db5d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(outputs)\n",
        "print(outputs.shape)  # (batch_size, length, d_model)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[ 1.3513e-02,  5.5535e-02,  3.2119e-02,  ..., -1.0572e-04,\n",
            "          -6.5021e-02, -6.6141e-02],\n",
            "         [ 3.5989e-02,  9.8419e-02,  8.6256e-02,  ..., -4.4619e-02,\n",
            "          -1.1402e-01, -3.2597e-02],\n",
            "         [ 6.4989e-02,  5.5447e-02,  6.0514e-02,  ..., -2.6617e-02,\n",
            "          -8.8778e-02, -1.0302e-01],\n",
            "         ...,\n",
            "         [ 5.4912e-02,  3.9996e-02,  9.2343e-02,  ..., -1.2819e-02,\n",
            "          -1.1423e-01, -7.3098e-02],\n",
            "         [ 5.4912e-02,  3.9996e-02,  9.2343e-02,  ..., -1.2819e-02,\n",
            "          -1.1423e-01, -7.3098e-02],\n",
            "         [ 5.4912e-02,  3.9996e-02,  9.2343e-02,  ..., -1.2819e-02,\n",
            "          -1.1423e-01, -7.3098e-02]],\n",
            "\n",
            "        [[ 6.3359e-03,  4.4390e-01,  7.0669e-02,  ..., -2.1539e-01,\n",
            "          -2.1502e-02,  1.2415e-01],\n",
            "         [-3.3830e-02,  4.3597e-01,  1.0081e-01,  ..., -2.0408e-01,\n",
            "          -1.4645e-02,  1.5854e-01],\n",
            "         [-1.4046e-01,  4.0944e-01,  3.5762e-02,  ..., -1.3867e-01,\n",
            "          -8.9681e-02,  1.4519e-01],\n",
            "         ...,\n",
            "         [-2.2807e-02,  4.0374e-01,  5.0164e-02,  ..., -2.1636e-01,\n",
            "          -2.7174e-02,  1.4246e-01],\n",
            "         [-2.2807e-02,  4.0374e-01,  5.0164e-02,  ..., -2.1636e-01,\n",
            "          -2.7174e-02,  1.4246e-01],\n",
            "         [-2.2807e-02,  4.0374e-01,  5.0164e-02,  ..., -2.1636e-01,\n",
            "          -2.7174e-02,  1.4246e-01]],\n",
            "\n",
            "        [[-1.3116e-01,  3.0866e-01,  5.3881e-02,  ...,  2.8091e-02,\n",
            "          -4.8516e-02,  9.8477e-02],\n",
            "         [-6.0845e-02,  2.9817e-01,  3.6894e-02,  ..., -8.1893e-02,\n",
            "          -2.1439e-02, -4.1791e-03],\n",
            "         [-8.6381e-02,  2.9449e-01,  4.3968e-02,  ..., -4.0726e-02,\n",
            "          -2.6777e-02,  4.6235e-02],\n",
            "         ...,\n",
            "         [-7.9858e-02,  2.1232e-01,  4.3336e-02,  ..., -8.6679e-02,\n",
            "          -2.3537e-02,  4.1579e-02],\n",
            "         [-7.9858e-02,  2.1232e-01,  4.3336e-02,  ..., -8.6679e-02,\n",
            "          -2.3537e-02,  4.1579e-02],\n",
            "         [-7.9858e-02,  2.1232e-01,  4.3336e-02,  ..., -8.6679e-02,\n",
            "          -2.3537e-02,  4.1579e-02]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ 3.6669e-03, -1.3899e-01,  1.2217e-01,  ...,  2.9634e-03,\n",
            "           3.3370e-02,  2.6259e-02],\n",
            "         [ 7.3923e-02, -7.2264e-02,  8.9267e-02,  ...,  1.0225e-02,\n",
            "           2.3034e-02,  3.0612e-02],\n",
            "         [ 3.6707e-03, -1.2186e-01,  1.3961e-01,  ..., -2.3763e-03,\n",
            "           4.0690e-02,  4.4446e-02],\n",
            "         ...,\n",
            "         [ 1.8952e-02, -7.4627e-02,  1.1322e-01,  ...,  1.0764e-02,\n",
            "           4.0110e-02, -3.5752e-03],\n",
            "         [ 3.2183e-02, -1.0829e-01,  1.3437e-01,  ..., -6.6524e-02,\n",
            "           6.4322e-02, -9.7074e-03],\n",
            "         [ 4.0131e-02, -8.3839e-02,  1.2064e-01,  ..., -2.8218e-02,\n",
            "           2.2480e-02,  4.6200e-02]],\n",
            "\n",
            "        [[ 3.2983e-02,  1.4652e-01, -6.2980e-02,  ..., -7.3771e-02,\n",
            "          -3.4934e-02, -9.5423e-04],\n",
            "         [ 2.6202e-02,  1.5127e-01,  3.7351e-02,  ..., -3.7340e-02,\n",
            "          -3.2400e-02, -5.8058e-03],\n",
            "         [ 5.3119e-02,  1.6323e-01,  2.4938e-02,  ..., -5.6528e-02,\n",
            "          -3.5871e-02,  2.1593e-03],\n",
            "         ...,\n",
            "         [ 6.4168e-02,  1.3343e-01, -2.8101e-02,  ..., -5.8943e-02,\n",
            "          -4.3936e-02, -2.8736e-02],\n",
            "         [ 6.4168e-02,  1.3343e-01, -2.8101e-02,  ..., -5.8943e-02,\n",
            "          -4.3936e-02, -2.8736e-02],\n",
            "         [ 6.4168e-02,  1.3343e-01, -2.8101e-02,  ..., -5.8943e-02,\n",
            "          -4.3936e-02, -2.8736e-02]],\n",
            "\n",
            "        [[-1.1003e-02,  1.7258e-01,  3.5333e-02,  ...,  9.4271e-03,\n",
            "          -7.9916e-02,  1.0279e-01],\n",
            "         [-1.6356e-02,  1.7450e-01,  6.0128e-02,  ...,  1.0145e-02,\n",
            "          -5.8519e-02,  1.1559e-01],\n",
            "         [-5.8616e-02,  2.2601e-01,  6.4892e-02,  ..., -5.7269e-03,\n",
            "          -4.3481e-02,  7.5643e-02],\n",
            "         ...,\n",
            "         [-7.0533e-02,  1.4867e-01,  3.5813e-02,  ..., -6.0959e-03,\n",
            "          -4.9646e-02,  9.6893e-02],\n",
            "         [-7.0533e-02,  1.4867e-01,  3.5813e-02,  ..., -6.0959e-03,\n",
            "          -4.9646e-02,  9.6893e-02],\n",
            "         [-7.0533e-02,  1.4867e-01,  3.5813e-02,  ..., -6.0959e-03,\n",
            "          -4.9646e-02,  9.6893e-02]]], grad_fn=<ViewBackward0>)\n",
            "torch.Size([10, 20, 512])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OTku1fySVR3L"
      },
      "source": [],
      "execution_count": 21,
      "outputs": []
    }
  ]
}